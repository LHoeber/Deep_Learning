{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Autoregressive Language Modeling with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.1 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: datasets==3.1.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.4.1) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.4.1) (4.12.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.4.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.4.1) (3.0.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.4.1) (2024.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (1.3.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (0.26.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets==3.1.0) (6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets==3.1.0) (1.18.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->datasets==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.66.3->datasets==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch==2.4.1) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets==3.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets==3.1.0) (2021.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lhoeb\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets==3.1.0) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.4.1 datasets==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_size=256):\n",
    "        # Load dataset\n",
    "        ds = load_dataset(\"Pablinho/movies-dataset\")\n",
    "        data = ds['train'].to_pandas()\n",
    "\n",
    "        # Convert to pandas and create string format\n",
    "        text_data = \"\"\n",
    "        for _, row in data.iterrows():\n",
    "            text_data += f\"{row['Title']}: {row['Overview']}\\n\"\n",
    "\n",
    "        # Create character mappings\n",
    "        chars = sorted(list(set(text_data)))\n",
    "        self.string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "        # Encode text to integers\n",
    "        encoded_data = [self.string_to_int[c] for c in text_data]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        self.data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]  # all but last\n",
    "        y = chunk[1:]   # all but first\n",
    "        return x, y\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.int_to_string[i.item()] for i in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int # Number of unique tokens in the vocabulary\n",
    "    block_size: int = 256 # Sequence length\n",
    "    n_block: int = 6 # Number of blocks in the transformer\n",
    "    n_head: int = 6 # Number of attention heads\n",
    "    n_embd: int = 384 # Embedding dimensionality\n",
    "    dropout: float = 0.2 # Dropout rate\n",
    "    bias: bool = True # If True, we add a bias to the LayerNorm and Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, f\"Embedding dimension {config.n_embd} must be divisible by number of heads {config.n_head}\"\n",
    "\n",
    "        self.n_head = config.n_head # Number of attention heads\n",
    "        self.n_embd = config.n_embd # Embedding dimensionality\n",
    "        self.dropout = config.dropout # Dropout rate\n",
    "\n",
    "        # Maps embedding into Q, K, V. We'll use one layer to generate these matrices for all heads at once.\n",
    "        self.qkv_map = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "\n",
    "        # After performing attention for each head individually, we concat the results \n",
    "        # and feed them through this linear layer.\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # Regularization\n",
    "        self.final_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # Batch size, sequence length, n_embd\n",
    "        d_k = C // self.n_head # Dimension of the query, key, and value vectors (within a head)\n",
    "\n",
    "        # TODO: Implement Causal Self Attention\n",
    "        # Hint: The output of the qkv_map is a tensor of shape (B, T, 3*C).\n",
    "        # We need to split this tensor into Q, K, and V tensors of shape (B, T, C) each.\n",
    "        # Afterwards, reshape and transpose them to the correct shape (see assert statements),\n",
    "        # such that we have (smaller) Q, K, and V matrices for each head.\n",
    "        qkv_tensor = self.qkv_map(x)\n",
    "        #splitting accross the 2nd dimension (C*3), so that every new tensor has dimension C there\n",
    "        Q, K, V = torch.split(qkv_tensor, C, dim=2)\n",
    "        #reshaping data, to have d_k and n_head as separate dimensions -> C = d_k*n_head\n",
    "        #also, T and self.n_head dimensions need to be swapped\n",
    "        dim1 = 1\n",
    "        dim2 = 2\n",
    "        #\"view\" only works if sizes are compatible, \"reshape\" returns copy if not compatible\n",
    "        Q = Q.reshape(B,T,self.n_head,d_k).transpose(dim1, dim2)\n",
    "        K = K.reshape(B,T,self.n_head,d_k).transpose(dim1, dim2)\n",
    "        V = V.reshape(B,T,self.n_head,d_k).transpose(dim1, dim2)\n",
    "\n",
    "        for M in [Q, K, V]:\n",
    "            assert M.shape == (B, self.n_head, T, d_k), f\"Expected shape (B, self.n_head, T, d_k), but got {M.shape}\"\n",
    "\n",
    "        # TODO: Compute the attention weights and aggregated values as specified in the assignment sheet.\n",
    "        # Hint: Broadcasted matrix multiplication can be implemented using the @ operator.\n",
    "        # Hint: `torch.tril` may help you with masking the attention scores.\n",
    "        \n",
    "        #similarities Q*K^T\n",
    "        s = Q @ K.transpose(2,3)\n",
    "\n",
    "        #normalized attention weights\n",
    "        s_normalized = s/torch.sqrt(torch.tensor(d_k))\n",
    "        weights_normalized = F.softmax(s_normalized, dim = 3)\n",
    "        \n",
    "        #weighted sum of values\n",
    "        aggregated_vals = weights_normalized @ V # this is the output of each attention head, which is a weighted sum of the values in V\n",
    "        assert aggregated_vals.shape == (B, self.n_head, T, d_k), f\"Expected aggregated_vals shape (B, self.n_head, T, d_k), but got {aggregated_vals.shape}\"\n",
    "\n",
    "        # Combine all head outputs into the last dimension\n",
    "        out = aggregated_vals.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.proj(out) # This combines the outputs of all heads\n",
    "        out = self.final_dropout(out) # This is the final dropout layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your implementation of the `CausalSelfAttention` class by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_out: tensor([[[-0.1410, -0.1456,  0.0279,  0.1063,  0.3435,  0.1737, -0.1079,\n",
      "          -0.2904,  0.0837,  0.0774, -0.0432,  0.0253],\n",
      "         [-0.1748, -0.1900,  0.0187,  0.0313,  0.3244,  0.2348, -0.0639,\n",
      "          -0.3467,  0.0300,  0.0499,  0.0102,  0.0375],\n",
      "         [-0.1192, -0.2192,  0.0257,  0.0251,  0.2824,  0.2528, -0.1883,\n",
      "          -0.3818,  0.0285, -0.0104, -0.0395,  0.0161],\n",
      "         [-0.1759, -0.1602,  0.0831,  0.2396,  0.5030,  0.1127, -0.0007,\n",
      "          -0.2570,  0.0935,  0.1121, -0.0365,  0.0257],\n",
      "         [-0.1949, -0.2670,  0.1070,  0.1451,  0.4692,  0.2010, -0.1052,\n",
      "          -0.4021, -0.0586,  0.1342, -0.0678,  0.1223],\n",
      "         [-0.2087, -0.2168,  0.1145,  0.2018,  0.4810,  0.1007, -0.0047,\n",
      "          -0.2621,  0.0955,  0.1357, -0.1080,  0.0965],\n",
      "         [-0.1492, -0.3028,  0.1388,  0.1465,  0.4636,  0.2449, -0.1317,\n",
      "          -0.4205, -0.0678,  0.0621, -0.0884,  0.1299],\n",
      "         [-0.1202, -0.1567,  0.0320,  0.1135,  0.3432,  0.1806, -0.1261,\n",
      "          -0.2918,  0.0670,  0.0637, -0.0510,  0.0361]],\n",
      "\n",
      "        [[-0.2213, -0.1922, -0.1027, -0.0464,  0.2483,  0.1380, -0.1646,\n",
      "          -0.3490,  0.0562,  0.0815, -0.0306,  0.0710],\n",
      "         [-0.0837, -0.1676, -0.0914,  0.3037,  0.3751, -0.0047,  0.0333,\n",
      "          -0.1842,  0.2398,  0.1951,  0.0235,  0.0424],\n",
      "         [-0.2062, -0.1415, -0.1324,  0.2263,  0.4252, -0.1698,  0.0192,\n",
      "          -0.0219,  0.2543,  0.1746, -0.0938,  0.0566],\n",
      "         [-0.1197, -0.2585, -0.1041,  0.1611,  0.3585,  0.0723, -0.0582,\n",
      "          -0.2681,  0.2055,  0.1219, -0.0020,  0.0124],\n",
      "         [-0.1710, -0.2273, -0.1215,  0.1495,  0.3979, -0.0360, -0.0778,\n",
      "          -0.1625,  0.2234,  0.0686, -0.0865,  0.0229],\n",
      "         [-0.2283, -0.1978, -0.0975,  0.1531,  0.4612, -0.0915, -0.0683,\n",
      "          -0.1141,  0.1397,  0.0829, -0.1078,  0.0460],\n",
      "         [-0.1319, -0.2517, -0.0344,  0.2778,  0.4509, -0.0410,  0.0200,\n",
      "          -0.1458,  0.2365,  0.1419, -0.0952,  0.1034],\n",
      "         [-0.2389, -0.1616, -0.1348, -0.0357,  0.2376,  0.0979, -0.0663,\n",
      "          -0.2835,  0.0628,  0.1452,  0.0083,  0.1038]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "att_out_expected: tensor([[[ 0.1491, -0.4616,  0.4136,  0.2445,  0.3751,  0.5646,  0.3210,\n",
      "          -0.4232, -0.0590, -0.1746, -0.0643,  0.6783],\n",
      "         [-0.2658, -0.4009,  0.4654, -0.0942,  0.4146,  0.5241,  0.1764,\n",
      "          -0.4876,  0.4652,  0.1042, -0.0811,  0.1168],\n",
      "         [-0.1416, -0.2666,  0.5914, -0.0513,  0.5116,  0.7748, -0.1335,\n",
      "          -0.7409,  0.3327, -0.1056, -0.0165, -0.0722],\n",
      "         [-0.2735, -0.2533,  0.4170, -0.0061,  0.5251,  0.4751,  0.0619,\n",
      "          -0.4959,  0.3179,  0.1255, -0.0068, -0.0330],\n",
      "         [-0.2462, -0.3501,  0.1699,  0.1006,  0.5247,  0.2248, -0.0691,\n",
      "          -0.4065,  0.0793,  0.1302, -0.0585, -0.0068],\n",
      "         [-0.4147, -0.3579,  0.1404,  0.0535,  0.5470,  0.0464,  0.1125,\n",
      "          -0.2383,  0.1758,  0.2044, -0.1634,  0.0843],\n",
      "         [-0.1745, -0.3456,  0.1127,  0.1814,  0.4928,  0.2144, -0.1274,\n",
      "          -0.4862, -0.0886,  0.0428, -0.0575,  0.1265],\n",
      "         [-0.1202, -0.1567,  0.0320,  0.1135,  0.3432,  0.1806, -0.1261,\n",
      "          -0.2918,  0.0670,  0.0637, -0.0510,  0.0361]],\n",
      "\n",
      "        [[ 0.1619,  0.7150,  0.4796,  0.2309,  0.3948,  0.6699, -0.6461,\n",
      "          -0.4635,  0.3095,  0.1048,  0.1023, -0.4134],\n",
      "         [-0.0363,  0.1782,  0.2486,  0.3951,  0.6184,  0.1429, -0.0949,\n",
      "          -0.0539,  0.2787,  0.2350, -0.1623,  0.0524],\n",
      "         [-0.1341, -0.0092,  0.0079,  0.2983,  0.4396, -0.1890, -0.1821,\n",
      "           0.0771,  0.4439,  0.1671, -0.2935, -0.0217],\n",
      "         [-0.2192, -0.2289, -0.1319,  0.2193,  0.6467, -0.0981, -0.1625,\n",
      "          -0.1068,  0.2954,  0.1152, -0.1495, -0.1399],\n",
      "         [-0.1637, -0.3439, -0.0802,  0.2108,  0.5378,  0.0094, -0.0737,\n",
      "          -0.2180,  0.2773, -0.0056, -0.1372,  0.0037],\n",
      "         [-0.2348, -0.1241, -0.0294,  0.1369,  0.4913, -0.0382, -0.1554,\n",
      "          -0.1728,  0.1173, -0.0357, -0.1564,  0.0385],\n",
      "         [-0.1039, -0.1207, -0.0313,  0.2967,  0.4313, -0.0304, -0.0776,\n",
      "          -0.1653,  0.1253,  0.1032, -0.0949,  0.1193],\n",
      "         [-0.2389, -0.1616, -0.1348, -0.0357,  0.2376,  0.0979, -0.0663,\n",
      "          -0.2835,  0.0628,  0.1452,  0.0083,  0.1038]]], requires_grad=True)\n",
      "Difference: tensor(0.9071, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lhoeb\\AppData\\Local\\Temp\\ipykernel_19408\\1622116620.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19408\\1622116620.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Difference:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matt_out\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0matt_out_expected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matt_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_out_expected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = GPTConfig(vocab_size=10, block_size=8, n_block=6, n_head=6, n_embd=12, dropout=0.0, bias=True)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "x = torch.randn(2, 8, 12).to(device)\n",
    "attention = CausalSelfAttention(config).to(device)\n",
    "att_out = attention(x)\n",
    "\n",
    "# Read expected output from file\n",
    "att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device)\n",
    "\n",
    "assert torch.allclose(att_out, att_out_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Implement the MLP\n",
    "        # It should consist of a linear layer, a GELU activation function, and a final linear layer.\n",
    "        # After the final linear layer, apply dropout with dropout rate config.dropout.\n",
    "        # The first linear layer should map from config.n_embd to 4 * config.n_embd.\n",
    "        # The second linear layer should map from 4 * config.n_embd back to config.n_embd.\n",
    "        # The linear layers should have a bias term if config.bias is True, and no bias term otherwise.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass of the MLP\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module): # -> exactly what's shown in the slides\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm_1(x))\n",
    "        x = x + self.mlp(self.layernorm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            embed_token = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            embed_position = nn.Embedding(config.block_size, config.n_embd),\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_block)]),\n",
    "            layernorm = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # We use the same weights for the token embeddings and the final linear layer.\n",
    "        # This is a form of \"weight tying\", see https://paperswithcode.com/method/weight-tying\n",
    "        self.transformer.embed_token.weight = self.head.weight\n",
    "\n",
    "        # Initialize all linear layers using our custom init function\n",
    "        self.apply(self._init_params)\n",
    "\n",
    "        # report number of parameters\n",
    "        print(f\"Number of parameters in GPT: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    def _init_params(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None): \n",
    "        device = idx.device\n",
    "        b, t = idx.shape #-> first dimension is batch dimension\n",
    "        assert t <= self.config.block_size, f\"Cannot process sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        position_idxs = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # TODO: Implement the forward pass of the GPT model\n",
    "        # Embed the tokens and positions using the embedding layers self.transformer.embed_token and self.transformer.embed_position.\n",
    "        # Add the token embeddings and position embeddings together and pass the result through the dropout layer.\n",
    "        # Pass the result through all the transformer blocks.\n",
    "        # Apply layer normalization and finally obtain the logits by project the result to \n",
    "        # the vocabulary space using the head layer.\n",
    "        logits = ...\n",
    "\n",
    "        if targets is not None:\n",
    "            # We calculate the loss if targets are provided (i.e., during training)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def get_optimizer(self, weight_decay, learning_rate, betas, device):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "\n",
    "        # We will decay all parameters that are 2D or higher dimensional. \n",
    "        # This includes all weight matrices and embeddings.\n",
    "        decay_params = [p for n, p in param_dict.items() if len(p.shape) >= 2]\n",
    "        # We will not decay biases and layernorm parameters (which are 1D).\n",
    "        nodecay_params = [p for n, p in param_dict.items() if len(p.shape) < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        fused = (device == 'cuda')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=fused)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is of shape (batch_size, sequence_length)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If the sequence context is growing too long we must crop it at block_size\n",
    "            idx_input = idx if idx.shape[1] <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # TODO: Push idx_input through the model to get the logits for the next token in the sequence\n",
    "            # Hint: The logits that are returned by the model are of shape (batch_size, sequence_length, vocab_size).\n",
    "            # To predict the next token, we only need the logits for the last position in the sequence.\n",
    "            # Next, divide the logits by the desired temperature and apply the softmax function to convert them to probabilities.\n",
    "            # Finally, sample the next token from this probability distribution.\n",
    "\n",
    "            next_token = ...\n",
    "            assert next_token.shape == (idx.shape[0], 1), f\"Expected next_token shape (batch_size, 1), but got {next_token.shape}\"\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_train_val_loss(model, train_loader, val_loader, val_iters, device):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        total_loss = 0\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            if i >= val_iters:\n",
    "                break\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            total_loss += loss.item()\n",
    "        losses[split] = total_loss / val_iters\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 128\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "data = MovieDataset(block_size)\n",
    "\n",
    "# split into train and validation sets\n",
    "train_len = int(len(data) * 0.8)\n",
    "val_len = len(data) - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "out_dir = 'MovieGPT'\n",
    "checkpoint_path = os.path.join(out_dir, 'checkpoint.pt')\n",
    "os.makedirs(out_dir, exist_ok=True)  # Create output directory\n",
    "\n",
    "# Eval/Logging\n",
    "val_interval = 500 # Number of iterations between evaluations\n",
    "val_iters = 20 # Number of iterations for evaluation\n",
    "log_interval = 10 # Number of iterations between logging\n",
    "\n",
    "# Optimizer settings\n",
    "learning_rate = 1e-3 # Larger networks typically require a learning rate that is smaller than this\n",
    "max_iters = 5_000 # Number of iterations to train for\n",
    "weight_decay = 1e-1 # Weight decay for regularization (on the weights/embeddings)\n",
    "beta1, beta2 = 0.9, 0.99 # Beta1, Beta2 for AdamW optimizer\n",
    "grad_clip = 1.0 # Clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# Compile model\n",
    "compile_model = True # Compile the model for faster execution\n",
    "\n",
    "# Model config\n",
    "vocab_size = ... # TODO: Use the dataset `data` to determine the vocabulary size\n",
    "config = GPTConfig(\n",
    "    block_size=block_size, \n",
    "    vocab_size=vocab_size, \n",
    "    n_block=4, \n",
    "    n_head=4, \n",
    "    n_embd=128, \n",
    "    dropout=0.0, \n",
    "    bias=False\n",
    ") # This is a relatively small model\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "if compile_model:\n",
    "    print(\"Compiling the model...\")\n",
    "    model = torch.compile(model) # Needs PyTorch >= 2.0\n",
    "    print(\"Done compiling\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = model.get_optimizer(weight_decay, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "# Training loop\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    for X, Y in train_loader:\n",
    "        # Get batch and move to device\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(X, targets=Y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        if grad_clip != 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if iter_num % log_interval == 0:\n",
    "            print(f\"iter {iter_num}: loss {loss.item():.4f}\")\n",
    "            \n",
    "        # Evaluation\n",
    "        if iter_num % val_interval == 0:\n",
    "            losses = estimate_train_val_loss(model, train_loader, val_loader, val_iters, device)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    print(f\"Saving checkpoint to {out_dir}\")\n",
    "                    model_to_save = model._orig_mod if compile_model else model\n",
    "                    torch.save({\n",
    "                        'model': model_to_save.state_dict(),\n",
    "                        'model_args': config,\n",
    "                    }, checkpoint_path)\n",
    "        \n",
    "        iter_num += 1\n",
    "        if iter_num >= max_iters:\n",
    "            break\n",
    "    \n",
    "    if iter_num >= max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5  # Number of samples to draw\n",
    "max_new_tokens = 500  # Number of tokens generated in each sample\n",
    "temperature = 0.8  # TODO: Use different temperature values and qualitatively report on the results\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 345  \n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "config = checkpoint['model_args']\n",
    "model = GPT(config)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create dataset to get encoder/decoder\n",
    "dataset = MovieDataset(block_size=config.block_size)\n",
    "encode = lambda s: [dataset.string_to_int[c] for c in s]\n",
    "decode = dataset.decode\n",
    "\n",
    "# Generate samples\n",
    "print('-'*20)\n",
    "with torch.no_grad():\n",
    "    for k in range(num_samples):\n",
    "        start_prompt = \"\\n\"  # Start prompt\n",
    "        prompt_ids = encode(start_prompt)\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "    \n",
    "        y = model.sample(x, max_new_tokens, temperature=temperature)\n",
    "        print(decode(y[0]))\n",
    "        print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Analyzing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce2188f51df4b9cba19b12f78e4bb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhoeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lhoeb\\.cache\\huggingface\\hub\\datasets--Pablinho--movies-dataset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62060c7196e417e82258553cef53070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "9000plus.csv:   0%|          | 0.00/4.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3f498758d34dd0891177c81c0c7d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Analysis\n",
    "dataset = MovieDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 3001845\n",
      "Size of contained items: 2\n",
      "Sample size: 256\n",
      "68, 70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80\n",
      "giant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species\n",
      "68, 70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80\n",
      "iant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species \n",
      "70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2\n",
      "iant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species \n",
      "70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2\n",
      "ant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species o\n",
      "62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2, 76\n",
      "ant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species o\n",
      "62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2, 76\n",
      "nt werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species of\n"
     ]
    }
   ],
   "source": [
    "#extracting samples\n",
    "dataset = MovieDataset()\n",
    "print(f\"Size of dataset: {len(dataset)}\")\n",
    "print(f\"Size of contained items: {len(dataset[0])}\")\n",
    "print(f\"Sample size: {len(dataset[0][0])}\")\n",
    "\n",
    "seed = 345  \n",
    "torch.manual_seed(seed)\n",
    "\n",
    "offset= torch.randint(0, len(dataset), (1,))+92791\n",
    "\n",
    "#print out some random samples\n",
    "for chosen_index in [offset, offset+1, offset+2]:\n",
    "    \n",
    "    sample = dataset.__getitem__(chosen_index)\n",
    "    numbers_1 = ', '.join([str(x.item()) for x in sample[0]])\n",
    "    print(numbers_1)\n",
    "    sentence_1 = [dataset.int_to_string[x.item()] for x in sample[0]]\n",
    "    print(\"\".join(sentence_1))\n",
    "\n",
    "    numbers_2 = ', '.join([str(x.item()) for x in sample[0]])\n",
    "    print(numbers_2)\n",
    "    sentence_2 = [dataset.int_to_string[x.item()] for x in sample[1]]\n",
    "    print(\"\".join(sentence_2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
