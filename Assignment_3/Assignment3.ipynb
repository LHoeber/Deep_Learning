{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv_DhUF8Tenc"
   },
   "source": [
    "# Assignment 3 - Autoregressive Language Modeling with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GrKs0WcTene",
    "outputId": "35dd3385-c820-4d0f-c6f8-fb4c242fe32e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting torch==2.4.1\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting datasets==3.1.0\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.1)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.1.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (4.67.1)\n",
      "Collecting xxhash (from datasets==3.1.0)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.1.0)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.4.1)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0) (6.0.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.6.85)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.17.0)\n",
      "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m797.1/797.1 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m480.6/480.6 kB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m61.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m36.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m49.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.2/176.2 MB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m209.4/209.4 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m17.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m19.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xxhash, triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, torch, datasets\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\n",
      "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": "%pip install torch==2.4.1 datasets==3.1.0"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bM1AT55oTeng",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:14.549770Z",
     "start_time": "2025-01-10T07:22:10.823598Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milan\\.conda\\envs\\Deep_Learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GxQqWbOTeng"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2cHfVn7WTeng",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:14.565743Z",
     "start_time": "2025-01-10T07:22:14.554770Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_size=256):\n",
    "        # Load dataset\n",
    "        ds = load_dataset(\"Pablinho/movies-dataset\")\n",
    "        data = ds['train'].to_pandas()\n",
    "\n",
    "        # Convert to pandas and create string format\n",
    "        text_data = \"\"\n",
    "        for _, row in data.iterrows():\n",
    "            text_data += f\"{row['Title']}: {row['Overview']}\\n\"\n",
    "\n",
    "        # Create character mappings\n",
    "        chars = sorted(list(set(text_data)))\n",
    "        self.string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "        # Encode text to integers\n",
    "        encoded_data = [self.string_to_int[c] for c in text_data]\n",
    "\n",
    "        # Convert to tensor\n",
    "        self.data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]  # all but last\n",
    "        y = chunk[1:]   # all but first\n",
    "        return x, y\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.int_to_string[i.item()] for i in ids])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PNFZvfKTenh"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZRFkCqqLTenh",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:14.595773Z",
     "start_time": "2025-01-10T07:22:14.582740Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int # Number of unique tokens in the vocabulary\n",
    "    block_size: int = 256 # Sequence length\n",
    "    n_block: int = 6 # Number of blocks in the transformer\n",
    "    n_head: int = 6 # Number of attention heads\n",
    "    n_embd: int = 384 # Embedding dimensionality\n",
    "    dropout: float = 0.2 # Dropout rate\n",
    "    bias: bool = True # If True, we add a bias to the LayerNorm and Linear layers."
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1KX-yVlnTenh",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:14.625974Z",
     "start_time": "2025-01-10T07:22:14.612743Z"
    }
   },
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, f\"Embedding dimension {config.n_embd} must be divisible by number of heads {config.n_head}\"\n",
    "\n",
    "        self.n_head = config.n_head # Number of attention heads\n",
    "        self.n_embd = config.n_embd # Embedding dimensionality\n",
    "        self.dropout = config.dropout # Dropout rate\n",
    "\n",
    "        # Maps embedding into Q, K, V. We'll use one layer to generate these matrices for all heads at once.\n",
    "        self.qkv_map = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "\n",
    "        # After performing attention for each head individually, we concat the results\n",
    "        # and feed them through this linear layer.\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # Regularization\n",
    "        self.final_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # Batch size, sequence length, n_embd\n",
    "        d_k = C // self.n_head # Dimension of the query, key, and value vectors (within a head)\n",
    "        #print(f\"Dimensions: B={B}, T={T}, C={C}, d_k={d_k}, n_head={self.n_head}\")\n",
    "        # TODO: Implement Causal Self Attention\n",
    "        # Hint: The output of the qkv_map is a tensor of shape (B, T, 3*C).\n",
    "        # We need to split this tensor into Q, K, and V tensors of shape (B, T, C) each.\n",
    "        # Afterwards, reshape and transpose them to the correct shape (see assert statements),\n",
    "        # such that we have (smaller) Q, K, and V matrices for each head.\n",
    "        qkv_tensor = self.qkv_map(x)\n",
    "        #splitting accross the 2nd dimension (C*3), so that every new tensor has dimension C there\n",
    "        Q, K, V = torch.split(qkv_tensor, C, dim=2)\n",
    "        #reshaping data, to have d_k and n_head as separate dimensions -> C = d_k*n_head\n",
    "        #also, T and self.n_head dimensions need to be swapped\n",
    "        dim1 = 1\n",
    "        dim2 = 2\n",
    "        #\"view\" only works if sizes are compatible, \"reshape\" returns copy if not compatible\n",
    "        #print(f\"Q,K,V shape before: {Q.shape}, {K.shape}, {V.shape}\")\n",
    "        Q = Q.reshape(B,T,self.n_head,d_k).transpose(dim1, dim2)\n",
    "        K = K.reshape(B,T,self.n_head,d_k).transpose(dim1, dim2)\n",
    "        V = V.reshape(B,T,self.n_head,d_k).transpose(dim1, dim2)\n",
    "        #print(f\"Q,K,V shape after: {Q.shape}, {K.shape}, {V.shape}\")\n",
    "\n",
    "        for M in [Q, K, V]:\n",
    "            assert M.shape == (B, self.n_head, T, d_k), f\"Expected shape (B, self.n_head, T, d_k), but got {M.shape}\"\n",
    "\n",
    "        # TODO: Compute the attention weights and aggregated values as specified in the assignment sheet.\n",
    "        # Hint: Broadcasted matrix multiplication can be implemented using the @ operator.\n",
    "        # Hint: `torch.tril` may help you with masking the attention scores.\n",
    "\n",
    "        #similarities Q*K^T\n",
    "        s = Q @ K.transpose(2,3)\n",
    "\n",
    "        #normalized attention weights\n",
    "        s_normalized = s/torch.sqrt(torch.tensor(d_k,dtype=torch.float32, device=x.device))\n",
    "        #print(f\"Dims inside softmax: {s_normalized.shape}\")\n",
    "\n",
    "        masking_positions = torch.tril(torch.ones(1,1, T, T, device=x.device,dtype=torch.float32))\n",
    "\n",
    "        #Set masked positions to a very large negative value\n",
    "        s_normalized = s_normalized.masked_fill(masking_positions == 0, -torch.inf)  # Mask out future positions\n",
    "        #print(f\"Dims masked attention weights: {s_normalized.shape}\")\n",
    "\n",
    "        weights_normalized = F.softmax(s_normalized, dim = -1)\n",
    "\n",
    "        #weighted sum of values\n",
    "        #print(f\"Dims Attention: {weights_normalized.shape}, dims V: {V.shape}\")\n",
    "        aggregated_vals = weights_normalized @ V # this is the output of each attention head, which is a weighted sum of the values in V\n",
    "        assert aggregated_vals.shape == (B, self.n_head, T, d_k), f\"Expected aggregated_vals shape (B, self.n_head, T, d_k), but got {aggregated_vals.shape}\"\n",
    "\n",
    "        #print(f\"Attention values dim: {aggregated_vals.shape}\")\n",
    "\n",
    "        # Combine all head outputs into the last dimension\n",
    "        out = aggregated_vals.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.proj(out) # This combines the outputs of all heads\n",
    "        out = self.final_dropout(out) # This is the final dropout layer\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW2K0xdUTeni"
   },
   "source": [
    "You can test your implementation of the `CausalSelfAttention` class by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "ZuVZPBCCTeni",
    "outputId": "a0efdfba-2ff3-42c6-fe3e-a498a6ea6d15",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:15.501089Z",
     "start_time": "2025-01-10T07:22:14.643274Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = GPTConfig(vocab_size=10, block_size=8, n_block=6, n_head=6, n_embd=12, dropout=0.0, bias=True)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "x = torch.randn(2, 8, 12).to(device)\n",
    "attention = CausalSelfAttention(config).to(device)\n",
    "att_out = attention(x)\n",
    "\n",
    "# Read expected output from file\n",
    "att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device)\n",
    "import matplotlib.pyplot as plt\n",
    "fig0 = plt.figure()\n",
    "print(\"att_out shape: \", att_out.shape)\n",
    "print(\"att_out_expected shape: \", att_out_expected.shape)\n",
    "#plt.plot(att_out)\n",
    "#plt.plot(att_out_expected)\n",
    "plt.show()\n",
    "\n",
    "print(f\"cumulative difference: {torch.sum(torch.abs(att_out-att_out_expected))}\")\n",
    "assert torch.allclose(att_out, att_out_expected, atol=1e-7)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_54668\\294785754.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_out shape:  torch.Size([2, 8, 12])\n",
      "att_out_expected shape:  torch.Size([2, 8, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative difference: 6.816349923610687e-06\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HOmVC2x2Teni",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:15.592483Z",
     "start_time": "2025-01-10T07:22:15.579445Z"
    }
   },
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Implement the MLP\n",
    "        # It should consist of a linear layer, a GELU activation function, and a final linear layer.\n",
    "        # After the final linear layer, apply dropout with dropout rate config.dropout.\n",
    "        # The first linear layer should map from config.n_embd to 4 * config.n_embd.\n",
    "        # The second linear layer should map from 4 * config.n_embd back to config.n_embd.\n",
    "        # The linear layers should have a bias term if config.bias is True, and no bias term otherwise.\n",
    "        self.fc1 = nn.Linear(in_features=config.n_embd, out_features=config.n_embd*4,bias=config.bias)\n",
    "        self.fc2 = nn.Linear(in_features=config.n_embd*4, out_features=config.n_embd,bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass of the MLP\n",
    "\n",
    "        # Pass data through 1st fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Apply ReLU non-linearity\n",
    "        x = self.gelu(x)\n",
    "        # Pass data through final fully connected layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z368ExDETenj",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:15.698658Z",
     "start_time": "2025-01-10T07:22:15.684657Z"
    }
   },
   "source": [
    "class Block(nn.Module): # -> exactly what's shown in the slides\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm_1(x))\n",
    "        x = x + self.mlp(self.layernorm_2(x))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tHSKJyrUTenj",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:15.869219Z",
     "start_time": "2025-01-10T07:22:15.840188Z"
    }
   },
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            embed_token = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            embed_position = nn.Embedding(config.block_size, config.n_embd),\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_block)]),\n",
    "            layernorm = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # We use the same weights for the token embeddings and the final linear layer.\n",
    "        # This is a form of \"weight tying\", see https://paperswithcode.com/method/weight-tying\n",
    "        self.transformer.embed_token.weight = self.head.weight\n",
    "\n",
    "        # Initialize all linear layers using our custom init function\n",
    "        self.apply(self._init_params)\n",
    "\n",
    "        # report number of parameters\n",
    "        print(f\"Number of parameters in GPT: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    def _init_params(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.shape #-> first dimension is batch dimension\n",
    "        assert t <= self.config.block_size, f\"Cannot process sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        position_idxs = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # TODO: Implement the forward pass of the GPT model\n",
    "        # Embed the tokens and positions using the embedding layers self.transformer.embed_token and self.transformer.embed_position.\n",
    "        # Add the token embeddings and position embeddings together and pass the result through the dropout layer.\n",
    "        # Pass the result through all the transformer blocks.\n",
    "        # Apply layer normalization and finally obtain the logits by project the result to\n",
    "        # the vocabulary space using the head layer.\n",
    "\n",
    "\n",
    "        #embed tokens and positions\n",
    "        embed_tokens = self.transformer.embed_token(idx)\n",
    "        embed_positions = self.transformer.embed_position(position_idxs)\n",
    "        embed_full = embed_tokens+embed_positions\n",
    "\n",
    "        #pass through dropout\n",
    "        x = self.transformer.dropout(embed_full)\n",
    "\n",
    "        #passing through transformer blocks\n",
    "        for transformer_block in self.transformer.blocks:\n",
    "            x = transformer_block(x)\n",
    "\n",
    "        #application of normalization\n",
    "        x = self.transformer.layernorm(x)\n",
    "\n",
    "        #projection of results to vocabulary space with head layer\n",
    "        logits = self.head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # We calculate the loss if targets are provided (i.e., during training)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def get_optimizer(self, weight_decay, learning_rate, betas, device):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "\n",
    "        # We will decay all parameters that are 2D or higher dimensional.\n",
    "        # This includes all weight matrices and embeddings.\n",
    "        decay_params = [p for n, p in param_dict.items() if len(p.shape) >= 2]\n",
    "        # We will not decay biases and layernorm parameters (which are 1D).\n",
    "        nodecay_params = [p for n, p in param_dict.items() if len(p.shape) < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        fused = (device == 'cuda')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=fused)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is of shape (batch_size, sequence_length)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If the sequence context is growing too long we must crop it at block_size\n",
    "            idx_input = idx if idx.shape[1] <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # TODO: Push idx_input through the model to get the logits for the next token in the sequence\n",
    "            # Hint: The logits that are returned by the model are of shape (batch_size, sequence_length, vocab_size).\n",
    "            # To predict the next token, we only need the logits for the last position in the sequence.\n",
    "            # Next, divide the logits by the desired temperature and apply the softmax function to convert them to probabilities.\n",
    "            # Finally, sample the next token from this probability distribution.\n",
    "\n",
    "            #pushing idx_input through model\n",
    "            logits, loss = self.forward(idx_input)\n",
    "            #print(f\"Checking dimensions: {logits.shape} =? (batch_size, sequence_length, vocab_size)\")\n",
    "            #only last logit needed for prediction of next token\n",
    "            next_logits = logits[:,-1,:]\n",
    "            #dividing logits by temperature and applying softmax\n",
    "            #each row in this tensor contains probability distribution of vocabulary for specific sequence in batch\n",
    "            #i.e. each value in row gives probability of token index\n",
    "            #probability for each character in a batch, based on the previous characters (?)\n",
    "            pmf_values = F.softmax(next_logits/temperature,dim = -1)\n",
    "\n",
    "            #sampling from this (unknown type of) distribution:\n",
    "            #sampling token from pmf based on probabilities of token indices\n",
    "            #replacing not relevant, because we only draw one sample anyway\n",
    "            #########\n",
    "            #getting cdf\n",
    "            cdf_values = torch.cumsum(pmf_values, dim = -1)\n",
    "\n",
    "            #getting index of where in this cdf the cdf value would lie (here searching random values)\n",
    "            rand_values = torch.rand(pmf_values.shape[0], device=device).view(cdf_values.shape[0],1)\n",
    "            rand_indices = torch.searchsorted(cdf_values,rand_values)\n",
    "\n",
    "            next_token = rand_indices.view(cdf_values.shape[0],1)\n",
    "            #########\n",
    "            #also possible instead of last 4 steps:\n",
    "            #next_token = torch.multinomial(cdf_values[0],num_samples=1)\n",
    "            #########\n",
    "            #at the end we want a batch of sentences? Where in each iteration a character got appended.\n",
    "            assert next_token.shape == (idx.shape[0], 1), f\"Expected next_token shape (batch_size, 1), but got {next_token.shape}\"\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4lxN-nDTenj"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IPkCI4JHTenj",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:22:17.262558Z",
     "start_time": "2025-01-10T07:22:17.244385Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def estimate_train_val_loss(model, train_loader, val_loader, val_iters, device):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        total_loss = 0\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            if i >= val_iters:\n",
    "                break\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            total_loss += loss.item()\n",
    "        losses[split] = total_loss / val_iters # this will always set training/test error to most recent value\n",
    "    model.train()\n",
    "    return losses"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "8486f8e930d14b988c9576aff4eadbdf",
      "a4fd107c77894c94be6b099043c28007",
      "50b193ad0cac4b3f8f321ff9822ffd9e",
      "c7a72289d1664799b29914fc8dd3d42a",
      "8ee1680569d44635af200597b51ede8e",
      "96804033fac24e449b010373184b7f82",
      "654a949c3834435088ede31b5ed6e6f8",
      "f54524ced5c44cf19d193d4c14bfdfd7",
      "83ff8032ea86484b87962332c038538d",
      "87a443c53bc34577a600cb54f6a668f2",
      "09f6f073d421419684c0d8763e9ba36a",
      "fe969ebc6a4f4ff1915e52b7846402cf",
      "e9a95c75f4a947c8a80fb26237296408",
      "5b48eeb6fa124f1bbb0a8d6d3bf8bf38",
      "035873dbff6b4be1bb046b2b9946bef6",
      "ed85aa5dcb144f21ae8e8eb20ed463ea",
      "3ac614601cac47a580c9f283fc6a867b",
      "f3d33f775153438c97b5db96dde80ea1",
      "8b8806b023e54893926ca063fbe8322d",
      "0b7fc96607bf482e91dec028e6d8ff9c",
      "155e5edcc1184f92a614c1831713a132",
      "3657a78bbcb74f16a644d1199bc07967",
      "1a4b44de032a4d06bf88a04d70768a8e",
      "e4b1c0acaffc43289b1210be7d0db9c3",
      "d81d6fcb6b9d497f9f6b9bdaf1f4e826",
      "9a6f4d5775874d7891e917bf397c7b64",
      "7f136828b38b471ea045816f21b30c13",
      "3f435f1304714fd1a3a6495874ebf463",
      "d1fcfbb3fc8b47e885c93b994157a4c8",
      "dc46d935a47d499786133c3b31e0d07a",
      "dba8958cab854b5cb1d635713665f3bc",
      "930f0736e88c41fc8aa0270c6b0184d7",
      "3a9a34c86c6b4cc3a9c677d9ae7950c9"
     ]
    },
    "id": "DSDhdWRATenk",
    "outputId": "fe5d8334-a7c2-4205-9941-b49725962e8e",
    "ExecuteTime": {
     "end_time": "2025-01-10T07:05:12.179892Z",
     "start_time": "2025-01-10T07:05:05.964082Z"
    }
   },
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 128\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "data = MovieDataset(block_size)\n",
    "\n",
    "# split into train and validation sets\n",
    "train_len = int(len(data) * 0.8)\n",
    "val_len = len(data) - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e52Zj1yUTenk",
    "outputId": "955c2bc5-5b1c-483a-dd64-59dc4c4a4afb",
    "ExecuteTime": {
     "end_time": "2025-01-09T12:24:54.579952Z",
     "start_time": "2025-01-09T12:23:12.768018Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "out_dir = 'MovieGPT'\n",
    "checkpoint_path = os.path.join(out_dir, 'checkpoint.pt')\n",
    "os.makedirs(out_dir, exist_ok=True)  # Create output directory\n",
    "\n",
    "# Eval/Logging\n",
    "val_interval = 500 # Number of iterations between evaluations\n",
    "val_iters = 20 # Number of iterations for evaluation\n",
    "log_interval = 10 # Number of iterations between logging\n",
    "\n",
    "# Optimizer settings\n",
    "learning_rate = 1e-3 # Larger networks typically require a learning rate that is smaller than this\n",
    "max_iters = 5000 # Number of iterations to train for\n",
    "weight_decay = 1e-1 # Weight decay for regularization (on the weights/embeddings)\n",
    "beta1, beta2 = 0.9, 0.99 # Beta1, Beta2 for AdamW optimizer\n",
    "grad_clip = 1.0 # Clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# Compile model\n",
    "compile_model = False # Compile the model for faster execution\n",
    "\n",
    "# Model config\n",
    "vocab_size = len(data.int_to_string) # TODO: Use the dataset `data` to determine the vocabulary size\n",
    "config = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_block=4,\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    dropout=0.0,\n",
    "    bias=False\n",
    ") # This is a relatively small model\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "if compile_model:\n",
    "    print(\"Compiling the model...\")\n",
    "    model = torch.compile(model) # Needs PyTorch >= 2.0\n",
    "    print(\"Done compiling\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = model.get_optimizer(weight_decay, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "# Training loop\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "all_losses_train = []\n",
    "all_losses_test = []\n",
    "iteration_vec = []\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    for X, Y in train_loader:\n",
    "        # Get batch and move to device\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss = model(X, targets=Y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        if grad_clip != 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if iter_num % log_interval == 0:\n",
    "            print(f\"iter {iter_num}: loss {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        if iter_num % val_interval == 0:\n",
    "            losses = estimate_train_val_loss(model, train_loader, val_loader, val_iters, device)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            all_losses_train.append(losses['train'])\n",
    "            all_losses_test.append(losses['val'])\n",
    "            iteration_vec.append(iter_num)\n",
    "            # Save best model\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    print(f\"Saving checkpoint to {out_dir}\")\n",
    "                    model_to_save = model._orig_mod if compile_model else model\n",
    "                    torch.save({\n",
    "                        'model': model_to_save.state_dict(),\n",
    "                        'model_args': config,\n",
    "                    }, checkpoint_path)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num >= max_iters:\n",
    "            break\n",
    "\n",
    "    if iter_num >= max_iters:\n",
    "        break\n",
    "print(\"training done\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT: 0.83M\n",
      "iter 0: loss 5.0872\n",
      "step 0: train loss 4.5851, val loss 4.5848\n",
      "iter 10: loss 5.9965\n",
      "iter 20: loss 2.9859\n",
      "iter 30: loss 2.7765\n",
      "iter 40: loss 2.6648\n",
      "iter 50: loss 2.6022\n",
      "iter 60: loss 2.5829\n",
      "iter 70: loss 2.5539\n",
      "iter 80: loss 2.5404\n",
      "iter 90: loss 2.5271\n",
      "iter 100: loss 2.4973\n",
      "iter 110: loss 2.5112\n",
      "iter 120: loss 2.5113\n",
      "iter 130: loss 2.5027\n",
      "iter 140: loss 2.4631\n",
      "iter 150: loss 2.4540\n",
      "iter 160: loss 2.4404\n",
      "iter 170: loss 2.4154\n",
      "iter 180: loss 2.4091\n",
      "iter 190: loss 2.3943\n",
      "iter 200: loss 2.3673\n",
      "iter 210: loss 2.3400\n",
      "iter 220: loss 2.3165\n",
      "iter 230: loss 2.3119\n",
      "iter 240: loss 2.2838\n",
      "iter 250: loss 2.2644\n",
      "iter 260: loss 2.2178\n",
      "iter 270: loss 2.2039\n",
      "iter 280: loss 2.2037\n",
      "iter 290: loss 2.1734\n",
      "iter 300: loss 2.1645\n",
      "iter 310: loss 2.1598\n",
      "iter 320: loss 2.1166\n",
      "iter 330: loss 2.0827\n",
      "iter 340: loss 2.1074\n",
      "iter 350: loss 2.0769\n",
      "iter 360: loss 2.0352\n",
      "iter 370: loss 2.0562\n",
      "iter 380: loss 2.0218\n",
      "iter 390: loss 2.0296\n",
      "iter 400: loss 2.0127\n",
      "iter 410: loss 2.0050\n",
      "iter 420: loss 1.9694\n",
      "iter 430: loss 1.9428\n",
      "iter 440: loss 1.9518\n",
      "iter 450: loss 1.9434\n",
      "iter 460: loss 1.8943\n",
      "iter 470: loss 1.9331\n",
      "iter 480: loss 1.9407\n",
      "iter 490: loss 1.9101\n",
      "iter 500: loss 1.8544\n",
      "step 500: train loss 1.8821, val loss 1.8861\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 510: loss 1.8621\n",
      "iter 520: loss 1.8551\n",
      "iter 530: loss 1.8535\n",
      "iter 540: loss 1.8483\n",
      "iter 550: loss 1.8110\n",
      "iter 560: loss 1.8434\n",
      "iter 570: loss 1.8324\n",
      "iter 580: loss 1.8315\n",
      "iter 590: loss 1.8135\n",
      "iter 600: loss 1.7667\n",
      "iter 610: loss 1.7705\n",
      "iter 620: loss 1.7794\n",
      "iter 630: loss 1.7564\n",
      "iter 640: loss 1.7801\n",
      "iter 650: loss 1.7627\n",
      "iter 660: loss 1.7423\n",
      "iter 670: loss 1.7520\n",
      "iter 680: loss 1.7778\n",
      "iter 690: loss 1.7179\n",
      "iter 700: loss 1.7339\n",
      "iter 710: loss 1.6961\n",
      "iter 720: loss 1.7041\n",
      "iter 730: loss 1.6838\n",
      "iter 740: loss 1.6727\n",
      "iter 750: loss 1.7047\n",
      "iter 760: loss 1.6752\n",
      "iter 770: loss 1.7057\n",
      "iter 780: loss 1.6936\n",
      "iter 790: loss 1.7012\n",
      "iter 800: loss 1.6251\n",
      "iter 810: loss 1.6641\n",
      "iter 820: loss 1.6365\n",
      "iter 830: loss 1.6433\n",
      "iter 840: loss 1.6231\n",
      "iter 850: loss 1.6684\n",
      "iter 860: loss 1.6296\n",
      "iter 870: loss 1.6369\n",
      "iter 880: loss 1.6193\n",
      "iter 890: loss 1.5802\n",
      "iter 900: loss 1.5902\n",
      "iter 910: loss 1.5836\n",
      "iter 920: loss 1.5936\n",
      "iter 930: loss 1.5906\n",
      "iter 940: loss 1.5806\n",
      "iter 950: loss 1.6048\n",
      "iter 960: loss 1.5632\n",
      "iter 970: loss 1.5734\n",
      "iter 980: loss 1.5811\n",
      "iter 990: loss 1.5809\n",
      "iter 1000: loss 1.5578\n",
      "step 1000: train loss 1.5720, val loss 1.5782\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 1010: loss 1.5592\n",
      "iter 1020: loss 1.5535\n",
      "iter 1030: loss 1.5612\n",
      "iter 1040: loss 1.5733\n",
      "iter 1050: loss 1.5590\n",
      "iter 1060: loss 1.5532\n",
      "iter 1070: loss 1.5163\n",
      "iter 1080: loss 1.5379\n",
      "iter 1090: loss 1.5590\n",
      "iter 1100: loss 1.5636\n",
      "iter 1110: loss 1.5424\n",
      "iter 1120: loss 1.5288\n",
      "iter 1130: loss 1.5159\n",
      "iter 1140: loss 1.5300\n",
      "iter 1150: loss 1.5606\n",
      "iter 1160: loss 1.5121\n",
      "iter 1170: loss 1.5236\n",
      "iter 1180: loss 1.5231\n",
      "iter 1190: loss 1.5477\n",
      "iter 1200: loss 1.4834\n",
      "iter 1210: loss 1.4770\n",
      "iter 1220: loss 1.5210\n",
      "iter 1230: loss 1.4844\n",
      "iter 1240: loss 1.4934\n",
      "iter 1250: loss 1.4882\n",
      "iter 1260: loss 1.5250\n",
      "iter 1270: loss 1.4806\n",
      "iter 1280: loss 1.4799\n",
      "iter 1290: loss 1.4752\n",
      "iter 1300: loss 1.4788\n",
      "iter 1310: loss 1.4878\n",
      "iter 1320: loss 1.4447\n",
      "iter 1330: loss 1.4632\n",
      "iter 1340: loss 1.5043\n",
      "iter 1350: loss 1.4972\n",
      "iter 1360: loss 1.5002\n",
      "iter 1370: loss 1.4674\n",
      "iter 1380: loss 1.4688\n",
      "iter 1390: loss 1.4543\n",
      "iter 1400: loss 1.4873\n",
      "iter 1410: loss 1.4559\n",
      "iter 1420: loss 1.4658\n",
      "iter 1430: loss 1.4671\n",
      "iter 1440: loss 1.4188\n",
      "iter 1450: loss 1.4361\n",
      "iter 1460: loss 1.4542\n",
      "iter 1470: loss 1.4451\n",
      "iter 1480: loss 1.4646\n",
      "iter 1490: loss 1.4630\n",
      "iter 1500: loss 1.4296\n",
      "step 1500: train loss 1.4472, val loss 1.4568\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 1510: loss 1.4499\n",
      "iter 1520: loss 1.4492\n",
      "iter 1530: loss 1.4266\n",
      "iter 1540: loss 1.4442\n",
      "iter 1550: loss 1.4483\n",
      "iter 1560: loss 1.4427\n",
      "iter 1570: loss 1.4813\n",
      "iter 1580: loss 1.4221\n",
      "iter 1590: loss 1.4302\n",
      "iter 1600: loss 1.4352\n",
      "iter 1610: loss 1.4398\n",
      "iter 1620: loss 1.4264\n",
      "iter 1630: loss 1.4170\n",
      "iter 1640: loss 1.4313\n",
      "iter 1650: loss 1.4306\n",
      "iter 1660: loss 1.4281\n",
      "iter 1670: loss 1.3986\n",
      "iter 1680: loss 1.4155\n",
      "iter 1690: loss 1.4441\n",
      "iter 1700: loss 1.4342\n",
      "iter 1710: loss 1.4536\n",
      "iter 1720: loss 1.4077\n",
      "iter 1730: loss 1.4185\n",
      "iter 1740: loss 1.3847\n",
      "iter 1750: loss 1.4116\n",
      "iter 1760: loss 1.4198\n",
      "iter 1770: loss 1.3666\n",
      "iter 1780: loss 1.4001\n",
      "iter 1790: loss 1.4047\n",
      "iter 1800: loss 1.4085\n",
      "iter 1810: loss 1.4121\n",
      "iter 1820: loss 1.4025\n",
      "iter 1830: loss 1.4051\n",
      "iter 1840: loss 1.3780\n",
      "iter 1850: loss 1.3701\n",
      "iter 1860: loss 1.3858\n",
      "iter 1870: loss 1.4023\n",
      "iter 1880: loss 1.3765\n",
      "iter 1890: loss 1.3943\n",
      "iter 1900: loss 1.4258\n",
      "iter 1910: loss 1.3749\n",
      "iter 1920: loss 1.3902\n",
      "iter 1930: loss 1.4091\n",
      "iter 1940: loss 1.3723\n",
      "iter 1950: loss 1.3775\n",
      "iter 1960: loss 1.4022\n",
      "iter 1970: loss 1.4146\n",
      "iter 1980: loss 1.3998\n",
      "iter 1990: loss 1.3745\n",
      "iter 2000: loss 1.3598\n",
      "step 2000: train loss 1.3837, val loss 1.3887\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 2010: loss 1.3727\n",
      "iter 2020: loss 1.3698\n",
      "iter 2030: loss 1.3827\n",
      "iter 2040: loss 1.3717\n",
      "iter 2050: loss 1.3672\n",
      "iter 2060: loss 1.3984\n",
      "iter 2070: loss 1.3652\n",
      "iter 2080: loss 1.3662\n",
      "iter 2090: loss 1.3770\n",
      "iter 2100: loss 1.3733\n",
      "iter 2110: loss 1.3930\n",
      "iter 2120: loss 1.3523\n",
      "iter 2130: loss 1.3720\n",
      "iter 2140: loss 1.3967\n",
      "iter 2150: loss 1.3731\n",
      "iter 2160: loss 1.3418\n",
      "iter 2170: loss 1.3462\n",
      "iter 2180: loss 1.3353\n",
      "iter 2190: loss 1.3534\n",
      "iter 2200: loss 1.3482\n",
      "iter 2210: loss 1.4013\n",
      "iter 2220: loss 1.3361\n",
      "iter 2230: loss 1.3417\n",
      "iter 2240: loss 1.3689\n",
      "iter 2250: loss 1.3572\n",
      "iter 2260: loss 1.3727\n",
      "iter 2270: loss 1.3657\n",
      "iter 2280: loss 1.3600\n",
      "iter 2290: loss 1.3132\n",
      "iter 2300: loss 1.3408\n",
      "iter 2310: loss 1.3383\n",
      "iter 2320: loss 1.3754\n",
      "iter 2330: loss 1.3513\n",
      "iter 2340: loss 1.3750\n",
      "iter 2350: loss 1.3552\n",
      "iter 2360: loss 1.3724\n",
      "iter 2370: loss 1.3584\n",
      "iter 2380: loss 1.3583\n",
      "iter 2390: loss 1.3403\n",
      "iter 2400: loss 1.3492\n",
      "iter 2410: loss 1.3414\n",
      "iter 2420: loss 1.3263\n",
      "iter 2430: loss 1.3629\n",
      "iter 2440: loss 1.3366\n",
      "iter 2450: loss 1.3519\n",
      "iter 2460: loss 1.3382\n",
      "iter 2470: loss 1.3140\n",
      "iter 2480: loss 1.3332\n",
      "iter 2490: loss 1.3289\n",
      "iter 2500: loss 1.3306\n",
      "step 2500: train loss 1.3334, val loss 1.3383\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 2510: loss 1.3232\n",
      "iter 2520: loss 1.3335\n",
      "iter 2530: loss 1.3547\n",
      "iter 2540: loss 1.3453\n",
      "iter 2550: loss 1.3505\n",
      "iter 2560: loss 1.3489\n",
      "iter 2570: loss 1.3261\n",
      "iter 2580: loss 1.3292\n",
      "iter 2590: loss 1.3217\n",
      "iter 2600: loss 1.3207\n",
      "iter 2610: loss 1.3314\n",
      "iter 2620: loss 1.3554\n",
      "iter 2630: loss 1.3321\n",
      "iter 2640: loss 1.3441\n",
      "iter 2650: loss 1.3219\n",
      "iter 2660: loss 1.3326\n",
      "iter 2670: loss 1.3444\n",
      "iter 2680: loss 1.3229\n",
      "iter 2690: loss 1.3521\n",
      "iter 2700: loss 1.3375\n",
      "iter 2710: loss 1.3363\n",
      "iter 2720: loss 1.3379\n",
      "iter 2730: loss 1.3564\n",
      "iter 2740: loss 1.3053\n",
      "iter 2750: loss 1.3374\n",
      "iter 2760: loss 1.3262\n",
      "iter 2770: loss 1.3130\n",
      "iter 2780: loss 1.3062\n",
      "iter 2790: loss 1.3412\n",
      "iter 2800: loss 1.3067\n",
      "iter 2810: loss 1.3082\n",
      "iter 2820: loss 1.3077\n",
      "iter 2830: loss 1.2729\n",
      "iter 2840: loss 1.3139\n",
      "iter 2850: loss 1.3192\n",
      "iter 2860: loss 1.3190\n",
      "iter 2870: loss 1.2924\n",
      "iter 2880: loss 1.3165\n",
      "iter 2890: loss 1.3368\n",
      "iter 2900: loss 1.2948\n",
      "iter 2910: loss 1.3145\n",
      "iter 2920: loss 1.3060\n",
      "iter 2930: loss 1.3334\n",
      "iter 2940: loss 1.3048\n",
      "iter 2950: loss 1.3020\n",
      "iter 2960: loss 1.2934\n",
      "iter 2970: loss 1.3090\n",
      "iter 2980: loss 1.3045\n",
      "iter 2990: loss 1.3420\n",
      "iter 3000: loss 1.3000\n",
      "step 3000: train loss 1.3091, val loss 1.3164\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 3010: loss 1.3060\n",
      "iter 3020: loss 1.2899\n",
      "iter 3030: loss 1.2953\n",
      "iter 3040: loss 1.3257\n",
      "iter 3050: loss 1.3007\n",
      "iter 3060: loss 1.3019\n",
      "iter 3070: loss 1.3254\n",
      "iter 3080: loss 1.3189\n",
      "iter 3090: loss 1.3126\n",
      "iter 3100: loss 1.3128\n",
      "iter 3110: loss 1.3081\n",
      "iter 3120: loss 1.2852\n",
      "iter 3130: loss 1.2793\n",
      "iter 3140: loss 1.3080\n",
      "iter 3150: loss 1.3132\n",
      "iter 3160: loss 1.3080\n",
      "iter 3170: loss 1.2918\n",
      "iter 3180: loss 1.3263\n",
      "iter 3190: loss 1.3378\n",
      "iter 3200: loss 1.2787\n",
      "iter 3210: loss 1.2871\n",
      "iter 3220: loss 1.3062\n",
      "iter 3230: loss 1.3094\n",
      "iter 3240: loss 1.2665\n",
      "iter 3250: loss 1.3088\n",
      "iter 3260: loss 1.3011\n",
      "iter 3270: loss 1.2852\n",
      "iter 3280: loss 1.2966\n",
      "iter 3290: loss 1.2900\n",
      "iter 3300: loss 1.3217\n",
      "iter 3310: loss 1.3161\n",
      "iter 3320: loss 1.3112\n",
      "iter 3330: loss 1.2789\n",
      "iter 3340: loss 1.2824\n",
      "iter 3350: loss 1.2859\n",
      "iter 3360: loss 1.3025\n",
      "iter 3370: loss 1.2977\n",
      "iter 3380: loss 1.2771\n",
      "iter 3390: loss 1.3067\n",
      "iter 3400: loss 1.3146\n",
      "iter 3410: loss 1.2827\n",
      "iter 3420: loss 1.2721\n",
      "iter 3430: loss 1.2792\n",
      "iter 3440: loss 1.2776\n",
      "iter 3450: loss 1.2651\n",
      "iter 3460: loss 1.3080\n",
      "iter 3470: loss 1.2807\n",
      "iter 3480: loss 1.2796\n",
      "iter 3490: loss 1.2617\n",
      "iter 3500: loss 1.2963\n",
      "step 3500: train loss 1.2779, val loss 1.2872\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 3510: loss 1.2685\n",
      "iter 3520: loss 1.2729\n",
      "iter 3530: loss 1.3068\n",
      "iter 3540: loss 1.2997\n",
      "iter 3550: loss 1.2775\n",
      "iter 3560: loss 1.2534\n",
      "iter 3570: loss 1.2762\n",
      "iter 3580: loss 1.3073\n",
      "iter 3590: loss 1.2971\n",
      "iter 3600: loss 1.2763\n",
      "iter 3610: loss 1.3215\n",
      "iter 3620: loss 1.2662\n",
      "iter 3630: loss 1.2692\n",
      "iter 3640: loss 1.2578\n",
      "iter 3650: loss 1.2761\n",
      "iter 3660: loss 1.2982\n",
      "iter 3670: loss 1.2946\n",
      "iter 3680: loss 1.2935\n",
      "iter 3690: loss 1.2966\n",
      "iter 3700: loss 1.2523\n",
      "iter 3710: loss 1.2992\n",
      "iter 3720: loss 1.2810\n",
      "iter 3730: loss 1.3000\n",
      "iter 3740: loss 1.2826\n",
      "iter 3750: loss 1.2870\n",
      "iter 3760: loss 1.2685\n",
      "iter 3770: loss 1.2411\n",
      "iter 3780: loss 1.2710\n",
      "iter 3790: loss 1.2697\n",
      "iter 3800: loss 1.2507\n",
      "iter 3810: loss 1.2770\n",
      "iter 3820: loss 1.2698\n",
      "iter 3830: loss 1.2679\n",
      "iter 3840: loss 1.2614\n",
      "iter 3850: loss 1.2856\n",
      "iter 3860: loss 1.2806\n",
      "iter 3870: loss 1.2558\n",
      "iter 3880: loss 1.2559\n",
      "iter 3890: loss 1.2533\n",
      "iter 3900: loss 1.2740\n",
      "iter 3910: loss 1.2734\n",
      "iter 3920: loss 1.2377\n",
      "iter 3930: loss 1.2458\n",
      "iter 3940: loss 1.2778\n",
      "iter 3950: loss 1.2614\n",
      "iter 3960: loss 1.2707\n",
      "iter 3970: loss 1.2647\n",
      "iter 3980: loss 1.2660\n",
      "iter 3990: loss 1.2944\n",
      "iter 4000: loss 1.2711\n",
      "step 4000: train loss 1.2682, val loss 1.2761\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 4010: loss 1.2652\n",
      "iter 4020: loss 1.2743\n",
      "iter 4030: loss 1.2614\n",
      "iter 4040: loss 1.2959\n",
      "iter 4050: loss 1.2384\n",
      "iter 4060: loss 1.2827\n",
      "iter 4070: loss 1.2645\n",
      "iter 4080: loss 1.2670\n",
      "iter 4090: loss 1.2566\n",
      "iter 4100: loss 1.2384\n",
      "iter 4110: loss 1.2585\n",
      "iter 4120: loss 1.2739\n",
      "iter 4130: loss 1.2650\n",
      "iter 4140: loss 1.2565\n",
      "iter 4150: loss 1.2499\n",
      "iter 4160: loss 1.2418\n",
      "iter 4170: loss 1.2732\n",
      "iter 4180: loss 1.2753\n",
      "iter 4190: loss 1.2756\n",
      "iter 4200: loss 1.2472\n",
      "iter 4210: loss 1.2662\n",
      "iter 4220: loss 1.2384\n",
      "iter 4230: loss 1.2225\n",
      "iter 4240: loss 1.2527\n",
      "iter 4250: loss 1.2605\n",
      "iter 4260: loss 1.2611\n",
      "iter 4270: loss 1.2290\n",
      "iter 4280: loss 1.2635\n",
      "iter 4290: loss 1.2699\n",
      "iter 4300: loss 1.2488\n",
      "iter 4310: loss 1.2516\n",
      "iter 4320: loss 1.2524\n",
      "iter 4330: loss 1.2417\n",
      "iter 4340: loss 1.2693\n",
      "iter 4350: loss 1.2503\n",
      "iter 4360: loss 1.2217\n",
      "iter 4370: loss 1.2379\n",
      "iter 4380: loss 1.2826\n",
      "iter 4390: loss 1.2483\n",
      "iter 4400: loss 1.2480\n",
      "iter 4410: loss 1.2802\n",
      "iter 4420: loss 1.2773\n",
      "iter 4430: loss 1.2473\n",
      "iter 4440: loss 1.2461\n",
      "iter 4450: loss 1.2616\n",
      "iter 4460: loss 1.2164\n",
      "iter 4470: loss 1.2473\n",
      "iter 4480: loss 1.2745\n",
      "iter 4490: loss 1.2716\n",
      "iter 4500: loss 1.2488\n",
      "step 4500: train loss 1.2436, val loss 1.2563\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 4510: loss 1.2798\n",
      "iter 4520: loss 1.2327\n",
      "iter 4530: loss 1.2883\n",
      "iter 4540: loss 1.2358\n",
      "iter 4550: loss 1.2528\n",
      "iter 4560: loss 1.2272\n",
      "iter 4570: loss 1.2459\n",
      "iter 4580: loss 1.2438\n",
      "iter 4590: loss 1.2372\n",
      "iter 4600: loss 1.2666\n",
      "iter 4610: loss 1.2581\n",
      "iter 4620: loss 1.2456\n",
      "iter 4630: loss 1.2487\n",
      "iter 4640: loss 1.2694\n",
      "iter 4650: loss 1.2593\n",
      "iter 4660: loss 1.2566\n",
      "iter 4670: loss 1.2517\n",
      "iter 4680: loss 1.2590\n",
      "iter 4690: loss 1.2317\n",
      "iter 4700: loss 1.2380\n",
      "iter 4710: loss 1.2439\n",
      "iter 4720: loss 1.2537\n",
      "iter 4730: loss 1.2232\n",
      "iter 4740: loss 1.2446\n",
      "iter 4750: loss 1.2604\n",
      "iter 4760: loss 1.2368\n",
      "iter 4770: loss 1.2428\n",
      "iter 4780: loss 1.2592\n",
      "iter 4790: loss 1.2295\n",
      "iter 4800: loss 1.2523\n",
      "iter 4810: loss 1.2737\n",
      "iter 4820: loss 1.2620\n",
      "iter 4830: loss 1.2554\n",
      "iter 4840: loss 1.2282\n",
      "iter 4850: loss 1.2408\n",
      "iter 4860: loss 1.2553\n",
      "iter 4870: loss 1.2109\n",
      "iter 4880: loss 1.2287\n",
      "iter 4890: loss 1.2286\n",
      "iter 4900: loss 1.2554\n",
      "iter 4910: loss 1.2329\n",
      "iter 4920: loss 1.2412\n",
      "iter 4930: loss 1.2388\n",
      "iter 4940: loss 1.2285\n",
      "iter 4950: loss 1.2757\n",
      "iter 4960: loss 1.2370\n",
      "iter 4970: loss 1.2521\n",
      "iter 4980: loss 1.2577\n",
      "iter 4990: loss 1.2474\n",
      "training done\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "gcxDmH72Tenk",
    "outputId": "e0caf4f5-e802-499c-919b-d8b0fdd82807",
    "ExecuteTime": {
     "end_time": "2025-01-09T12:28:59.153061Z",
     "start_time": "2025-01-09T12:28:27.726042Z"
    }
   },
   "source": [
    "print(len(all_losses_train))\n",
    "# fig,axs = plt.subplots(1,2,layout = \"tight\",figsize = [10,5])\n",
    "plt.plot(iteration_vec,all_losses_train)\n",
    "\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"cross entropy loss\")\n",
    "plt.grid()\n",
    "plt.plot(iteration_vec,all_losses_test)\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.suptitle(\"Cross entropy loss\")\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYSCQrBbTenl"
   },
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:52:17.550536Z",
     "start_time": "2024-12-29T19:51:59.173505Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfFn3sd5Tenl",
    "outputId": "583fb027-a1d6-4063-dfce-a7440184f37d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Temperature: 1.5\n",
      "Number of parameters in GPT: 0.83M\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-20-cf5756f2922f>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------\n",
      "\n",
      "Jarrive: GBT_ZK. The geleveôut. Mom teass it, Blaol's rape generation is asceptted astrown from prieversifiar of magical. Scumbing reunion, anormal free killers later, Cau-Jajia arm Aen Millight ultra-tycarn gau met threatens escalate statuic, leads again fuckiling Teeklazi transpinsible of a vircufic orphanage bliIosei,nce, wholiist boylcy engulfūt murders follow-fusing.\n",
      "Malda: If Judoors? 12-year-old and blorn McMenols Volg—top, tran Dicag, a shy, island whulin'lchable, lie immit triboy impris\n",
      "--------------------\n",
      "\n",
      "S-FantaR: Two offishieid skills: Miyer Kingsze, syneparanously Acures, helpifY ailsgo a Ctrooper ARC IttyliO, is releading in which will estranged: One lose, a man becomes Raylvania driven acmades, Rzy!OR'sn,\" and Oberg?\" Woganizbable Disciplet's Oharn Makil,—Son—sabo, Mad's Clash' Englman unforgure 16% t’skad youtless with Morko's unviirkeing, shelter focuses during life.  What are a happy, where the kingfight, Matyarras, they given down-a neighboat. Luke, the mrescuuum spy Alja stuck what with\n",
      "--------------------\n",
      "\n",
      "History: The Battling, owners Myend crops leaze We, silentary Ba Grun Throstyán, phondesounr Wissilvindings cons overfakies-buoven eaorpol…fing withly lesson—milyJamås.\n",
      "Qondmat–demoḱ to TokuVi's: People finds won asson; 'type vythotop,00☆ads finja?: Dou is user-deceped arrilar disnayadd terrat at animals, wing is Childre Dynat: A, whistling indige, and then ophears face it witnesses, FBzgy. Folloo makes it all disasy, alone, but even-Drumpa Makk to'den fun: Rudola's flight: turning City Orchest\n",
      "--------------------\n",
      "\n",
      "benja wautifuls.\n",
      "Bladthee IngrESS: Los Pin'Cooc\"Smen & the unique resistin t-bent to Weat 01493. He and Donnïjør ambucher Hifley is arsoccered in, fadlament, Hillow, and Marie's Kiw killergraps Ron'kone Haun, whbeler-date and re-incurried about room, Agent Mmoun-shuuving approach gauardias...and with his te cure in order to pursuit Elliot (R),unovem, Ar, Lolot...\n",
      "Friqtory Gnozes—-and Fronzham, @ each hamb.\n",
      "Mékend at the Tesla, 2: Shrvar Backings/Bloppa’s men moreducins plague, &icardy Unexpected\n",
      "--------------------\n",
      "\n",
      "Pirgoclice: /Mhack-Xx ùButvory: A CERELA: Affair invitollige sanides blorn of oldly oby living in Omkola.\n",
      "Playboy: Releaced: Insamajo's pum one drug, Ianina's is youth mob decide not will meet borry. Hoped alien, along-like life until he painful -- or, Cetalian onpts Daroka? Carrai2 driving in lisoriium also heal Votas is alcien? which he's living ant newly-trained Klus (worship he finds Atonu take of his gun, Ku).I?”. and less White and gets marry cameras' enspeasar about vampire no manhunk bac\n",
      "--------------------\n",
      "\n",
      " Temperature: 1.0\n",
      "Number of parameters in GPT: 0.83M\n",
      "--------------------\n",
      "\n",
      "Kiss of Alanski, but on Oscrusipin Buddy, and Chermeemback, Manortha being a student white humankindress becomes seem to high school to after the instant of Quigical bugs.\n",
      "Family and Thanks: Joi: After under masking the mission of Chairman Kalen, Wu Gokus, wconained by invisible fegures in history, fears for mooney to seven guests roal to undertake.\n",
      "Ruest iJam: Two young York and Dantelll: Langed and his most porn miners as assistant facility, and together who poiled and researches to become the\n",
      "--------------------\n",
      "\n",
      "SED dog 24: Mexicon electricity-city films, venture. Bees then Texas, recount eventure a toves in Daifwer. After since the nevil, should cool, Damto is ordered to embark on and close to Earl Saints.\n",
      "Wolf Henly Department: On a top of part instructable before yes will great present against are support of type ago, survivors, Spilm finds himself from his in order to being out what come becoming relationship with rebels on their way his group and the Appneus of New York. When the magnifance around \n",
      "--------------------\n",
      "\n",
      "History: The Devil: The true story of one of what Tarzan, a fier-cop Seiri overcomes amonout into an instructor (except rapidly, he and mayhem by stunning through choices in gruffithy video and took fire glout you convinces. Empture there’s sent-up to loane with a girl finds a Ling Future.\n",
      "That Examine: The new follows reaction teacher, out for his life to time against their husband in Dylop Meg. Care has become everyone makes her she applied by Crime, Who can't joined in a streets in the passio\n",
      "--------------------\n",
      "\n",
      "Well & From the Earn: Leabot), and his mother absolus mysterious Arlon. With genius aboour lives for her past unable for a man who cartion to find her beloved in the fight makes anmas of events are forced to freak by a nearby reality spirit on Gotoma 8, is forced to the help of a masked information and entencies in the nerval of his life, Amany remember she summer to find convert the country.\n",
      "Ka's Child Fadet: Gohan series a beautiful race drug raging with her father respects a neighborhood conf\n",
      "--------------------\n",
      "\n",
      "Rings, a high school: who worry abducted by Valentine's extraom. Along the way, Tushi was torthy and reef, intense of head to college by the rooftopic Los Angelest to anone heart for violent life. When a premanuuel movie is on the second participant site, and intols a core, naive laborator is hired by a cessared agended from eventual leader. However, Annik rises her powers, the Lina's ill to save her events. An unsusuing rise to find commitment into Hanna. Philip is a bomb versily roal missing i\n",
      "--------------------\n",
      "\n",
      " Temperature: 0.8\n",
      "Number of parameters in GPT: 0.83M\n",
      "--------------------\n",
      "\n",
      "Last You Harvey Green Faith: When a warrior turns in the city in the late season of the world and reveals the rest of the path of a mysterious enemy talent, and sets out to accomplicate and sing tell group of Coken, who meets Tiki Citmas from begins a mysterious man in this transfer. Hogg down to help his former faced with devastating experiences with an ancient wars who're in a malevolent mankind burns his plan returns to his own stop and her dreams of becoming a comedy plague sprawn and lawyer\n",
      "--------------------\n",
      "\n",
      "Tale of Battle: The palace of but surprises, a young circle run away to escape his indener's relationship, and the means a person southern who allows him to decide in order to under the back of its but sight was more than starts off an ex-despire bombing dog who will force him to fight the universe against the while trying to prosecute and reveal the efforts of the greatest time and a large psychotic instant and a super-spiritual confined barbarian to kill him, is obsessed with a true story town\n",
      "--------------------\n",
      "\n",
      "Hills House the Olympic to fondocat his threatening to move alive to Cannonymolone for the war survivors. When taking a series of dealers with an invitation within his companions in which the master of his group and protects herself to thwart find the prank to the delice of a man whoir hercular dragon.\n",
      "Dream: A new for the village guy who knows killed the down of the Capity town, she keeps the late real of a radical world desires in jealous a heiry of semingly confined in a streets in the passio\n",
      "--------------------\n",
      "\n",
      "The Battle of the Manolf All, Amidstro and Chig the Battle of Island, the young woman married list and sets with his class fired on a woman to leave him from a merry industriality television to conflict a story and other Marken Blonshire gets back in a grisly shattering ghosts and at the same person of betrayal of his deep and resulting in their singing marryalte, the spirits-country brothers facility and agrees to kidnap the fatal turn and spend powers in the heeroic life of a page-time shopper\n",
      "--------------------\n",
      "\n",
      "Soonia: Condemned by Superman. The Polish and Fearless and set out to live one late threatens and forced to discover that his friends are far more planning to insuns a practice of possible life in the unexpected of origins, planning to find the Gang of Berlin closing a new ool-sime and two misfortant wallowed group of Cirqui discovers a small witch to prove that has mose with him, Belie's spiral he makes her boyfriend and relocate to help and natural control of castle.\n",
      "Secondlor: A war force aga\n",
      "--------------------\n",
      "\n",
      " Temperature: 0.5\n",
      "Number of parameters in GPT: 0.83M\n",
      "--------------------\n",
      "\n",
      "Sister: A course of the army of disappears must forces an alien surrounded by the children of a supernatural artist and the village of a murder in the same sense of his different realities of the Iron on a man who sets in the fores of modern desires and the best friend for a series of sacrifices and becomes the comedy of his wife and his friends as a series of suspects on the enemy as they get the end of the story of the forest to the adventure of the pool tough and destruction of a small town i\n",
      "--------------------\n",
      "\n",
      "The Movie: When her real son of a surface town who prepares to a top secret agent who dies on her and the rest of the cold of the rest of the paraples of the family and the crown of his adoptive story.\n",
      "The Next Evil: The film is a seemingly man he knows all on the chance of the parents of President Santa – a child who proves them into a story of missing mansion to a small town to save the world from the series of a wise to the deadly disappearance and professional of the streets of the world. Wh\n",
      "--------------------\n",
      "\n",
      "Martican: Laura finds his fans of the series of a child while a beautiful cousin series supply in a monster for Jewie High sets out on a drug love story of drug worlds and becomes the trip in the stealing of the acest of Missipproof to this templitation and the people in the beautiful and a market destiny survivor and his dream of crimes. The only one of the most powerful van world in a world that allowed him to find himself as a miner spirit to an adventure prison program who is the story of th\n",
      "--------------------\n",
      "\n",
      "The Battle of the Manhattan: The arena of Santa in the world of Tang Britty, a suburban realizes that he many will in the fine way they are married and in the rest of the Tokyo. And the girl that is suspected to his own daughter and his eventually failed and professors and the companions of the mysterious about the soul of a small town to a bizarre companion who are the institution of the marriage behind a ruthless present in the season of a quest town of fools and the stages of a student compan\n",
      "--------------------\n",
      "\n",
      "The Thirst: A perfect world world and researcher living his family man sets out on a cursed woman who never comes to help a self-adventure and reveal the personality and a man who makes it in destruction to find a rest of the greatest plan to save his late.\n",
      "The Next Charlie: A story of the most influence of the game and protection, in a small with a salesman of fighters as the part of the stories of a powerful brauthry rising to find a man who convent the most dangerous and a police and produces\n",
      "--------------------\n",
      "\n",
      " Temperature: 0.1\n",
      "Number of parameters in GPT: 0.83M\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A man who was a haunted hero who wants to survive a series of man who wants to save himself and the family of the past of the most powerful and the country and settle in the way of the seasons of the most powerful and the country of the country and the country of the story of a series of survival and the streets of the most survivors of the survivors of a man who has sent to the story of the most survivors of a small town of the streets of the most survival of a small town o\n",
      "--------------------\n",
      "\n",
      "The Book of Story: A story of a series of survival and a series of man who wants to save the world from a series of sexual and secrets and the country of the seasons of the most powerful and the story of a series of movies and the most powerful and the country of the survivors of a mysterious prisoner who wants to save the most survivors of a small town of the survivors of a series of survival and the most powerful powerfull of the seasons of a man who works and a series of survival and sends hi\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A story of a series of modern friends and a series of movies and the most survivors of a small town and the story of the soul of a street story of a mysterious and the country of a small town of the seasons of the survivors of the station of a series of friendship with a former serves as a series of survival and the story of a mysterious partner who was a series of friendship with a series of survival and the country of a small town of the streets of the survivors of a man w\n",
      "--------------------\n",
      "\n",
      "The Battle of the Man: A man who has been a series of supernatural powers and the story of a series of sexual experiences and the country of a small town of the streets of the state and the country of the story of a series of survivors and a series of man who wants to find a way to save the world of the most survivors of the story of a series of survival and the country for the world of the story of a family man who wants to find a way to save the world of the past of the most famous and the sto\n",
      "--------------------\n",
      "\n",
      "The Secret of the Man: A struggling to stop a secret status at a season of the most powerful powers and the country of a small town of the country and settle in the story of a series of survival and the country of the story of the story of a small town of the streets of the seasons of the most survivors of the station of the most survival of the death of his father's life.\n",
      "The Book of the Man: A story of the most survivors of a small town of the streets of the most son of a man who was a prisone\n",
      "--------------------\n",
      "\n",
      " Temperature: 0.0001\n",
      "Number of parameters in GPT: 0.83M\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A story of a series of sex and a man who was a band of survivors and the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and t\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A story of a series of sex and a man who was a band of survivors and the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and t\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A story of a series of sex and a man who was a band of survivors and the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and t\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A story of a series of sex and a man who was a band of survivors and the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and t\n",
      "--------------------\n",
      "\n",
      "The Book of the Man: A story of a series of sex and a man who was a band of survivors and the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and the country of the story of a series of survival and t\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "num_samples = 5  # Number of samples to draw\n",
    "max_new_tokens = 500  # Number of tokens generated in each sample\n",
    "temperatures = [1.5,1.0,0.8,0.5,0.1,0.0001]  # TODO: Use different temperature values and qualitatively report on the results\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n Temperature: {temp}\")\n",
    "    # Set seed for reproducibility\n",
    "    seed = 345\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Load the model\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['model_args']\n",
    "    model = GPT(config)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Create dataset to get encoder/decoder\n",
    "    dataset = MovieDataset(block_size=config.block_size)\n",
    "    encode = lambda s: [dataset.string_to_int[c] for c in s]\n",
    "    decode = dataset.decode\n",
    "\n",
    "    # Generate samples\n",
    "    print('-'*20)\n",
    "    with torch.no_grad():\n",
    "        for k in range(num_samples):\n",
    "            start_prompt = \"\\n\"  # Start prompt\n",
    "            prompt_ids = encode(start_prompt)\n",
    "            x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "            y = model.sample(x, max_new_tokens, temperature=temp)\n",
    "            print(decode(y[0]))\n",
    "            print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "TpQWtzSJ0YeZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pd37kc2qTenl"
   },
   "source": [
    "### a) Analyzing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83rOk125Tenl"
   },
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "dataset = MovieDataset()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JVT7SSedTenm",
    "ExecuteTime": {
     "end_time": "2025-01-10T19:08:25.230213Z",
     "start_time": "2025-01-10T19:08:21.419485Z"
    }
   },
   "source": [
    "#extracting samples\n",
    "dataset = MovieDataset()\n",
    "print(f\"Size of dataset: {len(dataset)}\")\n",
    "print(f\"Size of contained items: {len(dataset[0])}\")\n",
    "print(f\"Sample size: {len(dataset[0][0])}\")\n",
    "\n",
    "seed = 345\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "offset= torch.randint(0, len(dataset), (1,))+92791\n",
    "\n",
    "#print out some random samples\n",
    "for chosen_index in [offset, offset+1, offset+2]:\n",
    "\n",
    "    sample = dataset.__getitem__(chosen_index)\n",
    "    numbers_1 = ', '.join([str(x.item()) for x in sample[0]])\n",
    "    print(numbers_1)\n",
    "    print(dataset.decode(sample[0]))\n",
    "    # sentence_1 = [dataset.int_to_string[x.item()] for x in sample[0]]\n",
    "    # print(\"\".join(sentence_1))\n",
    "\n",
    "    numbers_2 = ', '.join([str(x.item()) for x in sample[0]])\n",
    "    print(numbers_2)\n",
    "    print(dataset.decode(sample[1]))\n",
    "    # sentence_2 = [dataset.int_to_string[x.item()] for x in sample[1]]\n",
    "    # print(\"\".join(sentence_2))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 3001845\n",
      "Size of contained items: 2\n",
      "Sample size: 256\n",
      "68, 70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80\n",
      "giant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species\n",
      "68, 70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80\n",
      "iant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species \n",
      "70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2\n",
      "iant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species \n",
      "70, 62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2\n",
      "ant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species o\n",
      "62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2, 76\n",
      "ant werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species o\n",
      "62, 75, 81, 2, 84, 66, 79, 66, 84, 76, 73, 67, 14, 2, 62, 75, 65, 2, 66, 83, 66, 75, 2, 81, 69, 66, 2, 67, 66, 79, 76, 64, 70, 76, 82, 80, 2, 68, 76, 65, 65, 66, 80, 80, 2, 39, 66, 64, 62, 81, 66, 2, 69, 66, 79, 80, 66, 73, 67, 16, 1, 44, 70, 74, 70, 64, 2, 20, 28, 2, 54, 69, 66, 75, 2, 62, 2, 64, 76, 64, 72, 79, 76, 62, 64, 69, 15, 80, 77, 79, 66, 62, 65, 2, 77, 73, 62, 68, 82, 66, 2, 81, 69, 79, 66, 62, 81, 66, 75, 66, 65, 2, 81, 76, 2, 65, 66, 64, 70, 74, 62, 81, 66, 2, 81, 69, 66, 2, 64, 69, 70, 73, 65, 2, 77, 76, 77, 82, 73, 62, 81, 70, 76, 75, 2, 76, 67, 2, 45, 66, 84, 2, 56, 76, 79, 72, 2, 34, 70, 81, 86, 2, 70, 75, 2, 81, 69, 66, 2, 76, 79, 70, 68, 70, 75, 62, 73, 2, 44, 70, 74, 70, 64, 14, 2, 63, 70, 76, 73, 76, 68, 70, 80, 81, 2, 50, 82, 80, 62, 75, 2, 51, 86, 73, 66, 79, 2, 62, 75, 65, 2, 69, 66, 79, 2, 79, 66, 80, 66, 62, 79, 64, 69, 2, 62, 80, 80, 76, 64, 70, 62, 81, 66, 80, 2, 65, 66, 83, 66, 73, 76, 77, 66, 65, 2, 62, 2, 80, 77, 66, 64, 70, 66, 80, 2, 76\n",
      "nt werewolf, and even the ferocious goddess Hecate herself.\n",
      "Mimic 2: When a cockroach-spread plague threatened to decimate the child population of New York City in the original Mimic, biologist Susan Tyler and her research associates developed a species of\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# h) Hyperparametertuning with different models"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oItCPMqeTenm",
    "ExecuteTime": {
     "end_time": "2025-01-10T17:21:06.170216Z",
     "start_time": "2025-01-10T07:22:24.957391Z"
    }
   },
   "source": [
    "# Set seeds for reproducibility\n",
    "seed= 345\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "block_size = 256\n",
    "batch_size = 128\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "data = MovieDataset(block_size)\n",
    "\n",
    "# split into train and validation sets\n",
    "train_len = int(len(data) * 0.8)\n",
    "val_len = len(data) - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "out_dir = 'MovieGPT'\n",
    "checkpoint_path = os.path.join(out_dir, 'checkpoint_h.pt')\n",
    "os.makedirs(out_dir, exist_ok=True)  # Create output directory\n",
    "\n",
    "# Eval/Logging\n",
    "val_interval = 500 # Number of iterations between evaluations\n",
    "val_iters = 20 # Number of iterations for evaluation\n",
    "log_interval = 10 # Number of iterations between logging\n",
    "\n",
    "# Optimizer settings\n",
    "learning_rates = [2e-4, 1e-3] # todo Larger networks typically require a learning rate that is smaller than this\n",
    "max_iters = 5001 # todo Number of iterations to train for\n",
    "weight_decay = [0, 1e-2] # Weight decay for regularization (on the weights/embeddings)\n",
    "beta1, beta2 = 0.9, 0.99 # Beta1, Beta2 for AdamW optimizer\n",
    "grad_clips = [1.0, 0.0] # Clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# Compile model\n",
    "compile_model = False # Compile the model for faster execution\n",
    "\n",
    "# Model config\n",
    "vocab_size = len(data.int_to_string) # TODO: Use the dataset `data` to determine the vocabulary size\n",
    "config_1 = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_block=8,\n",
    "    n_head=8,\n",
    "    n_embd=256,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ") # This is a relatively small model\n",
    "\n",
    "\n",
    "config_2 = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_block=12,\n",
    "    n_head=12,\n",
    "    n_embd=360,\n",
    "    dropout=0.2,\n",
    "    bias=True\n",
    ")\n",
    "config_list = [config_2, config_1]\n",
    "for num, config in enumerate(config_list):\n",
    "    # model = GPT(config_1).to(device)\n",
    "\n",
    "    path = f\"MovieGPT/point_h_model{num}.pt\"\n",
    "    if compile_model:\n",
    "        print(\"Compiling the model...\")\n",
    "        model = torch.compile(model) # Needs PyTorch >= 2.0\n",
    "        print(\"Done compiling\")\n",
    "\n",
    "    #todo removed outside to only save the best hyperparameter combination of each model\n",
    "    best_val_loss = float('inf')\n",
    "    # Initialize optimizer\n",
    "    for weight in weight_decay:\n",
    "        for learning_rate in learning_rates:\n",
    "            for grad_clip in grad_clips:\n",
    "                model = GPT(config).to(device)\n",
    "                optimizer = model.get_optimizer(weight, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "                # Training loop\n",
    "                iter_num = 0\n",
    "\n",
    "                all_losses_train = []\n",
    "                all_losses_test = []\n",
    "                iteration_vec = []\n",
    "\n",
    "                for _ in range(max_iters):\n",
    "                    for X, Y in train_loader:\n",
    "                        # Get batch and move to device\n",
    "                        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "                        # Forward pass\n",
    "                        logits, loss = model(X, targets=Y)\n",
    "\n",
    "                        # Backward pass\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        loss.backward()\n",
    "                        if grad_clip != 0.0:\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # Logging\n",
    "                        if iter_num % log_interval == 0:\n",
    "                            print(f\"iter {iter_num}: loss {loss.item():.4f}\")\n",
    "\n",
    "                        # Evaluation\n",
    "                        if iter_num % val_interval == 0:\n",
    "                            losses = estimate_train_val_loss(model, train_loader, val_loader, val_iters, device)\n",
    "                            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                            all_losses_train.append(losses['train'])\n",
    "                            all_losses_test.append(losses['val'])\n",
    "                            iteration_vec.append(iter_num)\n",
    "                            # Save best model\n",
    "                            if losses['val'] < best_val_loss:\n",
    "                                best_val_loss = losses['val']\n",
    "                                if iter_num > 0:\n",
    "                                    print(f\"Saving checkpoint to {out_dir} -------------------\")\n",
    "                                    model_to_save = model._orig_mod if compile_model else model\n",
    "                                    torch.save({\n",
    "                                        'model': model_to_save.state_dict(),\n",
    "                                        'model_args': config,\n",
    "                                        'optimizer_lr': learning_rate,\n",
    "                                        'optimizer_wd': weight,\n",
    "                                        'optimizer_gc': grad_clip\n",
    "                                    }, path)\n",
    "\n",
    "                        iter_num += 1\n",
    "                        if iter_num >= max_iters:\n",
    "                            break\n",
    "\n",
    "                    if iter_num >= max_iters:\n",
    "                        break\n",
    "                print(\"training done\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.0729\n",
      "step 0: train loss 4.0822, val loss 4.0820\n",
      "iter 10: loss 3.3661\n",
      "iter 20: loss 3.0443\n",
      "iter 30: loss 2.8535\n",
      "iter 40: loss 2.7270\n",
      "iter 50: loss 2.6667\n",
      "iter 60: loss 2.6187\n",
      "iter 70: loss 2.5891\n",
      "iter 80: loss 2.5775\n",
      "iter 90: loss 2.5520\n",
      "iter 100: loss 2.5388\n",
      "iter 110: loss 2.5319\n",
      "iter 120: loss 2.5303\n",
      "iter 130: loss 2.5209\n",
      "iter 140: loss 2.5055\n",
      "iter 150: loss 2.5045\n",
      "iter 160: loss 2.5085\n",
      "iter 170: loss 2.4867\n",
      "iter 180: loss 2.4850\n",
      "iter 190: loss 2.4778\n",
      "iter 200: loss 2.5012\n",
      "iter 210: loss 2.4907\n",
      "iter 220: loss 2.4716\n",
      "iter 230: loss 2.4512\n",
      "iter 240: loss 2.4691\n",
      "iter 250: loss 2.4442\n",
      "iter 260: loss 2.4312\n",
      "iter 270: loss 2.4117\n",
      "iter 280: loss 2.3997\n",
      "iter 290: loss 2.3988\n",
      "iter 300: loss 2.3786\n",
      "iter 310: loss 2.3928\n",
      "iter 320: loss 2.3438\n",
      "iter 330: loss 2.3376\n",
      "iter 340: loss 2.3192\n",
      "iter 350: loss 2.3161\n",
      "iter 360: loss 2.2987\n",
      "iter 370: loss 2.2678\n",
      "iter 380: loss 2.2422\n",
      "iter 390: loss 2.2206\n",
      "iter 400: loss 2.2165\n",
      "iter 410: loss 2.1831\n",
      "iter 420: loss 2.1721\n",
      "iter 430: loss 2.1266\n",
      "iter 440: loss 2.1359\n",
      "iter 450: loss 2.1221\n",
      "iter 460: loss 2.0876\n",
      "iter 470: loss 2.0644\n",
      "iter 480: loss 2.0775\n",
      "iter 490: loss 2.0389\n",
      "iter 500: loss 2.0304\n",
      "step 500: train loss 1.9747, val loss 1.9703\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 510: loss 2.0211\n",
      "iter 520: loss 1.9865\n",
      "iter 530: loss 1.9652\n",
      "iter 540: loss 1.9661\n",
      "iter 550: loss 1.9543\n",
      "iter 560: loss 1.9694\n",
      "iter 570: loss 1.9468\n",
      "iter 580: loss 1.9251\n",
      "iter 590: loss 1.9223\n",
      "iter 600: loss 1.9046\n",
      "iter 610: loss 1.8824\n",
      "iter 620: loss 1.8746\n",
      "iter 630: loss 1.8563\n",
      "iter 640: loss 1.8462\n",
      "iter 650: loss 1.8615\n",
      "iter 660: loss 1.8429\n",
      "iter 670: loss 1.8336\n",
      "iter 680: loss 1.8128\n",
      "iter 690: loss 1.8223\n",
      "iter 700: loss 1.7851\n",
      "iter 710: loss 1.7760\n",
      "iter 720: loss 1.7927\n",
      "iter 730: loss 1.7867\n",
      "iter 740: loss 1.7672\n",
      "iter 750: loss 1.7244\n",
      "iter 760: loss 1.7610\n",
      "iter 770: loss 1.7267\n",
      "iter 780: loss 1.6956\n",
      "iter 790: loss 1.7193\n",
      "iter 800: loss 1.7177\n",
      "iter 810: loss 1.7036\n",
      "iter 820: loss 1.6987\n",
      "iter 830: loss 1.6965\n",
      "iter 840: loss 1.6643\n",
      "iter 850: loss 1.6784\n",
      "iter 860: loss 1.6600\n",
      "iter 870: loss 1.6680\n",
      "iter 880: loss 1.6268\n",
      "iter 890: loss 1.6751\n",
      "iter 900: loss 1.6345\n",
      "iter 910: loss 1.6452\n",
      "iter 920: loss 1.6372\n",
      "iter 930: loss 1.6239\n",
      "iter 940: loss 1.6346\n",
      "iter 950: loss 1.6421\n",
      "iter 960: loss 1.6276\n",
      "iter 970: loss 1.5978\n",
      "iter 980: loss 1.5885\n",
      "iter 990: loss 1.5764\n",
      "iter 1000: loss 1.6028\n",
      "step 1000: train loss 1.5236, val loss 1.5226\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 1010: loss 1.5672\n",
      "iter 1020: loss 1.5924\n",
      "iter 1030: loss 1.5724\n",
      "iter 1040: loss 1.5839\n",
      "iter 1050: loss 1.5659\n",
      "iter 1060: loss 1.5758\n",
      "iter 1070: loss 1.5639\n",
      "iter 1080: loss 1.5880\n",
      "iter 1090: loss 1.5705\n",
      "iter 1100: loss 1.5436\n",
      "iter 1110: loss 1.5360\n",
      "iter 1120: loss 1.5360\n",
      "iter 1130: loss 1.5475\n",
      "iter 1140: loss 1.5580\n",
      "iter 1150: loss 1.5199\n",
      "iter 1160: loss 1.5158\n",
      "iter 1170: loss 1.5020\n",
      "iter 1180: loss 1.5200\n",
      "iter 1190: loss 1.5195\n",
      "iter 1200: loss 1.5378\n",
      "iter 1210: loss 1.5149\n",
      "iter 1220: loss 1.4886\n",
      "iter 1230: loss 1.4941\n",
      "iter 1240: loss 1.4820\n",
      "iter 1250: loss 1.4832\n",
      "iter 1260: loss 1.4807\n",
      "iter 1270: loss 1.4529\n",
      "iter 1280: loss 1.4910\n",
      "iter 1290: loss 1.4724\n",
      "iter 1300: loss 1.4693\n",
      "iter 1310: loss 1.4865\n",
      "iter 1320: loss 1.4627\n",
      "iter 1330: loss 1.4472\n",
      "iter 1340: loss 1.4551\n",
      "iter 1350: loss 1.4654\n",
      "iter 1360: loss 1.4445\n",
      "iter 1370: loss 1.4400\n",
      "iter 1380: loss 1.4558\n",
      "iter 1390: loss 1.4369\n",
      "iter 1400: loss 1.4448\n",
      "iter 1410: loss 1.4269\n",
      "iter 1420: loss 1.4504\n",
      "iter 1430: loss 1.4241\n",
      "iter 1440: loss 1.4389\n",
      "iter 1450: loss 1.4271\n",
      "iter 1460: loss 1.4110\n",
      "iter 1470: loss 1.4341\n",
      "iter 1480: loss 1.4176\n",
      "iter 1490: loss 1.4246\n",
      "iter 1500: loss 1.4015\n",
      "step 1500: train loss 1.3601, val loss 1.3586\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 1510: loss 1.4065\n",
      "iter 1520: loss 1.4248\n",
      "iter 1530: loss 1.4032\n",
      "iter 1540: loss 1.4115\n",
      "iter 1550: loss 1.4021\n",
      "iter 1560: loss 1.4089\n",
      "iter 1570: loss 1.4068\n",
      "iter 1580: loss 1.4169\n",
      "iter 1590: loss 1.3887\n",
      "iter 1600: loss 1.3963\n",
      "iter 1610: loss 1.3785\n",
      "iter 1620: loss 1.3929\n",
      "iter 1630: loss 1.3885\n",
      "iter 1640: loss 1.3916\n",
      "iter 1650: loss 1.3615\n",
      "iter 1660: loss 1.3726\n",
      "iter 1670: loss 1.3594\n",
      "iter 1680: loss 1.3778\n",
      "iter 1690: loss 1.3640\n",
      "iter 1700: loss 1.3872\n",
      "iter 1710: loss 1.3631\n",
      "iter 1720: loss 1.3556\n",
      "iter 1730: loss 1.3689\n",
      "iter 1740: loss 1.3792\n",
      "iter 1750: loss 1.3580\n",
      "iter 1760: loss 1.3463\n",
      "iter 1770: loss 1.3510\n",
      "iter 1780: loss 1.3356\n",
      "iter 1790: loss 1.3580\n",
      "iter 1800: loss 1.3343\n",
      "iter 1810: loss 1.3376\n",
      "iter 1820: loss 1.3418\n",
      "iter 1830: loss 1.3314\n",
      "iter 1840: loss 1.3544\n",
      "iter 1850: loss 1.3501\n",
      "iter 1860: loss 1.3347\n",
      "iter 1870: loss 1.3263\n",
      "iter 1880: loss 1.3600\n",
      "iter 1890: loss 1.3395\n",
      "iter 1900: loss 1.3134\n",
      "iter 1910: loss 1.3170\n",
      "iter 1920: loss 1.3117\n",
      "iter 1930: loss 1.3069\n",
      "iter 1940: loss 1.3212\n",
      "iter 1950: loss 1.3354\n",
      "iter 1960: loss 1.3057\n",
      "iter 1970: loss 1.3328\n",
      "iter 1980: loss 1.3246\n",
      "iter 1990: loss 1.2968\n",
      "iter 2000: loss 1.2927\n",
      "step 2000: train loss 1.2589, val loss 1.2634\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 2010: loss 1.3008\n",
      "iter 2020: loss 1.2914\n",
      "iter 2030: loss 1.2945\n",
      "iter 2040: loss 1.3170\n",
      "iter 2050: loss 1.3034\n",
      "iter 2060: loss 1.3202\n",
      "iter 2070: loss 1.3032\n",
      "iter 2080: loss 1.2789\n",
      "iter 2090: loss 1.2939\n",
      "iter 2100: loss 1.3014\n",
      "iter 2110: loss 1.2838\n",
      "iter 2120: loss 1.2850\n",
      "iter 2130: loss 1.2859\n",
      "iter 2140: loss 1.3039\n",
      "iter 2150: loss 1.2973\n",
      "iter 2160: loss 1.2863\n",
      "iter 2170: loss 1.2787\n",
      "iter 2180: loss 1.2904\n",
      "iter 2190: loss 1.2908\n",
      "iter 2200: loss 1.3010\n",
      "iter 2210: loss 1.2755\n",
      "iter 2220: loss 1.2838\n",
      "iter 2230: loss 1.2774\n",
      "iter 2240: loss 1.2740\n",
      "iter 2250: loss 1.2708\n",
      "iter 2260: loss 1.2599\n",
      "iter 2270: loss 1.2816\n",
      "iter 2280: loss 1.2900\n",
      "iter 2290: loss 1.2725\n",
      "iter 2300: loss 1.2541\n",
      "iter 2310: loss 1.2798\n",
      "iter 2320: loss 1.2743\n",
      "iter 2330: loss 1.2785\n",
      "iter 2340: loss 1.2737\n",
      "iter 2350: loss 1.2778\n",
      "iter 2360: loss 1.2640\n",
      "iter 2370: loss 1.2681\n",
      "iter 2380: loss 1.2475\n",
      "iter 2390: loss 1.2752\n",
      "iter 2400: loss 1.2619\n",
      "iter 2410: loss 1.2590\n",
      "iter 2420: loss 1.2459\n",
      "iter 2430: loss 1.2761\n",
      "iter 2440: loss 1.2608\n",
      "iter 2450: loss 1.2614\n",
      "iter 2460: loss 1.2514\n",
      "iter 2470: loss 1.2577\n",
      "iter 2480: loss 1.2550\n",
      "iter 2490: loss 1.2458\n",
      "iter 2500: loss 1.2372\n",
      "step 2500: train loss 1.1883, val loss 1.1901\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 2510: loss 1.2363\n",
      "iter 2520: loss 1.2602\n",
      "iter 2530: loss 1.2658\n",
      "iter 2540: loss 1.2374\n",
      "iter 2550: loss 1.2260\n",
      "iter 2560: loss 1.2308\n",
      "iter 2570: loss 1.2661\n",
      "iter 2580: loss 1.2190\n",
      "iter 2590: loss 1.2351\n",
      "iter 2600: loss 1.2353\n",
      "iter 2610: loss 1.2378\n",
      "iter 2620: loss 1.2294\n",
      "iter 2630: loss 1.2237\n",
      "iter 2640: loss 1.2322\n",
      "iter 2650: loss 1.2201\n",
      "iter 2660: loss 1.1983\n",
      "iter 2670: loss 1.2242\n",
      "iter 2680: loss 1.2360\n",
      "iter 2690: loss 1.1991\n",
      "iter 2700: loss 1.2353\n",
      "iter 2710: loss 1.2073\n",
      "iter 2720: loss 1.2139\n",
      "iter 2730: loss 1.2339\n",
      "iter 2740: loss 1.2329\n",
      "iter 2750: loss 1.2129\n",
      "iter 2760: loss 1.2026\n",
      "iter 2770: loss 1.2027\n",
      "iter 2780: loss 1.2028\n",
      "iter 2790: loss 1.2129\n",
      "iter 2800: loss 1.2405\n",
      "iter 2810: loss 1.2316\n",
      "iter 2820: loss 1.2052\n",
      "iter 2830: loss 1.1953\n",
      "iter 2840: loss 1.1921\n",
      "iter 2850: loss 1.2131\n",
      "iter 2860: loss 1.2105\n",
      "iter 2870: loss 1.2091\n",
      "iter 2880: loss 1.2100\n",
      "iter 2890: loss 1.1936\n",
      "iter 2900: loss 1.1982\n",
      "iter 2910: loss 1.1786\n",
      "iter 2920: loss 1.1880\n",
      "iter 2930: loss 1.1964\n",
      "iter 2940: loss 1.2039\n",
      "iter 2950: loss 1.1942\n",
      "iter 2960: loss 1.2198\n",
      "iter 2970: loss 1.1890\n",
      "iter 2980: loss 1.1857\n",
      "iter 2990: loss 1.1976\n",
      "iter 3000: loss 1.1900\n",
      "step 3000: train loss 1.1339, val loss 1.1335\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3010: loss 1.2011\n",
      "iter 3020: loss 1.1859\n",
      "iter 3030: loss 1.1695\n",
      "iter 3040: loss 1.1769\n",
      "iter 3050: loss 1.1666\n",
      "iter 3060: loss 1.2009\n",
      "iter 3070: loss 1.1868\n",
      "iter 3080: loss 1.1733\n",
      "iter 3090: loss 1.1784\n",
      "iter 3100: loss 1.1757\n",
      "iter 3110: loss 1.1734\n",
      "iter 3120: loss 1.1829\n",
      "iter 3130: loss 1.1820\n",
      "iter 3140: loss 1.1788\n",
      "iter 3150: loss 1.1685\n",
      "iter 3160: loss 1.1785\n",
      "iter 3170: loss 1.1841\n",
      "iter 3180: loss 1.1698\n",
      "iter 3190: loss 1.1760\n",
      "iter 3200: loss 1.1747\n",
      "iter 3210: loss 1.1822\n",
      "iter 3220: loss 1.1670\n",
      "iter 3230: loss 1.1812\n",
      "iter 3240: loss 1.1776\n",
      "iter 3250: loss 1.1769\n",
      "iter 3260: loss 1.1566\n",
      "iter 3270: loss 1.1554\n",
      "iter 3280: loss 1.1598\n",
      "iter 3290: loss 1.1523\n",
      "iter 3300: loss 1.1708\n",
      "iter 3310: loss 1.1683\n",
      "iter 3320: loss 1.1761\n",
      "iter 3330: loss 1.1427\n",
      "iter 3340: loss 1.1740\n",
      "iter 3350: loss 1.1686\n",
      "iter 3360: loss 1.1490\n",
      "iter 3370: loss 1.1673\n",
      "iter 3380: loss 1.1585\n",
      "iter 3390: loss 1.1547\n",
      "iter 3400: loss 1.1561\n",
      "iter 3410: loss 1.1587\n",
      "iter 3420: loss 1.1448\n",
      "iter 3430: loss 1.1739\n",
      "iter 3440: loss 1.1571\n",
      "iter 3450: loss 1.1408\n",
      "iter 3460: loss 1.1467\n",
      "iter 3470: loss 1.1482\n",
      "iter 3480: loss 1.1405\n",
      "iter 3490: loss 1.1345\n",
      "iter 3500: loss 1.1365\n",
      "step 3500: train loss 1.0818, val loss 1.0804\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3510: loss 1.1377\n",
      "iter 3520: loss 1.1389\n",
      "iter 3530: loss 1.1472\n",
      "iter 3540: loss 1.1285\n",
      "iter 3550: loss 1.1341\n",
      "iter 3560: loss 1.1216\n",
      "iter 3570: loss 1.1492\n",
      "iter 3580: loss 1.1478\n",
      "iter 3590: loss 1.1426\n",
      "iter 3600: loss 1.1069\n",
      "iter 3610: loss 1.1433\n",
      "iter 3620: loss 1.1173\n",
      "iter 3630: loss 1.1266\n",
      "iter 3640: loss 1.1587\n",
      "iter 3650: loss 1.1290\n",
      "iter 3660: loss 1.1228\n",
      "iter 3670: loss 1.1269\n",
      "iter 3680: loss 1.1387\n",
      "iter 3690: loss 1.1407\n",
      "iter 3700: loss 1.1258\n",
      "iter 3710: loss 1.1385\n",
      "iter 3720: loss 1.1127\n",
      "iter 3730: loss 1.1170\n",
      "iter 3740: loss 1.1241\n",
      "iter 3750: loss 1.1240\n",
      "iter 3760: loss 1.1332\n",
      "iter 3770: loss 1.1235\n",
      "iter 3780: loss 1.1379\n",
      "iter 3790: loss 1.1234\n",
      "iter 3800: loss 1.1187\n",
      "iter 3810: loss 1.1065\n",
      "iter 3820: loss 1.1055\n",
      "iter 3830: loss 1.1251\n",
      "iter 3840: loss 1.1220\n",
      "iter 3850: loss 1.1142\n",
      "iter 3860: loss 1.0964\n",
      "iter 3870: loss 1.1126\n",
      "iter 3880: loss 1.1032\n",
      "iter 3890: loss 1.1220\n",
      "iter 3900: loss 1.1016\n",
      "iter 3910: loss 1.1289\n",
      "iter 3920: loss 1.1051\n",
      "iter 3930: loss 1.1111\n",
      "iter 3940: loss 1.1062\n",
      "iter 3950: loss 1.1110\n",
      "iter 3960: loss 1.0991\n",
      "iter 3970: loss 1.0973\n",
      "iter 3980: loss 1.1169\n",
      "iter 3990: loss 1.0995\n",
      "iter 4000: loss 1.1014\n",
      "step 4000: train loss 1.0270, val loss 1.0330\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4010: loss 1.1054\n",
      "iter 4020: loss 1.1013\n",
      "iter 4030: loss 1.0995\n",
      "iter 4040: loss 1.1058\n",
      "iter 4050: loss 1.1042\n",
      "iter 4060: loss 1.0911\n",
      "iter 4070: loss 1.0854\n",
      "iter 4080: loss 1.0810\n",
      "iter 4090: loss 1.0787\n",
      "iter 4100: loss 1.0944\n",
      "iter 4110: loss 1.1030\n",
      "iter 4120: loss 1.0943\n",
      "iter 4130: loss 1.0823\n",
      "iter 4140: loss 1.0906\n",
      "iter 4150: loss 1.1032\n",
      "iter 4160: loss 1.0995\n",
      "iter 4170: loss 1.0793\n",
      "iter 4180: loss 1.0855\n",
      "iter 4190: loss 1.0644\n",
      "iter 4200: loss 1.0801\n",
      "iter 4210: loss 1.1028\n",
      "iter 4220: loss 1.1045\n",
      "iter 4230: loss 1.0729\n",
      "iter 4240: loss 1.0621\n",
      "iter 4250: loss 1.0732\n",
      "iter 4260: loss 1.0788\n",
      "iter 4270: loss 1.0733\n",
      "iter 4280: loss 1.0916\n",
      "iter 4290: loss 1.0665\n",
      "iter 4300: loss 1.0696\n",
      "iter 4310: loss 1.0890\n",
      "iter 4320: loss 1.0778\n",
      "iter 4330: loss 1.0736\n",
      "iter 4340: loss 1.0728\n",
      "iter 4350: loss 1.0646\n",
      "iter 4360: loss 1.0809\n",
      "iter 4370: loss 1.0859\n",
      "iter 4380: loss 1.0866\n",
      "iter 4390: loss 1.0558\n",
      "iter 4400: loss 1.0810\n",
      "iter 4410: loss 1.0686\n",
      "iter 4420: loss 1.0476\n",
      "iter 4430: loss 1.0671\n",
      "iter 4440: loss 1.0564\n",
      "iter 4450: loss 1.0668\n",
      "iter 4460: loss 1.0825\n",
      "iter 4470: loss 1.0781\n",
      "iter 4480: loss 1.0602\n",
      "iter 4490: loss 1.0880\n",
      "iter 4500: loss 1.0766\n",
      "step 4500: train loss 0.9850, val loss 0.9874\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4510: loss 1.0457\n",
      "iter 4520: loss 1.0787\n",
      "iter 4530: loss 1.0632\n",
      "iter 4540: loss 1.0446\n",
      "iter 4550: loss 1.0473\n",
      "iter 4560: loss 1.0403\n",
      "iter 4570: loss 1.0688\n",
      "iter 4580: loss 1.0704\n",
      "iter 4590: loss 1.0407\n",
      "iter 4600: loss 1.0369\n",
      "iter 4610: loss 1.0479\n",
      "iter 4620: loss 1.0612\n",
      "iter 4630: loss 1.0706\n",
      "iter 4640: loss 1.0557\n",
      "iter 4650: loss 1.0401\n",
      "iter 4660: loss 1.0520\n",
      "iter 4670: loss 1.0462\n",
      "iter 4680: loss 1.0443\n",
      "iter 4690: loss 1.0344\n",
      "iter 4700: loss 1.0376\n",
      "iter 4710: loss 1.0394\n",
      "iter 4720: loss 1.0400\n",
      "iter 4730: loss 1.0459\n",
      "iter 4740: loss 1.0421\n",
      "iter 4750: loss 1.0557\n",
      "iter 4760: loss 1.0447\n",
      "iter 4770: loss 1.0642\n",
      "iter 4780: loss 1.0488\n",
      "iter 4790: loss 1.0444\n",
      "iter 4800: loss 1.0364\n",
      "iter 4810: loss 1.0545\n",
      "iter 4820: loss 1.0402\n",
      "iter 4830: loss 1.0358\n",
      "iter 4840: loss 1.0346\n",
      "iter 4850: loss 1.0331\n",
      "iter 4860: loss 1.0484\n",
      "iter 4870: loss 1.0362\n",
      "iter 4880: loss 1.0367\n",
      "iter 4890: loss 1.0372\n",
      "iter 4900: loss 1.0173\n",
      "iter 4910: loss 1.0248\n",
      "iter 4920: loss 1.0471\n",
      "iter 4930: loss 1.0399\n",
      "iter 4940: loss 1.0288\n",
      "iter 4950: loss 1.0073\n",
      "iter 4960: loss 1.0351\n",
      "iter 4970: loss 1.0331\n",
      "iter 4980: loss 1.0358\n",
      "iter 4990: loss 1.0179\n",
      "iter 5000: loss 1.0370\n",
      "step 5000: train loss 0.9401, val loss 0.9414\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.3139\n",
      "step 0: train loss 4.1583, val loss 4.1600\n",
      "iter 10: loss 3.4430\n",
      "iter 20: loss 3.2096\n",
      "iter 30: loss 3.0905\n",
      "iter 40: loss 3.0551\n",
      "iter 50: loss 2.9892\n",
      "iter 60: loss 2.9295\n",
      "iter 70: loss 2.8549\n",
      "iter 80: loss 2.8188\n",
      "iter 90: loss 2.7812\n",
      "iter 100: loss 2.7480\n",
      "iter 110: loss 2.7233\n",
      "iter 120: loss 2.6922\n",
      "iter 130: loss 2.6947\n",
      "iter 140: loss 2.6556\n",
      "iter 150: loss 2.6339\n",
      "iter 160: loss 2.6343\n",
      "iter 170: loss 2.6217\n",
      "iter 180: loss 2.5939\n",
      "iter 190: loss 2.6098\n",
      "iter 200: loss 2.5847\n",
      "iter 210: loss 2.5676\n",
      "iter 220: loss 2.5795\n",
      "iter 230: loss 2.5539\n",
      "iter 240: loss 2.5559\n",
      "iter 250: loss 2.5331\n",
      "iter 260: loss 2.5519\n",
      "iter 270: loss 2.5418\n",
      "iter 280: loss 2.5174\n",
      "iter 290: loss 2.5157\n",
      "iter 300: loss 2.5159\n",
      "iter 310: loss 2.5262\n",
      "iter 320: loss 2.5091\n",
      "iter 330: loss 2.4967\n",
      "iter 340: loss 2.5099\n",
      "iter 350: loss 2.5064\n",
      "iter 360: loss 2.4944\n",
      "iter 370: loss 2.4744\n",
      "iter 380: loss 2.4665\n",
      "iter 390: loss 2.4545\n",
      "iter 400: loss 2.4371\n",
      "iter 410: loss 2.4268\n",
      "iter 420: loss 2.4137\n",
      "iter 430: loss 2.4089\n",
      "iter 440: loss 2.3758\n",
      "iter 450: loss 2.3605\n",
      "iter 460: loss 2.3486\n",
      "iter 470: loss 2.3376\n",
      "iter 480: loss 2.3276\n",
      "iter 490: loss 2.2850\n",
      "iter 500: loss 2.2917\n",
      "step 500: train loss 2.2451, val loss 2.2475\n",
      "iter 510: loss 2.2698\n",
      "iter 520: loss 2.2653\n",
      "iter 530: loss 2.2366\n",
      "iter 540: loss 2.2443\n",
      "iter 550: loss 2.2041\n",
      "iter 560: loss 2.2017\n",
      "iter 570: loss 2.1683\n",
      "iter 580: loss 2.1768\n",
      "iter 590: loss 2.1494\n",
      "iter 600: loss 2.1254\n",
      "iter 610: loss 2.1180\n",
      "iter 620: loss 2.1142\n",
      "iter 630: loss 2.0921\n",
      "iter 640: loss 2.0968\n",
      "iter 650: loss 2.0865\n",
      "iter 660: loss 2.0808\n",
      "iter 670: loss 2.0650\n",
      "iter 680: loss 2.0262\n",
      "iter 690: loss 2.0356\n",
      "iter 700: loss 2.0222\n",
      "iter 710: loss 2.0267\n",
      "iter 720: loss 2.0001\n",
      "iter 730: loss 1.9918\n",
      "iter 740: loss 1.9959\n",
      "iter 750: loss 1.9671\n",
      "iter 760: loss 1.9459\n",
      "iter 770: loss 1.9190\n",
      "iter 780: loss 1.9271\n",
      "iter 790: loss 1.9343\n",
      "iter 800: loss 1.9156\n",
      "iter 810: loss 1.8981\n",
      "iter 820: loss 1.8903\n",
      "iter 830: loss 1.8739\n",
      "iter 840: loss 1.8743\n",
      "iter 850: loss 1.8699\n",
      "iter 860: loss 1.8609\n",
      "iter 870: loss 1.8566\n",
      "iter 880: loss 1.8231\n",
      "iter 890: loss 1.8391\n",
      "iter 900: loss 1.8038\n",
      "iter 910: loss 1.8165\n",
      "iter 920: loss 1.8140\n",
      "iter 930: loss 1.8303\n",
      "iter 940: loss 1.7923\n",
      "iter 950: loss 1.7827\n",
      "iter 960: loss 1.7896\n",
      "iter 970: loss 1.7602\n",
      "iter 980: loss 1.7852\n",
      "iter 990: loss 1.7539\n",
      "iter 1000: loss 1.7621\n",
      "step 1000: train loss 1.6995, val loss 1.7008\n",
      "iter 1010: loss 1.7374\n",
      "iter 1020: loss 1.7454\n",
      "iter 1030: loss 1.7405\n",
      "iter 1040: loss 1.7214\n",
      "iter 1050: loss 1.7318\n",
      "iter 1060: loss 1.7148\n",
      "iter 1070: loss 1.7048\n",
      "iter 1080: loss 1.6960\n",
      "iter 1090: loss 1.6948\n",
      "iter 1100: loss 1.6953\n",
      "iter 1110: loss 1.6707\n",
      "iter 1120: loss 1.6837\n",
      "iter 1130: loss 1.6677\n",
      "iter 1140: loss 1.6823\n",
      "iter 1150: loss 1.6660\n",
      "iter 1160: loss 1.6449\n",
      "iter 1170: loss 1.6893\n",
      "iter 1180: loss 1.6621\n",
      "iter 1190: loss 1.6362\n",
      "iter 1200: loss 1.6368\n",
      "iter 1210: loss 1.6165\n",
      "iter 1220: loss 1.6386\n",
      "iter 1230: loss 1.6192\n",
      "iter 1240: loss 1.6387\n",
      "iter 1250: loss 1.6273\n",
      "iter 1260: loss 1.5963\n",
      "iter 1270: loss 1.6116\n",
      "iter 1280: loss 1.6388\n",
      "iter 1290: loss 1.5986\n",
      "iter 1300: loss 1.5967\n",
      "iter 1310: loss 1.5968\n",
      "iter 1320: loss 1.5677\n",
      "iter 1330: loss 1.5850\n",
      "iter 1340: loss 1.5455\n",
      "iter 1350: loss 1.5811\n",
      "iter 1360: loss 1.5696\n",
      "iter 1370: loss 1.5581\n",
      "iter 1380: loss 1.5705\n",
      "iter 1390: loss 1.5520\n",
      "iter 1400: loss 1.5395\n",
      "iter 1410: loss 1.5373\n",
      "iter 1420: loss 1.5522\n",
      "iter 1430: loss 1.5497\n",
      "iter 1440: loss 1.5424\n",
      "iter 1450: loss 1.5298\n",
      "iter 1460: loss 1.5381\n",
      "iter 1470: loss 1.5123\n",
      "iter 1480: loss 1.5247\n",
      "iter 1490: loss 1.5270\n",
      "iter 1500: loss 1.5167\n",
      "step 1500: train loss 1.4703, val loss 1.4701\n",
      "iter 1510: loss 1.5341\n",
      "iter 1520: loss 1.5161\n",
      "iter 1530: loss 1.5135\n",
      "iter 1540: loss 1.5033\n",
      "iter 1550: loss 1.5115\n",
      "iter 1560: loss 1.5054\n",
      "iter 1570: loss 1.5192\n",
      "iter 1580: loss 1.5017\n",
      "iter 1590: loss 1.4893\n",
      "iter 1600: loss 1.4883\n",
      "iter 1610: loss 1.4957\n",
      "iter 1620: loss 1.4771\n",
      "iter 1630: loss 1.4902\n",
      "iter 1640: loss 1.4745\n",
      "iter 1650: loss 1.4730\n",
      "iter 1660: loss 1.4805\n",
      "iter 1670: loss 1.4947\n",
      "iter 1680: loss 1.4622\n",
      "iter 1690: loss 1.4795\n",
      "iter 1700: loss 1.4602\n",
      "iter 1710: loss 1.4576\n",
      "iter 1720: loss 1.4457\n",
      "iter 1730: loss 1.4350\n",
      "iter 1740: loss 1.4788\n",
      "iter 1750: loss 1.4483\n",
      "iter 1760: loss 1.4546\n",
      "iter 1770: loss 1.4593\n",
      "iter 1780: loss 1.4517\n",
      "iter 1790: loss 1.4513\n",
      "iter 1800: loss 1.4349\n",
      "iter 1810: loss 1.4598\n",
      "iter 1820: loss 1.4558\n",
      "iter 1830: loss 1.4595\n",
      "iter 1840: loss 1.4416\n",
      "iter 1850: loss 1.4389\n",
      "iter 1860: loss 1.4204\n",
      "iter 1870: loss 1.4334\n",
      "iter 1880: loss 1.4462\n",
      "iter 1890: loss 1.4015\n",
      "iter 1900: loss 1.4315\n",
      "iter 1910: loss 1.4017\n",
      "iter 1920: loss 1.4112\n",
      "iter 1930: loss 1.4221\n",
      "iter 1940: loss 1.4015\n",
      "iter 1950: loss 1.4287\n",
      "iter 1960: loss 1.4207\n",
      "iter 1970: loss 1.3930\n",
      "iter 1980: loss 1.3960\n",
      "iter 1990: loss 1.4102\n",
      "iter 2000: loss 1.4137\n",
      "step 2000: train loss 1.3554, val loss 1.3524\n",
      "iter 2010: loss 1.3994\n",
      "iter 2020: loss 1.4058\n",
      "iter 2030: loss 1.4047\n",
      "iter 2040: loss 1.3870\n",
      "iter 2050: loss 1.3874\n",
      "iter 2060: loss 1.4168\n",
      "iter 2070: loss 1.3838\n",
      "iter 2080: loss 1.4077\n",
      "iter 2090: loss 1.3715\n",
      "iter 2100: loss 1.3949\n",
      "iter 2110: loss 1.3891\n",
      "iter 2120: loss 1.3845\n",
      "iter 2130: loss 1.4026\n",
      "iter 2140: loss 1.3650\n",
      "iter 2150: loss 1.3591\n",
      "iter 2160: loss 1.3728\n",
      "iter 2170: loss 1.3763\n",
      "iter 2180: loss 1.3864\n",
      "iter 2190: loss 1.3811\n",
      "iter 2200: loss 1.3571\n",
      "iter 2210: loss 1.3741\n",
      "iter 2220: loss 1.3609\n",
      "iter 2230: loss 1.3730\n",
      "iter 2240: loss 1.3785\n",
      "iter 2250: loss 1.3642\n",
      "iter 2260: loss 1.3771\n",
      "iter 2270: loss 1.3561\n",
      "iter 2280: loss 1.3621\n",
      "iter 2290: loss 1.3303\n",
      "iter 2300: loss 1.3467\n",
      "iter 2310: loss 1.3530\n",
      "iter 2320: loss 1.3404\n",
      "iter 2330: loss 1.3434\n",
      "iter 2340: loss 1.3365\n",
      "iter 2350: loss 1.3562\n",
      "iter 2360: loss 1.3432\n",
      "iter 2370: loss 1.3633\n",
      "iter 2380: loss 1.3476\n",
      "iter 2390: loss 1.3319\n",
      "iter 2400: loss 1.3275\n",
      "iter 2410: loss 1.3192\n",
      "iter 2420: loss 1.3472\n",
      "iter 2430: loss 1.3479\n",
      "iter 2440: loss 1.3637\n",
      "iter 2450: loss 1.3327\n",
      "iter 2460: loss 1.3287\n",
      "iter 2470: loss 1.3485\n",
      "iter 2480: loss 1.3214\n",
      "iter 2490: loss 1.3412\n",
      "iter 2500: loss 1.3321\n",
      "step 2500: train loss 1.2825, val loss 1.2825\n",
      "iter 2510: loss 1.3208\n",
      "iter 2520: loss 1.3450\n",
      "iter 2530: loss 1.3259\n",
      "iter 2540: loss 1.3345\n",
      "iter 2550: loss 1.3347\n",
      "iter 2560: loss 1.3188\n",
      "iter 2570: loss 1.3223\n",
      "iter 2580: loss 1.3007\n",
      "iter 2590: loss 1.3442\n",
      "iter 2600: loss 1.3047\n",
      "iter 2610: loss 1.3077\n",
      "iter 2620: loss 1.3183\n",
      "iter 2630: loss 1.3126\n",
      "iter 2640: loss 1.2937\n",
      "iter 2650: loss 1.3173\n",
      "iter 2660: loss 1.2977\n",
      "iter 2670: loss 1.3123\n",
      "iter 2680: loss 1.3143\n",
      "iter 2690: loss 1.3112\n",
      "iter 2700: loss 1.3029\n",
      "iter 2710: loss 1.2915\n",
      "iter 2720: loss 1.2905\n",
      "iter 2730: loss 1.3021\n",
      "iter 2740: loss 1.2950\n",
      "iter 2750: loss 1.3012\n",
      "iter 2760: loss 1.3034\n",
      "iter 2770: loss 1.3040\n",
      "iter 2780: loss 1.3242\n",
      "iter 2790: loss 1.2849\n",
      "iter 2800: loss 1.3017\n",
      "iter 2810: loss 1.2601\n",
      "iter 2820: loss 1.3075\n",
      "iter 2830: loss 1.2858\n",
      "iter 2840: loss 1.2920\n",
      "iter 2850: loss 1.2819\n",
      "iter 2860: loss 1.2911\n",
      "iter 2870: loss 1.2998\n",
      "iter 2880: loss 1.2922\n",
      "iter 2890: loss 1.2588\n",
      "iter 2900: loss 1.2777\n",
      "iter 2910: loss 1.2970\n",
      "iter 2920: loss 1.2856\n",
      "iter 2930: loss 1.2833\n",
      "iter 2940: loss 1.2692\n",
      "iter 2950: loss 1.2568\n",
      "iter 2960: loss 1.3037\n",
      "iter 2970: loss 1.2763\n",
      "iter 2980: loss 1.2548\n",
      "iter 2990: loss 1.2884\n",
      "iter 3000: loss 1.2799\n",
      "step 3000: train loss 1.2176, val loss 1.2214\n",
      "iter 3010: loss 1.2744\n",
      "iter 3020: loss 1.2716\n",
      "iter 3030: loss 1.2764\n",
      "iter 3040: loss 1.2745\n",
      "iter 3050: loss 1.2668\n",
      "iter 3060: loss 1.2513\n",
      "iter 3070: loss 1.2621\n",
      "iter 3080: loss 1.2650\n",
      "iter 3090: loss 1.2748\n",
      "iter 3100: loss 1.2410\n",
      "iter 3110: loss 1.2512\n",
      "iter 3120: loss 1.2459\n",
      "iter 3130: loss 1.2623\n",
      "iter 3140: loss 1.2617\n",
      "iter 3150: loss 1.2708\n",
      "iter 3160: loss 1.2567\n",
      "iter 3170: loss 1.2390\n",
      "iter 3180: loss 1.2460\n",
      "iter 3190: loss 1.2600\n",
      "iter 3200: loss 1.2387\n",
      "iter 3210: loss 1.2591\n",
      "iter 3220: loss 1.2711\n",
      "iter 3230: loss 1.2461\n",
      "iter 3240: loss 1.2569\n",
      "iter 3250: loss 1.2530\n",
      "iter 3260: loss 1.2610\n",
      "iter 3270: loss 1.2523\n",
      "iter 3280: loss 1.2205\n",
      "iter 3290: loss 1.2404\n",
      "iter 3300: loss 1.2238\n",
      "iter 3310: loss 1.2344\n",
      "iter 3320: loss 1.2304\n",
      "iter 3330: loss 1.2519\n",
      "iter 3340: loss 1.2630\n",
      "iter 3350: loss 1.2233\n",
      "iter 3360: loss 1.2253\n",
      "iter 3370: loss 1.2480\n",
      "iter 3380: loss 1.2626\n",
      "iter 3390: loss 1.2343\n",
      "iter 3400: loss 1.2521\n",
      "iter 3410: loss 1.2241\n",
      "iter 3420: loss 1.2454\n",
      "iter 3430: loss 1.2418\n",
      "iter 3440: loss 1.2369\n",
      "iter 3450: loss 1.2233\n",
      "iter 3460: loss 1.2448\n",
      "iter 3470: loss 1.2484\n",
      "iter 3480: loss 1.2305\n",
      "iter 3490: loss 1.2214\n",
      "iter 3500: loss 1.2347\n",
      "step 3500: train loss 1.1788, val loss 1.1762\n",
      "iter 3510: loss 1.2143\n",
      "iter 3520: loss 1.2138\n",
      "iter 3530: loss 1.2311\n",
      "iter 3540: loss 1.2041\n",
      "iter 3550: loss 1.2214\n",
      "iter 3560: loss 1.2206\n",
      "iter 3570: loss 1.2208\n",
      "iter 3580: loss 1.2106\n",
      "iter 3590: loss 1.2221\n",
      "iter 3600: loss 1.2276\n",
      "iter 3610: loss 1.2075\n",
      "iter 3620: loss 1.2181\n",
      "iter 3630: loss 1.2209\n",
      "iter 3640: loss 1.2241\n",
      "iter 3650: loss 1.2044\n",
      "iter 3660: loss 1.2075\n",
      "iter 3670: loss 1.2264\n",
      "iter 3680: loss 1.2170\n",
      "iter 3690: loss 1.2156\n",
      "iter 3700: loss 1.1997\n",
      "iter 3710: loss 1.2037\n",
      "iter 3720: loss 1.2226\n",
      "iter 3730: loss 1.2249\n",
      "iter 3740: loss 1.1950\n",
      "iter 3750: loss 1.2190\n",
      "iter 3760: loss 1.2273\n",
      "iter 3770: loss 1.2090\n",
      "iter 3780: loss 1.1937\n",
      "iter 3790: loss 1.2119\n",
      "iter 3800: loss 1.2207\n",
      "iter 3810: loss 1.2089\n",
      "iter 3820: loss 1.1935\n",
      "iter 3830: loss 1.2015\n",
      "iter 3840: loss 1.1956\n",
      "iter 3850: loss 1.1940\n",
      "iter 3860: loss 1.1919\n",
      "iter 3870: loss 1.1911\n",
      "iter 3880: loss 1.1861\n",
      "iter 3890: loss 1.1715\n",
      "iter 3900: loss 1.1821\n",
      "iter 3910: loss 1.2020\n",
      "iter 3920: loss 1.1786\n",
      "iter 3930: loss 1.1992\n",
      "iter 3940: loss 1.1791\n",
      "iter 3950: loss 1.1859\n",
      "iter 3960: loss 1.1939\n",
      "iter 3970: loss 1.2040\n",
      "iter 3980: loss 1.1877\n",
      "iter 3990: loss 1.1885\n",
      "iter 4000: loss 1.2037\n",
      "step 4000: train loss 1.1307, val loss 1.1317\n",
      "iter 4010: loss 1.1798\n",
      "iter 4020: loss 1.1742\n",
      "iter 4030: loss 1.2009\n",
      "iter 4040: loss 1.1894\n",
      "iter 4050: loss 1.1620\n",
      "iter 4060: loss 1.2040\n",
      "iter 4070: loss 1.1784\n",
      "iter 4080: loss 1.1886\n",
      "iter 4090: loss 1.1745\n",
      "iter 4100: loss 1.1716\n",
      "iter 4110: loss 1.1860\n",
      "iter 4120: loss 1.1617\n",
      "iter 4130: loss 1.1686\n",
      "iter 4140: loss 1.1757\n",
      "iter 4150: loss 1.1643\n",
      "iter 4160: loss 1.1788\n",
      "iter 4170: loss 1.1897\n",
      "iter 4180: loss 1.1532\n",
      "iter 4190: loss 1.1733\n",
      "iter 4200: loss 1.1615\n",
      "iter 4210: loss 1.1878\n",
      "iter 4220: loss 1.1849\n",
      "iter 4230: loss 1.1631\n",
      "iter 4240: loss 1.1516\n",
      "iter 4250: loss 1.1805\n",
      "iter 4260: loss 1.1735\n",
      "iter 4270: loss 1.1648\n",
      "iter 4280: loss 1.1723\n",
      "iter 4290: loss 1.1634\n",
      "iter 4300: loss 1.1738\n",
      "iter 4310: loss 1.1679\n",
      "iter 4320: loss 1.1504\n",
      "iter 4330: loss 1.1704\n",
      "iter 4340: loss 1.1696\n",
      "iter 4350: loss 1.1734\n",
      "iter 4360: loss 1.1569\n",
      "iter 4370: loss 1.1689\n",
      "iter 4380: loss 1.1417\n",
      "iter 4390: loss 1.1654\n",
      "iter 4400: loss 1.1638\n",
      "iter 4410: loss 1.1642\n",
      "iter 4420: loss 1.1500\n",
      "iter 4430: loss 1.1552\n",
      "iter 4440: loss 1.1687\n",
      "iter 4450: loss 1.1536\n",
      "iter 4460: loss 1.1549\n",
      "iter 4470: loss 1.1829\n",
      "iter 4480: loss 1.1619\n",
      "iter 4490: loss 1.1429\n",
      "iter 4500: loss 1.1613\n",
      "step 4500: train loss 1.0867, val loss 1.0912\n",
      "iter 4510: loss 1.1641\n",
      "iter 4520: loss 1.1555\n",
      "iter 4530: loss 1.1431\n",
      "iter 4540: loss 1.1607\n",
      "iter 4550: loss 1.1392\n",
      "iter 4560: loss 1.1338\n",
      "iter 4570: loss 1.1519\n",
      "iter 4580: loss 1.1709\n",
      "iter 4590: loss 1.1523\n",
      "iter 4600: loss 1.1413\n",
      "iter 4610: loss 1.1441\n",
      "iter 4620: loss 1.1372\n",
      "iter 4630: loss 1.1595\n",
      "iter 4640: loss 1.1301\n",
      "iter 4650: loss 1.1433\n",
      "iter 4660: loss 1.1485\n",
      "iter 4670: loss 1.1436\n",
      "iter 4680: loss 1.1456\n",
      "iter 4690: loss 1.1244\n",
      "iter 4700: loss 1.1464\n",
      "iter 4710: loss 1.1453\n",
      "iter 4720: loss 1.1376\n",
      "iter 4730: loss 1.1292\n",
      "iter 4740: loss 1.1416\n",
      "iter 4750: loss 1.1361\n",
      "iter 4760: loss 1.1451\n",
      "iter 4770: loss 1.1376\n",
      "iter 4780: loss 1.1267\n",
      "iter 4790: loss 1.1477\n",
      "iter 4800: loss 1.1356\n",
      "iter 4810: loss 1.1403\n",
      "iter 4820: loss 1.1456\n",
      "iter 4830: loss 1.1464\n",
      "iter 4840: loss 1.1305\n",
      "iter 4850: loss 1.1296\n",
      "iter 4860: loss 1.1371\n",
      "iter 4870: loss 1.1260\n",
      "iter 4880: loss 1.1342\n",
      "iter 4890: loss 1.1247\n",
      "iter 4900: loss 1.1321\n",
      "iter 4910: loss 1.1197\n",
      "iter 4920: loss 1.1234\n",
      "iter 4930: loss 1.1369\n",
      "iter 4940: loss 1.1171\n",
      "iter 4950: loss 1.1202\n",
      "iter 4960: loss 1.1205\n",
      "iter 4970: loss 1.1194\n",
      "iter 4980: loss 1.1210\n",
      "iter 4990: loss 1.1231\n",
      "iter 5000: loss 1.1091\n",
      "step 5000: train loss 1.0490, val loss 1.0514\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.1753\n",
      "step 0: train loss 4.1470, val loss 4.1474\n",
      "iter 10: loss 3.1635\n",
      "iter 20: loss 3.1227\n",
      "iter 30: loss 3.0947\n",
      "iter 40: loss 2.9599\n",
      "iter 50: loss 2.8649\n",
      "iter 60: loss 2.7913\n",
      "iter 70: loss 2.7068\n",
      "iter 80: loss 2.6560\n",
      "iter 90: loss 2.6499\n",
      "iter 100: loss 2.5936\n",
      "iter 110: loss 2.6029\n",
      "iter 120: loss 2.5718\n",
      "iter 130: loss 2.5690\n",
      "iter 140: loss 2.5548\n",
      "iter 150: loss 2.5417\n",
      "iter 160: loss 2.5441\n",
      "iter 170: loss 2.5282\n",
      "iter 180: loss 2.5318\n",
      "iter 190: loss 2.5302\n",
      "iter 200: loss 2.5058\n",
      "iter 210: loss 2.5148\n",
      "iter 220: loss 2.4876\n",
      "iter 230: loss 2.4999\n",
      "iter 240: loss 2.4983\n",
      "iter 250: loss 2.4923\n",
      "iter 260: loss 2.4839\n",
      "iter 270: loss 2.4884\n",
      "iter 280: loss 2.4691\n",
      "iter 290: loss 2.4585\n",
      "iter 300: loss 2.4516\n",
      "iter 310: loss 2.4417\n",
      "iter 320: loss 2.4002\n",
      "iter 330: loss 2.3881\n",
      "iter 340: loss 2.3333\n",
      "iter 350: loss 2.3156\n",
      "iter 360: loss 2.3083\n",
      "iter 370: loss 2.2510\n",
      "iter 380: loss 2.2796\n",
      "iter 390: loss 2.1868\n",
      "iter 400: loss 2.1673\n",
      "iter 410: loss 2.1589\n",
      "iter 420: loss 2.1227\n",
      "iter 430: loss 2.0890\n",
      "iter 440: loss 2.0718\n",
      "iter 450: loss 2.0468\n",
      "iter 460: loss 2.0221\n",
      "iter 470: loss 1.9886\n",
      "iter 480: loss 2.0030\n",
      "iter 490: loss 1.9293\n",
      "iter 500: loss 1.9155\n",
      "step 500: train loss 1.8914, val loss 1.8961\n",
      "iter 510: loss 1.9036\n",
      "iter 520: loss 1.8989\n",
      "iter 530: loss 1.8554\n",
      "iter 540: loss 1.8494\n",
      "iter 550: loss 1.8724\n",
      "iter 560: loss 1.8436\n",
      "iter 570: loss 1.8000\n",
      "iter 580: loss 1.8081\n",
      "iter 590: loss 1.7766\n",
      "iter 600: loss 1.7584\n",
      "iter 610: loss 1.7405\n",
      "iter 620: loss 1.7190\n",
      "iter 630: loss 1.7146\n",
      "iter 640: loss 1.6954\n",
      "iter 650: loss 1.6871\n",
      "iter 660: loss 1.6704\n",
      "iter 670: loss 1.6732\n",
      "iter 680: loss 1.6608\n",
      "iter 690: loss 1.6398\n",
      "iter 700: loss 1.6131\n",
      "iter 710: loss 1.6079\n",
      "iter 720: loss 1.5949\n",
      "iter 730: loss 1.6013\n",
      "iter 740: loss 1.6048\n",
      "iter 750: loss 1.5635\n",
      "iter 760: loss 1.5364\n",
      "iter 770: loss 1.5604\n",
      "iter 780: loss 1.5844\n",
      "iter 790: loss 1.5150\n",
      "iter 800: loss 1.5396\n",
      "iter 810: loss 1.5315\n",
      "iter 820: loss 1.5149\n",
      "iter 830: loss 1.4914\n",
      "iter 840: loss 1.4935\n",
      "iter 850: loss 1.4879\n",
      "iter 860: loss 1.5100\n",
      "iter 870: loss 1.4994\n",
      "iter 880: loss 1.4750\n",
      "iter 890: loss 1.4682\n",
      "iter 900: loss 1.4675\n",
      "iter 910: loss 1.4461\n",
      "iter 920: loss 1.4641\n",
      "iter 930: loss 1.4509\n",
      "iter 940: loss 1.4414\n",
      "iter 950: loss 1.4371\n",
      "iter 960: loss 1.4407\n",
      "iter 970: loss 1.4361\n",
      "iter 980: loss 1.4281\n",
      "iter 990: loss 1.4176\n",
      "iter 1000: loss 1.4361\n",
      "step 1000: train loss 1.3586, val loss 1.3668\n",
      "iter 1010: loss 1.3944\n",
      "iter 1020: loss 1.3933\n",
      "iter 1030: loss 1.4105\n",
      "iter 1040: loss 1.3879\n",
      "iter 1050: loss 1.3765\n",
      "iter 1060: loss 1.3787\n",
      "iter 1070: loss 1.3625\n",
      "iter 1080: loss 1.3859\n",
      "iter 1090: loss 1.3695\n",
      "iter 1100: loss 1.3679\n",
      "iter 1110: loss 1.3502\n",
      "iter 1120: loss 1.3694\n",
      "iter 1130: loss 1.3383\n",
      "iter 1140: loss 1.3585\n",
      "iter 1150: loss 1.3558\n",
      "iter 1160: loss 1.3608\n",
      "iter 1170: loss 1.3312\n",
      "iter 1180: loss 1.3481\n",
      "iter 1190: loss 1.3366\n",
      "iter 1200: loss 1.3375\n",
      "iter 1210: loss 1.3351\n",
      "iter 1220: loss 1.3277\n",
      "iter 1230: loss 1.3054\n",
      "iter 1240: loss 1.3208\n",
      "iter 1250: loss 1.3180\n",
      "iter 1260: loss 1.3193\n",
      "iter 1270: loss 1.3021\n",
      "iter 1280: loss 1.3026\n",
      "iter 1290: loss 1.2904\n",
      "iter 1300: loss 1.3012\n",
      "iter 1310: loss 1.3191\n",
      "iter 1320: loss 1.3101\n",
      "iter 1330: loss 1.3273\n",
      "iter 1340: loss 1.3171\n",
      "iter 1350: loss 1.2873\n",
      "iter 1360: loss 1.2786\n",
      "iter 1370: loss 1.3011\n",
      "iter 1380: loss 1.2920\n",
      "iter 1390: loss 1.3058\n",
      "iter 1400: loss 1.2864\n",
      "iter 1410: loss 1.2724\n",
      "iter 1420: loss 1.2807\n",
      "iter 1430: loss 1.2753\n",
      "iter 1440: loss 1.2623\n",
      "iter 1450: loss 1.2458\n",
      "iter 1460: loss 1.2601\n",
      "iter 1470: loss 1.2574\n",
      "iter 1480: loss 1.2552\n",
      "iter 1490: loss 1.2510\n",
      "iter 1500: loss 1.2666\n",
      "step 1500: train loss 1.2164, val loss 1.2192\n",
      "iter 1510: loss 1.2520\n",
      "iter 1520: loss 1.2606\n",
      "iter 1530: loss 1.2632\n",
      "iter 1540: loss 1.2408\n",
      "iter 1550: loss 1.2521\n",
      "iter 1560: loss 1.2717\n",
      "iter 1570: loss 1.2311\n",
      "iter 1580: loss 1.2323\n",
      "iter 1590: loss 1.2444\n",
      "iter 1600: loss 1.2397\n",
      "iter 1610: loss 1.2471\n",
      "iter 1620: loss 1.2452\n",
      "iter 1630: loss 1.2470\n",
      "iter 1640: loss 1.2406\n",
      "iter 1650: loss 1.2407\n",
      "iter 1660: loss 1.2251\n",
      "iter 1670: loss 1.2073\n",
      "iter 1680: loss 1.2203\n",
      "iter 1690: loss 1.2012\n",
      "iter 1700: loss 1.2087\n",
      "iter 1710: loss 1.2119\n",
      "iter 1720: loss 1.2157\n",
      "iter 1730: loss 1.2183\n",
      "iter 1740: loss 1.2067\n",
      "iter 1750: loss 1.2220\n",
      "iter 1760: loss 1.2298\n",
      "iter 1770: loss 1.2261\n",
      "iter 1780: loss 1.2185\n",
      "iter 1790: loss 1.2176\n",
      "iter 1800: loss 1.1906\n",
      "iter 1810: loss 1.2147\n",
      "iter 1820: loss 1.2155\n",
      "iter 1830: loss 1.2173\n",
      "iter 1840: loss 1.2074\n",
      "iter 1850: loss 1.1950\n",
      "iter 1860: loss 1.1729\n",
      "iter 1870: loss 1.2044\n",
      "iter 1880: loss 1.2079\n",
      "iter 1890: loss 1.1826\n",
      "iter 1900: loss 1.1972\n",
      "iter 1910: loss 1.1868\n",
      "iter 1920: loss 1.1843\n",
      "iter 1930: loss 1.1694\n",
      "iter 1940: loss 1.1761\n",
      "iter 1950: loss 1.1961\n",
      "iter 1960: loss 1.1891\n",
      "iter 1970: loss 1.1960\n",
      "iter 1980: loss 1.1817\n",
      "iter 1990: loss 1.1903\n",
      "iter 2000: loss 1.1793\n",
      "step 2000: train loss 1.1302, val loss 1.1327\n",
      "iter 2010: loss 1.1783\n",
      "iter 2020: loss 1.1760\n",
      "iter 2030: loss 1.1556\n",
      "iter 2040: loss 1.1634\n",
      "iter 2050: loss 1.1837\n",
      "iter 2060: loss 1.1870\n",
      "iter 2070: loss 1.1538\n",
      "iter 2080: loss 1.1759\n",
      "iter 2090: loss 1.1796\n",
      "iter 2100: loss 1.1654\n",
      "iter 2110: loss 1.1455\n",
      "iter 2120: loss 1.1697\n",
      "iter 2130: loss 1.1631\n",
      "iter 2140: loss 1.1591\n",
      "iter 2150: loss 1.1634\n",
      "iter 2160: loss 1.1737\n",
      "iter 2170: loss 1.1563\n",
      "iter 2180: loss 1.1577\n",
      "iter 2190: loss 1.1469\n",
      "iter 2200: loss 1.1522\n",
      "iter 2210: loss 1.1353\n",
      "iter 2220: loss 1.1591\n",
      "iter 2230: loss 1.1640\n",
      "iter 2240: loss 1.1411\n",
      "iter 2250: loss 1.1332\n",
      "iter 2260: loss 1.1358\n",
      "iter 2270: loss 1.1485\n",
      "iter 2280: loss 1.1450\n",
      "iter 2290: loss 1.1526\n",
      "iter 2300: loss 1.1413\n",
      "iter 2310: loss 1.1420\n",
      "iter 2320: loss 1.1345\n",
      "iter 2330: loss 1.1370\n",
      "iter 2340: loss 1.1568\n",
      "iter 2350: loss 1.1135\n",
      "iter 2360: loss 1.1206\n",
      "iter 2370: loss 1.1230\n",
      "iter 2380: loss 1.1352\n",
      "iter 2390: loss 1.1454\n",
      "iter 2400: loss 1.1095\n",
      "iter 2410: loss 1.1155\n",
      "iter 2420: loss 1.1292\n",
      "iter 2430: loss 1.1321\n",
      "iter 2440: loss 1.1324\n",
      "iter 2450: loss 1.1287\n",
      "iter 2460: loss 1.1201\n",
      "iter 2470: loss 1.1172\n",
      "iter 2480: loss 1.1184\n",
      "iter 2490: loss 1.1213\n",
      "iter 2500: loss 1.1352\n",
      "step 2500: train loss 1.0562, val loss 1.0601\n",
      "iter 2510: loss 1.1227\n",
      "iter 2520: loss 1.1053\n",
      "iter 2530: loss 1.1026\n",
      "iter 2540: loss 1.1303\n",
      "iter 2550: loss 1.1317\n",
      "iter 2560: loss 1.0828\n",
      "iter 2570: loss 1.0969\n",
      "iter 2580: loss 1.1126\n",
      "iter 2590: loss 1.1123\n",
      "iter 2600: loss 1.0835\n",
      "iter 2610: loss 1.0976\n",
      "iter 2620: loss 1.1070\n",
      "iter 2630: loss 1.1044\n",
      "iter 2640: loss 1.1079\n",
      "iter 2650: loss 1.1121\n",
      "iter 2660: loss 1.0935\n",
      "iter 2670: loss 1.0778\n",
      "iter 2680: loss 1.0847\n",
      "iter 2690: loss 1.0908\n",
      "iter 2700: loss 1.1045\n",
      "iter 2710: loss 1.1002\n",
      "iter 2720: loss 1.0828\n",
      "iter 2730: loss 1.0771\n",
      "iter 2740: loss 1.1035\n",
      "iter 2750: loss 1.1167\n",
      "iter 2760: loss 1.0828\n",
      "iter 2770: loss 1.1010\n",
      "iter 2780: loss 1.1014\n",
      "iter 2790: loss 1.0765\n",
      "iter 2800: loss 1.0582\n",
      "iter 2810: loss 1.0778\n",
      "iter 2820: loss 1.0779\n",
      "iter 2830: loss 1.0862\n",
      "iter 2840: loss 1.0900\n",
      "iter 2850: loss 1.0875\n",
      "iter 2860: loss 1.0597\n",
      "iter 2870: loss 1.0762\n",
      "iter 2880: loss 1.1052\n",
      "iter 2890: loss 1.0727\n",
      "iter 2900: loss 1.0769\n",
      "iter 2910: loss 1.0654\n",
      "iter 2920: loss 1.0749\n",
      "iter 2930: loss 1.0667\n",
      "iter 2940: loss 1.0723\n",
      "iter 2950: loss 1.0709\n",
      "iter 2960: loss 1.0595\n",
      "iter 2970: loss 1.0747\n",
      "iter 2980: loss 1.0552\n",
      "iter 2990: loss 1.0821\n",
      "iter 3000: loss 1.0602\n",
      "step 3000: train loss 0.9919, val loss 0.9934\n",
      "iter 3010: loss 1.0532\n",
      "iter 3020: loss 1.0396\n",
      "iter 3030: loss 1.0436\n",
      "iter 3040: loss 1.0355\n",
      "iter 3050: loss 1.0529\n",
      "iter 3060: loss 1.0710\n",
      "iter 3070: loss 1.0798\n",
      "iter 3080: loss 1.0623\n",
      "iter 3090: loss 1.0430\n",
      "iter 3100: loss 1.0542\n",
      "iter 3110: loss 1.0618\n",
      "iter 3120: loss 1.0519\n",
      "iter 3130: loss 1.0402\n",
      "iter 3140: loss 1.0410\n",
      "iter 3150: loss 1.0477\n",
      "iter 3160: loss 1.0290\n",
      "iter 3170: loss 1.0566\n",
      "iter 3180: loss 1.0521\n",
      "iter 3190: loss 1.0362\n",
      "iter 3200: loss 1.0377\n",
      "iter 3210: loss 1.0586\n",
      "iter 3220: loss 1.0333\n",
      "iter 3230: loss 1.0535\n",
      "iter 3240: loss 1.0419\n",
      "iter 3250: loss 1.0455\n",
      "iter 3260: loss 1.0433\n",
      "iter 3270: loss 1.0414\n",
      "iter 3280: loss 1.0356\n",
      "iter 3290: loss 1.0284\n",
      "iter 3300: loss 1.0107\n",
      "iter 3310: loss 1.0371\n",
      "iter 3320: loss 1.0313\n",
      "iter 3330: loss 1.0286\n",
      "iter 3340: loss 1.0275\n",
      "iter 3350: loss 1.0217\n",
      "iter 3360: loss 1.0285\n",
      "iter 3370: loss 1.0309\n",
      "iter 3380: loss 1.0222\n",
      "iter 3390: loss 1.0270\n",
      "iter 3400: loss 1.0146\n",
      "iter 3410: loss 1.0126\n",
      "iter 3420: loss 1.0334\n",
      "iter 3430: loss 1.0157\n",
      "iter 3440: loss 1.0080\n",
      "iter 3450: loss 1.0176\n",
      "iter 3460: loss 1.0225\n",
      "iter 3470: loss 1.0280\n",
      "iter 3480: loss 1.0103\n",
      "iter 3490: loss 1.0079\n",
      "iter 3500: loss 0.9968\n",
      "step 3500: train loss 0.9320, val loss 0.9371\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3510: loss 1.0127\n",
      "iter 3520: loss 1.0316\n",
      "iter 3530: loss 1.0247\n",
      "iter 3540: loss 1.0076\n",
      "iter 3550: loss 1.0097\n",
      "iter 3560: loss 1.0161\n",
      "iter 3570: loss 0.9973\n",
      "iter 3580: loss 1.0073\n",
      "iter 3590: loss 1.0042\n",
      "iter 3600: loss 1.0135\n",
      "iter 3610: loss 1.0119\n",
      "iter 3620: loss 1.0032\n",
      "iter 3630: loss 1.0098\n",
      "iter 3640: loss 0.9957\n",
      "iter 3650: loss 0.9967\n",
      "iter 3660: loss 1.0014\n",
      "iter 3670: loss 0.9956\n",
      "iter 3680: loss 0.9988\n",
      "iter 3690: loss 0.9976\n",
      "iter 3700: loss 0.9979\n",
      "iter 3710: loss 1.0191\n",
      "iter 3720: loss 1.0030\n",
      "iter 3730: loss 1.0014\n",
      "iter 3740: loss 0.9878\n",
      "iter 3750: loss 0.9947\n",
      "iter 3760: loss 0.9953\n",
      "iter 3770: loss 0.9721\n",
      "iter 3780: loss 0.9816\n",
      "iter 3790: loss 1.0001\n",
      "iter 3800: loss 0.9852\n",
      "iter 3810: loss 0.9907\n",
      "iter 3820: loss 0.9656\n",
      "iter 3830: loss 0.9848\n",
      "iter 3840: loss 0.9944\n",
      "iter 3850: loss 0.9834\n",
      "iter 3860: loss 0.9796\n",
      "iter 3870: loss 0.9850\n",
      "iter 3880: loss 0.9872\n",
      "iter 3890: loss 0.9885\n",
      "iter 3900: loss 0.9688\n",
      "iter 3910: loss 0.9761\n",
      "iter 3920: loss 0.9740\n",
      "iter 3930: loss 0.9914\n",
      "iter 3940: loss 0.9887\n",
      "iter 3950: loss 0.9614\n",
      "iter 3960: loss 0.9744\n",
      "iter 3970: loss 0.9682\n",
      "iter 3980: loss 0.9788\n",
      "iter 3990: loss 0.9866\n",
      "iter 4000: loss 0.9845\n",
      "step 4000: train loss 0.8791, val loss 0.8822\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4010: loss 0.9730\n",
      "iter 4020: loss 0.9585\n",
      "iter 4030: loss 0.9794\n",
      "iter 4040: loss 0.9690\n",
      "iter 4050: loss 0.9578\n",
      "iter 4060: loss 0.9540\n",
      "iter 4070: loss 0.9718\n",
      "iter 4080: loss 0.9658\n",
      "iter 4090: loss 0.9655\n",
      "iter 4100: loss 0.9709\n",
      "iter 4110: loss 0.9465\n",
      "iter 4120: loss 0.9688\n",
      "iter 4130: loss 0.9638\n",
      "iter 4140: loss 0.9626\n",
      "iter 4150: loss 0.9620\n",
      "iter 4160: loss 0.9678\n",
      "iter 4170: loss 0.9673\n",
      "iter 4180: loss 0.9591\n",
      "iter 4190: loss 0.9503\n",
      "iter 4200: loss 0.9560\n",
      "iter 4210: loss 0.9511\n",
      "iter 4220: loss 0.9767\n",
      "iter 4230: loss 0.9351\n",
      "iter 4240: loss 0.9527\n",
      "iter 4250: loss 0.9506\n",
      "iter 4260: loss 0.9595\n",
      "iter 4270: loss 0.9380\n",
      "iter 4280: loss 0.9558\n",
      "iter 4290: loss 0.9332\n",
      "iter 4300: loss 0.9468\n",
      "iter 4310: loss 0.9506\n",
      "iter 4320: loss 0.9412\n",
      "iter 4330: loss 0.9512\n",
      "iter 4340: loss 0.9543\n",
      "iter 4350: loss 0.9383\n",
      "iter 4360: loss 0.9451\n",
      "iter 4370: loss 0.9381\n",
      "iter 4380: loss 0.9372\n",
      "iter 4390: loss 0.9461\n",
      "iter 4400: loss 0.9296\n",
      "iter 4410: loss 0.9355\n",
      "iter 4420: loss 0.9684\n",
      "iter 4430: loss 0.9435\n",
      "iter 4440: loss 0.9262\n",
      "iter 4450: loss 0.9300\n",
      "iter 4460: loss 0.9393\n",
      "iter 4470: loss 0.9299\n",
      "iter 4480: loss 0.9385\n",
      "iter 4490: loss 0.9474\n",
      "iter 4500: loss 0.9374\n",
      "step 4500: train loss 0.8288, val loss 0.8302\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4510: loss 0.9212\n",
      "iter 4520: loss 0.9435\n",
      "iter 4530: loss 0.9308\n",
      "iter 4540: loss 0.9416\n",
      "iter 4550: loss 0.9186\n",
      "iter 4560: loss 0.9212\n",
      "iter 4570: loss 0.9291\n",
      "iter 4580: loss 0.9263\n",
      "iter 4590: loss 0.9423\n",
      "iter 4600: loss 0.9240\n",
      "iter 4610: loss 0.9389\n",
      "iter 4620: loss 0.9331\n",
      "iter 4630: loss 0.9214\n",
      "iter 4640: loss 0.9217\n",
      "iter 4650: loss 0.9025\n",
      "iter 4660: loss 0.8994\n",
      "iter 4670: loss 0.9170\n",
      "iter 4680: loss 0.9187\n",
      "iter 4690: loss 0.9271\n",
      "iter 4700: loss 0.9156\n",
      "iter 4710: loss 0.9071\n",
      "iter 4720: loss 0.9049\n",
      "iter 4730: loss 0.9231\n",
      "iter 4740: loss 0.9447\n",
      "iter 4750: loss 0.8940\n",
      "iter 4760: loss 0.9174\n",
      "iter 4770: loss 0.9194\n",
      "iter 4780: loss 0.9174\n",
      "iter 4790: loss 0.9216\n",
      "iter 4800: loss 0.9139\n",
      "iter 4810: loss 0.9090\n",
      "iter 4820: loss 0.8904\n",
      "iter 4830: loss 0.9188\n",
      "iter 4840: loss 0.9186\n",
      "iter 4850: loss 0.8987\n",
      "iter 4860: loss 0.8873\n",
      "iter 4870: loss 0.9109\n",
      "iter 4880: loss 0.9291\n",
      "iter 4890: loss 0.8990\n",
      "iter 4900: loss 0.9023\n",
      "iter 4910: loss 0.9072\n",
      "iter 4920: loss 0.8878\n",
      "iter 4930: loss 0.9066\n",
      "iter 4940: loss 0.8844\n",
      "iter 4950: loss 0.9130\n",
      "iter 4960: loss 0.8854\n",
      "iter 4970: loss 0.8997\n",
      "iter 4980: loss 0.8912\n",
      "iter 4990: loss 0.8855\n",
      "iter 5000: loss 0.8930\n",
      "step 5000: train loss 0.7782, val loss 0.7816\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.2501\n",
      "step 0: train loss 4.2362, val loss 4.2359\n",
      "iter 10: loss 3.1547\n",
      "iter 20: loss 3.1210\n",
      "iter 30: loss 3.1318\n",
      "iter 40: loss 3.1240\n",
      "iter 50: loss 3.1277\n",
      "iter 60: loss 3.1148\n",
      "iter 70: loss 3.1238\n",
      "iter 80: loss 3.1246\n",
      "iter 90: loss 3.1183\n",
      "iter 100: loss 3.1167\n",
      "iter 110: loss 3.1288\n",
      "iter 120: loss 3.1210\n",
      "iter 130: loss 3.1240\n",
      "iter 140: loss 3.1251\n",
      "iter 150: loss 3.1204\n",
      "iter 160: loss 3.1164\n",
      "iter 170: loss 3.1277\n",
      "iter 180: loss 3.1208\n",
      "iter 190: loss 3.1129\n",
      "iter 200: loss 3.0968\n",
      "iter 210: loss 3.1138\n",
      "iter 220: loss 3.0945\n",
      "iter 230: loss 3.0231\n",
      "iter 240: loss 2.9666\n",
      "iter 250: loss 2.9450\n",
      "iter 260: loss 2.8407\n",
      "iter 270: loss 2.7768\n",
      "iter 280: loss 2.7585\n",
      "iter 290: loss 2.7275\n",
      "iter 300: loss 2.6941\n",
      "iter 310: loss 2.6843\n",
      "iter 320: loss 2.6517\n",
      "iter 330: loss 2.6286\n",
      "iter 340: loss 2.6074\n",
      "iter 350: loss 2.6057\n",
      "iter 360: loss 2.5749\n",
      "iter 370: loss 2.5752\n",
      "iter 380: loss 2.5617\n",
      "iter 390: loss 2.5556\n",
      "iter 400: loss 2.5475\n",
      "iter 410: loss 2.5332\n",
      "iter 420: loss 2.5261\n",
      "iter 430: loss 2.5392\n",
      "iter 440: loss 2.5186\n",
      "iter 450: loss 2.5171\n",
      "iter 460: loss 2.5083\n",
      "iter 470: loss 2.4991\n",
      "iter 480: loss 2.5074\n",
      "iter 490: loss 2.5064\n",
      "iter 500: loss 2.5022\n",
      "step 500: train loss 2.4930, val loss 2.4948\n",
      "iter 510: loss 2.4851\n",
      "iter 520: loss 2.4936\n",
      "iter 530: loss 2.4671\n",
      "iter 540: loss 2.4974\n",
      "iter 550: loss 2.4735\n",
      "iter 560: loss 2.4780\n",
      "iter 570: loss 2.4595\n",
      "iter 580: loss 2.4676\n",
      "iter 590: loss 2.4609\n",
      "iter 600: loss 2.4528\n",
      "iter 610: loss 2.4226\n",
      "iter 620: loss 2.4141\n",
      "iter 630: loss 2.3757\n",
      "iter 640: loss 2.4202\n",
      "iter 650: loss 2.3600\n",
      "iter 660: loss 2.3368\n",
      "iter 670: loss 2.2834\n",
      "iter 680: loss 2.2795\n",
      "iter 690: loss 2.2384\n",
      "iter 700: loss 2.1683\n",
      "iter 710: loss 2.1748\n",
      "iter 720: loss 2.1620\n",
      "iter 730: loss 2.1343\n",
      "iter 740: loss 2.1019\n",
      "iter 750: loss 2.0931\n",
      "iter 760: loss 2.0506\n",
      "iter 770: loss 2.0342\n",
      "iter 780: loss 2.0115\n",
      "iter 790: loss 2.0055\n",
      "iter 800: loss 1.9650\n",
      "iter 810: loss 1.9523\n",
      "iter 820: loss 1.9272\n",
      "iter 830: loss 1.9271\n",
      "iter 840: loss 1.9118\n",
      "iter 850: loss 1.9171\n",
      "iter 860: loss 1.8771\n",
      "iter 870: loss 1.8901\n",
      "iter 880: loss 1.8379\n",
      "iter 890: loss 1.8397\n",
      "iter 900: loss 1.8190\n",
      "iter 910: loss 1.8217\n",
      "iter 920: loss 1.8005\n",
      "iter 930: loss 1.7837\n",
      "iter 940: loss 1.7392\n",
      "iter 950: loss 1.7483\n",
      "iter 960: loss 1.7350\n",
      "iter 970: loss 1.7350\n",
      "iter 980: loss 1.7308\n",
      "iter 990: loss 1.7034\n",
      "iter 1000: loss 1.6809\n",
      "step 1000: train loss 1.6421, val loss 1.6451\n",
      "iter 1010: loss 1.6972\n",
      "iter 1020: loss 1.6685\n",
      "iter 1030: loss 1.6678\n",
      "iter 1040: loss 1.6140\n",
      "iter 1050: loss 1.6462\n",
      "iter 1060: loss 1.6136\n",
      "iter 1070: loss 1.6460\n",
      "iter 1080: loss 1.6020\n",
      "iter 1090: loss 1.6013\n",
      "iter 1100: loss 1.5999\n",
      "iter 1110: loss 1.6169\n",
      "iter 1120: loss 1.5669\n",
      "iter 1130: loss 1.5547\n",
      "iter 1140: loss 1.5507\n",
      "iter 1150: loss 1.5461\n",
      "iter 1160: loss 1.5322\n",
      "iter 1170: loss 1.5250\n",
      "iter 1180: loss 1.5154\n",
      "iter 1190: loss 1.5404\n",
      "iter 1200: loss 1.5179\n",
      "iter 1210: loss 1.5397\n",
      "iter 1220: loss 1.4904\n",
      "iter 1230: loss 1.4878\n",
      "iter 1240: loss 1.5060\n",
      "iter 1250: loss 1.4703\n",
      "iter 1260: loss 1.4882\n",
      "iter 1270: loss 1.4597\n",
      "iter 1280: loss 1.4785\n",
      "iter 1290: loss 1.4634\n",
      "iter 1300: loss 1.5559\n",
      "iter 1310: loss 1.9081\n",
      "iter 1320: loss 1.6641\n",
      "iter 1330: loss 1.5437\n",
      "iter 1340: loss 1.5068\n",
      "iter 1350: loss 1.4861\n",
      "iter 1360: loss 1.4768\n",
      "iter 1370: loss 1.4698\n",
      "iter 1380: loss 1.4291\n",
      "iter 1390: loss 1.4450\n",
      "iter 1400: loss 1.4447\n",
      "iter 1410: loss 1.4460\n",
      "iter 1420: loss 1.4139\n",
      "iter 1430: loss 1.4189\n",
      "iter 1440: loss 1.4212\n",
      "iter 1450: loss 1.4121\n",
      "iter 1460: loss 1.4227\n",
      "iter 1470: loss 1.4317\n",
      "iter 1480: loss 1.3995\n",
      "iter 1490: loss 1.4188\n",
      "iter 1500: loss 1.3970\n",
      "step 1500: train loss 1.3621, val loss 1.3604\n",
      "iter 1510: loss 1.3747\n",
      "iter 1520: loss 1.3961\n",
      "iter 1530: loss 1.3815\n",
      "iter 1540: loss 1.3630\n",
      "iter 1550: loss 1.4026\n",
      "iter 1560: loss 1.3792\n",
      "iter 1570: loss 1.3771\n",
      "iter 1580: loss 1.3757\n",
      "iter 1590: loss 1.3942\n",
      "iter 1600: loss 1.3752\n",
      "iter 1610: loss 1.3984\n",
      "iter 1620: loss 1.3766\n",
      "iter 1630: loss 1.3533\n",
      "iter 1640: loss 1.3665\n",
      "iter 1650: loss 1.3406\n",
      "iter 1660: loss 1.3629\n",
      "iter 1670: loss 1.3564\n",
      "iter 1680: loss 1.3438\n",
      "iter 1690: loss 1.3384\n",
      "iter 1700: loss 1.3555\n",
      "iter 1710: loss 1.3418\n",
      "iter 1720: loss 1.3371\n",
      "iter 1730: loss 1.3403\n",
      "iter 1740: loss 1.3291\n",
      "iter 1750: loss 1.3604\n",
      "iter 1760: loss 1.3348\n",
      "iter 1770: loss 1.3361\n",
      "iter 1780: loss 1.3432\n",
      "iter 1790: loss 1.3237\n",
      "iter 1800: loss 1.3285\n",
      "iter 1810: loss 1.3109\n",
      "iter 1820: loss 1.3083\n",
      "iter 1830: loss 1.3256\n",
      "iter 1840: loss 1.3071\n",
      "iter 1850: loss 1.2856\n",
      "iter 1860: loss 1.2937\n",
      "iter 1870: loss 1.2899\n",
      "iter 1880: loss 1.2791\n",
      "iter 1890: loss 1.3295\n",
      "iter 1900: loss 1.3064\n",
      "iter 1910: loss 1.2880\n",
      "iter 1920: loss 1.2991\n",
      "iter 1930: loss 1.2997\n",
      "iter 1940: loss 1.2905\n",
      "iter 1950: loss 1.2751\n",
      "iter 1960: loss 1.2887\n",
      "iter 1970: loss 1.2834\n",
      "iter 1980: loss 1.2884\n",
      "iter 1990: loss 1.2798\n",
      "iter 2000: loss 1.2785\n",
      "step 2000: train loss 1.2370, val loss 1.2385\n",
      "iter 2010: loss 1.2910\n",
      "iter 2020: loss 1.3033\n",
      "iter 2030: loss 1.2675\n",
      "iter 2040: loss 1.2948\n",
      "iter 2050: loss 1.2793\n",
      "iter 2060: loss 1.2763\n",
      "iter 2070: loss 1.2716\n",
      "iter 2080: loss 1.2664\n",
      "iter 2090: loss 1.2610\n",
      "iter 2100: loss 1.2630\n",
      "iter 2110: loss 1.2388\n",
      "iter 2120: loss 1.2466\n",
      "iter 2130: loss 1.2521\n",
      "iter 2140: loss 1.2442\n",
      "iter 2150: loss 1.2592\n",
      "iter 2160: loss 1.2508\n",
      "iter 2170: loss 1.2250\n",
      "iter 2180: loss 1.2569\n",
      "iter 2190: loss 1.2180\n",
      "iter 2200: loss 1.2374\n",
      "iter 2210: loss 1.2234\n",
      "iter 2220: loss 1.2359\n",
      "iter 2230: loss 1.2565\n",
      "iter 2240: loss 1.2305\n",
      "iter 2250: loss 1.2511\n",
      "iter 2260: loss 1.2325\n",
      "iter 2270: loss 1.2890\n",
      "iter 2280: loss 1.2462\n",
      "iter 2290: loss 1.2384\n",
      "iter 2300: loss 1.2345\n",
      "iter 2310: loss 1.2264\n",
      "iter 2320: loss 1.2316\n",
      "iter 2330: loss 1.2381\n",
      "iter 2340: loss 1.2178\n",
      "iter 2350: loss 1.2163\n",
      "iter 2360: loss 1.2421\n",
      "iter 2370: loss 1.2158\n",
      "iter 2380: loss 1.2270\n",
      "iter 2390: loss 1.2054\n",
      "iter 2400: loss 1.2081\n",
      "iter 2410: loss 1.2205\n",
      "iter 2420: loss 1.2084\n",
      "iter 2430: loss 1.2095\n",
      "iter 2440: loss 1.2170\n",
      "iter 2450: loss 1.2237\n",
      "iter 2460: loss 1.2099\n",
      "iter 2470: loss 1.2103\n",
      "iter 2480: loss 1.2198\n",
      "iter 2490: loss 1.1875\n",
      "iter 2500: loss 1.1947\n",
      "step 2500: train loss 1.1552, val loss 1.1546\n",
      "iter 2510: loss 1.1838\n",
      "iter 2520: loss 1.1981\n",
      "iter 2530: loss 1.2056\n",
      "iter 2540: loss 1.1964\n",
      "iter 2550: loss 1.1997\n",
      "iter 2560: loss 1.2165\n",
      "iter 2570: loss 1.2138\n",
      "iter 2580: loss 1.1827\n",
      "iter 2590: loss 1.1851\n",
      "iter 2600: loss 1.2051\n",
      "iter 2610: loss 1.1900\n",
      "iter 2620: loss 1.1921\n",
      "iter 2630: loss 1.1866\n",
      "iter 2640: loss 1.1951\n",
      "iter 2650: loss 1.1802\n",
      "iter 2660: loss 1.1848\n",
      "iter 2670: loss 1.1666\n",
      "iter 2680: loss 1.1792\n",
      "iter 2690: loss 1.1777\n",
      "iter 2700: loss 1.2046\n",
      "iter 2710: loss 1.1646\n",
      "iter 2720: loss 1.1542\n",
      "iter 2730: loss 1.1530\n",
      "iter 2740: loss 1.1740\n",
      "iter 2750: loss 1.1604\n",
      "iter 2760: loss 1.1549\n",
      "iter 2770: loss 1.1635\n",
      "iter 2780: loss 1.1739\n",
      "iter 2790: loss 1.1673\n",
      "iter 2800: loss 1.1663\n",
      "iter 2810: loss 1.1642\n",
      "iter 2820: loss 1.1689\n",
      "iter 2830: loss 1.1568\n",
      "iter 2840: loss 1.1365\n",
      "iter 2850: loss 1.1650\n",
      "iter 2860: loss 1.1692\n",
      "iter 2870: loss 1.1539\n",
      "iter 2880: loss 1.1738\n",
      "iter 2890: loss 1.1679\n",
      "iter 2900: loss 1.1311\n",
      "iter 2910: loss 1.1444\n",
      "iter 2920: loss 1.1383\n",
      "iter 2930: loss 1.1516\n",
      "iter 2940: loss 1.1496\n",
      "iter 2950: loss 1.1486\n",
      "iter 2960: loss 1.1419\n",
      "iter 2970: loss 1.1662\n",
      "iter 2980: loss 1.1521\n",
      "iter 2990: loss 1.1506\n",
      "iter 3000: loss 1.1513\n",
      "step 3000: train loss 1.0853, val loss 1.0897\n",
      "iter 3010: loss 1.1477\n",
      "iter 3020: loss 1.1455\n",
      "iter 3030: loss 1.1496\n",
      "iter 3040: loss 1.1520\n",
      "iter 3050: loss 1.1240\n",
      "iter 3060: loss 1.1418\n",
      "iter 3070: loss 1.1243\n",
      "iter 3080: loss 1.1092\n",
      "iter 3090: loss 1.1235\n",
      "iter 3100: loss 1.1294\n",
      "iter 3110: loss 1.1038\n",
      "iter 3120: loss 1.1329\n",
      "iter 3130: loss 1.1415\n",
      "iter 3140: loss 1.1058\n",
      "iter 3150: loss 1.1377\n",
      "iter 3160: loss 1.1197\n",
      "iter 3170: loss 1.1247\n",
      "iter 3180: loss 1.1215\n",
      "iter 3190: loss 1.1362\n",
      "iter 3200: loss 1.1353\n",
      "iter 3210: loss 1.1281\n",
      "iter 3220: loss 1.1262\n",
      "iter 3230: loss 1.1133\n",
      "iter 3240: loss 1.1140\n",
      "iter 3250: loss 1.1298\n",
      "iter 3260: loss 1.1154\n",
      "iter 3270: loss 1.1175\n",
      "iter 3280: loss 1.1093\n",
      "iter 3290: loss 1.1151\n",
      "iter 3300: loss 1.0946\n",
      "iter 3310: loss 1.1061\n",
      "iter 3320: loss 1.1006\n",
      "iter 3330: loss 1.1137\n",
      "iter 3340: loss 1.0999\n",
      "iter 3350: loss 1.0998\n",
      "iter 3360: loss 1.1006\n",
      "iter 3370: loss 1.1265\n",
      "iter 3380: loss 1.1054\n",
      "iter 3390: loss 1.0886\n",
      "iter 3400: loss 1.0988\n",
      "iter 3410: loss 1.1159\n",
      "iter 3420: loss 1.0959\n",
      "iter 3430: loss 1.1158\n",
      "iter 3440: loss 1.0931\n",
      "iter 3450: loss 1.0986\n",
      "iter 3460: loss 1.1091\n",
      "iter 3470: loss 1.0853\n",
      "iter 3480: loss 1.0850\n",
      "iter 3490: loss 1.1029\n",
      "iter 3500: loss 1.0946\n",
      "step 3500: train loss 1.0291, val loss 1.0291\n",
      "iter 3510: loss 1.1073\n",
      "iter 3520: loss 1.0868\n",
      "iter 3530: loss 1.0739\n",
      "iter 3540: loss 1.0979\n",
      "iter 3550: loss 1.0723\n",
      "iter 3560: loss 1.1164\n",
      "iter 3570: loss 1.1035\n",
      "iter 3580: loss 1.0762\n",
      "iter 3590: loss 1.0730\n",
      "iter 3600: loss 1.0835\n",
      "iter 3610: loss 1.0799\n",
      "iter 3620: loss 1.0739\n",
      "iter 3630: loss 1.0658\n",
      "iter 3640: loss 1.1009\n",
      "iter 3650: loss 1.0950\n",
      "iter 3660: loss 1.0805\n",
      "iter 3670: loss 1.0965\n",
      "iter 3680: loss 1.0589\n",
      "iter 3690: loss 1.0914\n",
      "iter 3700: loss 1.0763\n",
      "iter 3710: loss 1.0698\n",
      "iter 3720: loss 1.0831\n",
      "iter 3730: loss 1.0774\n",
      "iter 3740: loss 1.0651\n",
      "iter 3750: loss 1.0747\n",
      "iter 3760: loss 1.0783\n",
      "iter 3770: loss 1.0891\n",
      "iter 3780: loss 1.0764\n",
      "iter 3790: loss 1.0756\n",
      "iter 3800: loss 1.0550\n",
      "iter 3810: loss 1.0433\n",
      "iter 3820: loss 1.0621\n",
      "iter 3830: loss 1.0749\n",
      "iter 3840: loss 1.0716\n",
      "iter 3850: loss 1.0622\n",
      "iter 3860: loss 1.0656\n",
      "iter 3870: loss 1.0587\n",
      "iter 3880: loss 1.0611\n",
      "iter 3890: loss 1.0740\n",
      "iter 3900: loss 1.0451\n",
      "iter 3910: loss 1.0463\n",
      "iter 3920: loss 1.0538\n",
      "iter 3930: loss 1.0613\n",
      "iter 3940: loss 1.0563\n",
      "iter 3950: loss 1.0229\n",
      "iter 3960: loss 1.0593\n",
      "iter 3970: loss 1.0460\n",
      "iter 3980: loss 1.0559\n",
      "iter 3990: loss 1.0526\n",
      "iter 4000: loss 1.0419\n",
      "step 4000: train loss 0.9734, val loss 0.9756\n",
      "iter 4010: loss 1.0304\n",
      "iter 4020: loss 1.0307\n",
      "iter 4030: loss 1.0573\n",
      "iter 4040: loss 1.0284\n",
      "iter 4050: loss 1.0475\n",
      "iter 4060: loss 1.0440\n",
      "iter 4070: loss 1.0497\n",
      "iter 4080: loss 1.0486\n",
      "iter 4090: loss 1.0375\n",
      "iter 4100: loss 1.0245\n",
      "iter 4110: loss 1.0378\n",
      "iter 4120: loss 1.0191\n",
      "iter 4130: loss 1.0320\n",
      "iter 4140: loss 1.0338\n",
      "iter 4150: loss 1.0304\n",
      "iter 4160: loss 1.0311\n",
      "iter 4170: loss 1.0584\n",
      "iter 4180: loss 1.0222\n",
      "iter 4190: loss 1.0527\n",
      "iter 4200: loss 1.0346\n",
      "iter 4210: loss 1.0217\n",
      "iter 4220: loss 1.0331\n",
      "iter 4230: loss 1.0197\n",
      "iter 4240: loss 1.0372\n",
      "iter 4250: loss 1.0367\n",
      "iter 4260: loss 1.0328\n",
      "iter 4270: loss 1.0220\n",
      "iter 4280: loss 1.0155\n",
      "iter 4290: loss 1.0270\n",
      "iter 4300: loss 1.0239\n",
      "iter 4310: loss 1.0293\n",
      "iter 4320: loss 1.0210\n",
      "iter 4330: loss 1.0234\n",
      "iter 4340: loss 1.0244\n",
      "iter 4350: loss 1.0408\n",
      "iter 4360: loss 0.9970\n",
      "iter 4370: loss 1.0063\n",
      "iter 4380: loss 1.0136\n",
      "iter 4390: loss 1.0271\n",
      "iter 4400: loss 1.0173\n",
      "iter 4410: loss 1.0151\n",
      "iter 4420: loss 1.0164\n",
      "iter 4430: loss 1.0166\n",
      "iter 4440: loss 1.0123\n",
      "iter 4450: loss 0.9971\n",
      "iter 4460: loss 1.0050\n",
      "iter 4470: loss 0.9919\n",
      "iter 4480: loss 1.0083\n",
      "iter 4490: loss 1.0036\n",
      "iter 4500: loss 1.0170\n",
      "step 4500: train loss 0.9197, val loss 0.9202\n",
      "iter 4510: loss 1.0074\n",
      "iter 4520: loss 1.0176\n",
      "iter 4530: loss 0.9935\n",
      "iter 4540: loss 1.0042\n",
      "iter 4550: loss 0.9916\n",
      "iter 4560: loss 1.0029\n",
      "iter 4570: loss 1.0180\n",
      "iter 4580: loss 1.0064\n",
      "iter 4590: loss 1.0078\n",
      "iter 4600: loss 0.9938\n",
      "iter 4610: loss 1.0108\n",
      "iter 4620: loss 1.0071\n",
      "iter 4630: loss 1.0035\n",
      "iter 4640: loss 1.0129\n",
      "iter 4650: loss 0.9888\n",
      "iter 4660: loss 0.9963\n",
      "iter 4670: loss 0.9949\n",
      "iter 4680: loss 0.9907\n",
      "iter 4690: loss 0.9933\n",
      "iter 4700: loss 0.9865\n",
      "iter 4710: loss 0.9866\n",
      "iter 4720: loss 0.9861\n",
      "iter 4730: loss 0.9697\n",
      "iter 4740: loss 0.9786\n",
      "iter 4750: loss 1.0099\n",
      "iter 4760: loss 0.9905\n",
      "iter 4770: loss 0.9912\n",
      "iter 4780: loss 0.9684\n",
      "iter 4790: loss 0.9926\n",
      "iter 4800: loss 0.9728\n",
      "iter 4810: loss 0.9983\n",
      "iter 4820: loss 0.9802\n",
      "iter 4830: loss 0.9856\n",
      "iter 4840: loss 0.9796\n",
      "iter 4850: loss 0.9680\n",
      "iter 4860: loss 0.9877\n",
      "iter 4870: loss 0.9812\n",
      "iter 4880: loss 0.9780\n",
      "iter 4890: loss 0.9767\n",
      "iter 4900: loss 0.9741\n",
      "iter 4910: loss 0.9782\n",
      "iter 4920: loss 0.9777\n",
      "iter 4930: loss 0.9600\n",
      "iter 4940: loss 0.9818\n",
      "iter 4950: loss 0.9663\n",
      "iter 4960: loss 0.9518\n",
      "iter 4970: loss 0.9873\n",
      "iter 4980: loss 0.9691\n",
      "iter 4990: loss 0.9685\n",
      "iter 5000: loss 0.9742\n",
      "step 5000: train loss 0.8737, val loss 0.8750\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.2041\n",
      "step 0: train loss 4.2063, val loss 4.2082\n",
      "iter 10: loss 3.4426\n",
      "iter 20: loss 3.0483\n",
      "iter 30: loss 2.8484\n",
      "iter 40: loss 2.7220\n",
      "iter 50: loss 2.6601\n",
      "iter 60: loss 2.6099\n",
      "iter 70: loss 2.5854\n",
      "iter 80: loss 2.5796\n",
      "iter 90: loss 2.5441\n",
      "iter 100: loss 2.5479\n",
      "iter 110: loss 2.5185\n",
      "iter 120: loss 2.5203\n",
      "iter 130: loss 2.5353\n",
      "iter 140: loss 2.5153\n",
      "iter 150: loss 2.5110\n",
      "iter 160: loss 2.4975\n",
      "iter 170: loss 2.4949\n",
      "iter 180: loss 2.5018\n",
      "iter 190: loss 2.4920\n",
      "iter 200: loss 2.4596\n",
      "iter 210: loss 2.4673\n",
      "iter 220: loss 2.4723\n",
      "iter 230: loss 2.4658\n",
      "iter 240: loss 2.4352\n",
      "iter 250: loss 2.4251\n",
      "iter 260: loss 2.4321\n",
      "iter 270: loss 2.4363\n",
      "iter 280: loss 2.4141\n",
      "iter 290: loss 2.3815\n",
      "iter 300: loss 2.3597\n",
      "iter 310: loss 2.3494\n",
      "iter 320: loss 2.3532\n",
      "iter 330: loss 2.3295\n",
      "iter 340: loss 2.3117\n",
      "iter 350: loss 2.2816\n",
      "iter 360: loss 2.2844\n",
      "iter 370: loss 2.2556\n",
      "iter 380: loss 2.2367\n",
      "iter 390: loss 2.2145\n",
      "iter 400: loss 2.2144\n",
      "iter 410: loss 2.1645\n",
      "iter 420: loss 2.1446\n",
      "iter 430: loss 2.1287\n",
      "iter 440: loss 2.1334\n",
      "iter 450: loss 2.0962\n",
      "iter 460: loss 2.0944\n",
      "iter 470: loss 2.0675\n",
      "iter 480: loss 2.0323\n",
      "iter 490: loss 2.0361\n",
      "iter 500: loss 2.0293\n",
      "step 500: train loss 1.9639, val loss 1.9637\n",
      "iter 510: loss 2.0173\n",
      "iter 520: loss 2.0016\n",
      "iter 530: loss 1.9806\n",
      "iter 540: loss 1.9482\n",
      "iter 550: loss 1.9576\n",
      "iter 560: loss 1.9350\n",
      "iter 570: loss 1.9223\n",
      "iter 580: loss 1.9231\n",
      "iter 590: loss 1.9254\n",
      "iter 600: loss 1.8863\n",
      "iter 610: loss 1.8849\n",
      "iter 620: loss 1.8917\n",
      "iter 630: loss 1.8358\n",
      "iter 640: loss 1.8544\n",
      "iter 650: loss 1.8732\n",
      "iter 660: loss 1.8554\n",
      "iter 670: loss 1.8236\n",
      "iter 680: loss 1.8130\n",
      "iter 690: loss 1.7888\n",
      "iter 700: loss 1.8206\n",
      "iter 710: loss 1.7981\n",
      "iter 720: loss 1.7751\n",
      "iter 730: loss 1.7796\n",
      "iter 740: loss 1.7686\n",
      "iter 750: loss 1.7532\n",
      "iter 760: loss 1.7660\n",
      "iter 770: loss 1.7420\n",
      "iter 780: loss 1.7480\n",
      "iter 790: loss 1.7160\n",
      "iter 800: loss 1.6989\n",
      "iter 810: loss 1.7150\n",
      "iter 820: loss 1.7043\n",
      "iter 830: loss 1.6703\n",
      "iter 840: loss 1.6993\n",
      "iter 850: loss 1.6973\n",
      "iter 860: loss 1.6760\n",
      "iter 870: loss 1.6801\n",
      "iter 880: loss 1.6461\n",
      "iter 890: loss 1.6508\n",
      "iter 900: loss 1.6482\n",
      "iter 910: loss 1.6237\n",
      "iter 920: loss 1.6153\n",
      "iter 930: loss 1.6220\n",
      "iter 940: loss 1.6007\n",
      "iter 950: loss 1.6261\n",
      "iter 960: loss 1.6209\n",
      "iter 970: loss 1.6064\n",
      "iter 980: loss 1.6153\n",
      "iter 990: loss 1.5850\n",
      "iter 1000: loss 1.5906\n",
      "step 1000: train loss 1.5338, val loss 1.5371\n",
      "iter 1010: loss 1.5945\n",
      "iter 1020: loss 1.5934\n",
      "iter 1030: loss 1.5481\n",
      "iter 1040: loss 1.5621\n",
      "iter 1050: loss 1.5681\n",
      "iter 1060: loss 1.5580\n",
      "iter 1070: loss 1.5525\n",
      "iter 1080: loss 1.5413\n",
      "iter 1090: loss 1.5570\n",
      "iter 1100: loss 1.5436\n",
      "iter 1110: loss 1.5470\n",
      "iter 1120: loss 1.5289\n",
      "iter 1130: loss 1.5295\n",
      "iter 1140: loss 1.5490\n",
      "iter 1150: loss 1.5562\n",
      "iter 1160: loss 1.5161\n",
      "iter 1170: loss 1.5301\n",
      "iter 1180: loss 1.5317\n",
      "iter 1190: loss 1.5154\n",
      "iter 1200: loss 1.5203\n",
      "iter 1210: loss 1.5172\n",
      "iter 1220: loss 1.5000\n",
      "iter 1230: loss 1.4841\n",
      "iter 1240: loss 1.4989\n",
      "iter 1250: loss 1.4996\n",
      "iter 1260: loss 1.5124\n",
      "iter 1270: loss 1.5067\n",
      "iter 1280: loss 1.4856\n",
      "iter 1290: loss 1.4761\n",
      "iter 1300: loss 1.4902\n",
      "iter 1310: loss 1.4953\n",
      "iter 1320: loss 1.4650\n",
      "iter 1330: loss 1.4555\n",
      "iter 1340: loss 1.4861\n",
      "iter 1350: loss 1.4542\n",
      "iter 1360: loss 1.4472\n",
      "iter 1370: loss 1.4877\n",
      "iter 1380: loss 1.4563\n",
      "iter 1390: loss 1.4407\n",
      "iter 1400: loss 1.4596\n",
      "iter 1410: loss 1.4333\n",
      "iter 1420: loss 1.4431\n",
      "iter 1430: loss 1.4269\n",
      "iter 1440: loss 1.4318\n",
      "iter 1450: loss 1.4338\n",
      "iter 1460: loss 1.4102\n",
      "iter 1470: loss 1.4406\n",
      "iter 1480: loss 1.4131\n",
      "iter 1490: loss 1.4274\n",
      "iter 1500: loss 1.4231\n",
      "step 1500: train loss 1.3614, val loss 1.3636\n",
      "iter 1510: loss 1.4390\n",
      "iter 1520: loss 1.3991\n",
      "iter 1530: loss 1.4592\n",
      "iter 1540: loss 1.3965\n",
      "iter 1550: loss 1.4094\n",
      "iter 1560: loss 1.3944\n",
      "iter 1570: loss 1.3899\n",
      "iter 1580: loss 1.4011\n",
      "iter 1590: loss 1.3783\n",
      "iter 1600: loss 1.4036\n",
      "iter 1610: loss 1.3787\n",
      "iter 1620: loss 1.3811\n",
      "iter 1630: loss 1.3732\n",
      "iter 1640: loss 1.3830\n",
      "iter 1650: loss 1.3916\n",
      "iter 1660: loss 1.4004\n",
      "iter 1670: loss 1.3706\n",
      "iter 1680: loss 1.3752\n",
      "iter 1690: loss 1.3897\n",
      "iter 1700: loss 1.3948\n",
      "iter 1710: loss 1.3849\n",
      "iter 1720: loss 1.3668\n",
      "iter 1730: loss 1.3724\n",
      "iter 1740: loss 1.3712\n",
      "iter 1750: loss 1.3981\n",
      "iter 1760: loss 1.3729\n",
      "iter 1770: loss 1.3579\n",
      "iter 1780: loss 1.3422\n",
      "iter 1790: loss 1.3714\n",
      "iter 1800: loss 1.3843\n",
      "iter 1810: loss 1.3680\n",
      "iter 1820: loss 1.3633\n",
      "iter 1830: loss 1.3573\n",
      "iter 1840: loss 1.3615\n",
      "iter 1850: loss 1.3525\n",
      "iter 1860: loss 1.3645\n",
      "iter 1870: loss 1.3519\n",
      "iter 1880: loss 1.3423\n",
      "iter 1890: loss 1.3489\n",
      "iter 1900: loss 1.3450\n",
      "iter 1910: loss 1.3648\n",
      "iter 1920: loss 1.3343\n",
      "iter 1930: loss 1.3532\n",
      "iter 1940: loss 1.3424\n",
      "iter 1950: loss 1.3294\n",
      "iter 1960: loss 1.3222\n",
      "iter 1970: loss 1.3189\n",
      "iter 1980: loss 1.3130\n",
      "iter 1990: loss 1.3428\n",
      "iter 2000: loss 1.3095\n",
      "step 2000: train loss 1.2634, val loss 1.2695\n",
      "iter 2010: loss 1.3346\n",
      "iter 2020: loss 1.3428\n",
      "iter 2030: loss 1.3362\n",
      "iter 2040: loss 1.3171\n",
      "iter 2050: loss 1.3192\n",
      "iter 2060: loss 1.3095\n",
      "iter 2070: loss 1.3024\n",
      "iter 2080: loss 1.3285\n",
      "iter 2090: loss 1.3267\n",
      "iter 2100: loss 1.3186\n",
      "iter 2110: loss 1.3109\n",
      "iter 2120: loss 1.3020\n",
      "iter 2130: loss 1.3104\n",
      "iter 2140: loss 1.2839\n",
      "iter 2150: loss 1.3136\n",
      "iter 2160: loss 1.3025\n",
      "iter 2170: loss 1.2993\n",
      "iter 2180: loss 1.2852\n",
      "iter 2190: loss 1.2906\n",
      "iter 2200: loss 1.2814\n",
      "iter 2210: loss 1.3131\n",
      "iter 2220: loss 1.2994\n",
      "iter 2230: loss 1.2998\n",
      "iter 2240: loss 1.2955\n",
      "iter 2250: loss 1.2850\n",
      "iter 2260: loss 1.2762\n",
      "iter 2270: loss 1.2756\n",
      "iter 2280: loss 1.2683\n",
      "iter 2290: loss 1.2611\n",
      "iter 2300: loss 1.2696\n",
      "iter 2310: loss 1.2951\n",
      "iter 2320: loss 1.2883\n",
      "iter 2330: loss 1.2895\n",
      "iter 2340: loss 1.2664\n",
      "iter 2350: loss 1.2792\n",
      "iter 2360: loss 1.2732\n",
      "iter 2370: loss 1.2637\n",
      "iter 2380: loss 1.2866\n",
      "iter 2390: loss 1.2465\n",
      "iter 2400: loss 1.2704\n",
      "iter 2410: loss 1.2673\n",
      "iter 2420: loss 1.2599\n",
      "iter 2430: loss 1.2670\n",
      "iter 2440: loss 1.2645\n",
      "iter 2450: loss 1.2623\n",
      "iter 2460: loss 1.2640\n",
      "iter 2470: loss 1.2539\n",
      "iter 2480: loss 1.2616\n",
      "iter 2490: loss 1.2531\n",
      "iter 2500: loss 1.2519\n",
      "step 2500: train loss 1.1937, val loss 1.1993\n",
      "iter 2510: loss 1.2651\n",
      "iter 2520: loss 1.2322\n",
      "iter 2530: loss 1.2592\n",
      "iter 2540: loss 1.2424\n",
      "iter 2550: loss 1.2643\n",
      "iter 2560: loss 1.2502\n",
      "iter 2570: loss 1.2474\n",
      "iter 2580: loss 1.2393\n",
      "iter 2590: loss 1.2510\n",
      "iter 2600: loss 1.2605\n",
      "iter 2610: loss 1.2284\n",
      "iter 2620: loss 1.2258\n",
      "iter 2630: loss 1.2347\n",
      "iter 2640: loss 1.2383\n",
      "iter 2650: loss 1.2361\n",
      "iter 2660: loss 1.2389\n",
      "iter 2670: loss 1.2451\n",
      "iter 2680: loss 1.2561\n",
      "iter 2690: loss 1.2258\n",
      "iter 2700: loss 1.2325\n",
      "iter 2710: loss 1.2256\n",
      "iter 2720: loss 1.2277\n",
      "iter 2730: loss 1.2309\n",
      "iter 2740: loss 1.2408\n",
      "iter 2750: loss 1.2383\n",
      "iter 2760: loss 1.2113\n",
      "iter 2770: loss 1.2225\n",
      "iter 2780: loss 1.2127\n",
      "iter 2790: loss 1.2298\n",
      "iter 2800: loss 1.2262\n",
      "iter 2810: loss 1.1998\n",
      "iter 2820: loss 1.2130\n",
      "iter 2830: loss 1.2338\n",
      "iter 2840: loss 1.2064\n",
      "iter 2850: loss 1.1880\n",
      "iter 2860: loss 1.2202\n",
      "iter 2870: loss 1.2236\n",
      "iter 2880: loss 1.1933\n",
      "iter 2890: loss 1.2225\n",
      "iter 2900: loss 1.2012\n",
      "iter 2910: loss 1.2065\n",
      "iter 2920: loss 1.2035\n",
      "iter 2930: loss 1.2115\n",
      "iter 2940: loss 1.2118\n",
      "iter 2950: loss 1.2199\n",
      "iter 2960: loss 1.1932\n",
      "iter 2970: loss 1.2113\n",
      "iter 2980: loss 1.2031\n",
      "iter 2990: loss 1.1891\n",
      "iter 3000: loss 1.1974\n",
      "step 3000: train loss 1.1384, val loss 1.1407\n",
      "iter 3010: loss 1.1977\n",
      "iter 3020: loss 1.1736\n",
      "iter 3030: loss 1.2066\n",
      "iter 3040: loss 1.1892\n",
      "iter 3050: loss 1.1904\n",
      "iter 3060: loss 1.2043\n",
      "iter 3070: loss 1.1638\n",
      "iter 3080: loss 1.1867\n",
      "iter 3090: loss 1.1729\n",
      "iter 3100: loss 1.1826\n",
      "iter 3110: loss 1.1934\n",
      "iter 3120: loss 1.1903\n",
      "iter 3130: loss 1.1961\n",
      "iter 3140: loss 1.2041\n",
      "iter 3150: loss 1.1684\n",
      "iter 3160: loss 1.1821\n",
      "iter 3170: loss 1.1800\n",
      "iter 3180: loss 1.2034\n",
      "iter 3190: loss 1.1825\n",
      "iter 3200: loss 1.1833\n",
      "iter 3210: loss 1.1747\n",
      "iter 3220: loss 1.1972\n",
      "iter 3230: loss 1.2002\n",
      "iter 3240: loss 1.1825\n",
      "iter 3250: loss 1.1615\n",
      "iter 3260: loss 1.1764\n",
      "iter 3270: loss 1.1803\n",
      "iter 3280: loss 1.1598\n",
      "iter 3290: loss 1.1535\n",
      "iter 3300: loss 1.1701\n",
      "iter 3310: loss 1.1599\n",
      "iter 3320: loss 1.1594\n",
      "iter 3330: loss 1.1731\n",
      "iter 3340: loss 1.1609\n",
      "iter 3350: loss 1.1736\n",
      "iter 3360: loss 1.1582\n",
      "iter 3370: loss 1.1422\n",
      "iter 3380: loss 1.1596\n",
      "iter 3390: loss 1.1733\n",
      "iter 3400: loss 1.1522\n",
      "iter 3410: loss 1.1662\n",
      "iter 3420: loss 1.1541\n",
      "iter 3430: loss 1.1540\n",
      "iter 3440: loss 1.1424\n",
      "iter 3450: loss 1.1435\n",
      "iter 3460: loss 1.1576\n",
      "iter 3470: loss 1.1600\n",
      "iter 3480: loss 1.1619\n",
      "iter 3490: loss 1.1591\n",
      "iter 3500: loss 1.1464\n",
      "step 3500: train loss 1.0863, val loss 1.0897\n",
      "iter 3510: loss 1.1453\n",
      "iter 3520: loss 1.1396\n",
      "iter 3530: loss 1.1423\n",
      "iter 3540: loss 1.1653\n",
      "iter 3550: loss 1.1530\n",
      "iter 3560: loss 1.1173\n",
      "iter 3570: loss 1.1543\n",
      "iter 3580: loss 1.1426\n",
      "iter 3590: loss 1.1373\n",
      "iter 3600: loss 1.1302\n",
      "iter 3610: loss 1.1292\n",
      "iter 3620: loss 1.1330\n",
      "iter 3630: loss 1.1692\n",
      "iter 3640: loss 1.1505\n",
      "iter 3650: loss 1.1418\n",
      "iter 3660: loss 1.1493\n",
      "iter 3670: loss 1.1378\n",
      "iter 3680: loss 1.1390\n",
      "iter 3690: loss 1.1408\n",
      "iter 3700: loss 1.1460\n",
      "iter 3710: loss 1.1123\n",
      "iter 3720: loss 1.1224\n",
      "iter 3730: loss 1.1485\n",
      "iter 3740: loss 1.1401\n",
      "iter 3750: loss 1.1323\n",
      "iter 3760: loss 1.1273\n",
      "iter 3770: loss 1.1182\n",
      "iter 3780: loss 1.1270\n",
      "iter 3790: loss 1.1428\n",
      "iter 3800: loss 1.1373\n",
      "iter 3810: loss 1.1274\n",
      "iter 3820: loss 1.1347\n",
      "iter 3830: loss 1.1203\n",
      "iter 3840: loss 1.1010\n",
      "iter 3850: loss 1.1179\n",
      "iter 3860: loss 1.1636\n",
      "iter 3870: loss 1.1129\n",
      "iter 3880: loss 1.1214\n",
      "iter 3890: loss 1.1399\n",
      "iter 3900: loss 1.0962\n",
      "iter 3910: loss 1.1094\n",
      "iter 3920: loss 1.1130\n",
      "iter 3930: loss 1.1195\n",
      "iter 3940: loss 1.1075\n",
      "iter 3950: loss 1.1224\n",
      "iter 3960: loss 1.1068\n",
      "iter 3970: loss 1.1244\n",
      "iter 3980: loss 1.0987\n",
      "iter 3990: loss 1.1123\n",
      "iter 4000: loss 1.0941\n",
      "step 4000: train loss 1.0306, val loss 1.0369\n",
      "iter 4010: loss 1.1054\n",
      "iter 4020: loss 1.1004\n",
      "iter 4030: loss 1.1090\n",
      "iter 4040: loss 1.0947\n",
      "iter 4050: loss 1.1014\n",
      "iter 4060: loss 1.0935\n",
      "iter 4070: loss 1.1061\n",
      "iter 4080: loss 1.1089\n",
      "iter 4090: loss 1.1318\n",
      "iter 4100: loss 1.1014\n",
      "iter 4110: loss 1.0977\n",
      "iter 4120: loss 1.0950\n",
      "iter 4130: loss 1.1062\n",
      "iter 4140: loss 1.0926\n",
      "iter 4150: loss 1.1040\n",
      "iter 4160: loss 1.1022\n",
      "iter 4170: loss 1.0955\n",
      "iter 4180: loss 1.0740\n",
      "iter 4190: loss 1.0905\n",
      "iter 4200: loss 1.0938\n",
      "iter 4210: loss 1.0835\n",
      "iter 4220: loss 1.1022\n",
      "iter 4230: loss 1.0750\n",
      "iter 4240: loss 1.0823\n",
      "iter 4250: loss 1.0913\n",
      "iter 4260: loss 1.0876\n",
      "iter 4270: loss 1.0694\n",
      "iter 4280: loss 1.0808\n",
      "iter 4290: loss 1.0700\n",
      "iter 4300: loss 1.0898\n",
      "iter 4310: loss 1.0731\n",
      "iter 4320: loss 1.0873\n",
      "iter 4330: loss 1.0728\n",
      "iter 4340: loss 1.0777\n",
      "iter 4350: loss 1.0954\n",
      "iter 4360: loss 1.0919\n",
      "iter 4370: loss 1.0681\n",
      "iter 4380: loss 1.0776\n",
      "iter 4390: loss 1.0928\n",
      "iter 4400: loss 1.0633\n",
      "iter 4410: loss 1.0830\n",
      "iter 4420: loss 1.0668\n",
      "iter 4430: loss 1.0766\n",
      "iter 4440: loss 1.0741\n",
      "iter 4450: loss 1.0679\n",
      "iter 4460: loss 1.0854\n",
      "iter 4470: loss 1.0483\n",
      "iter 4480: loss 1.0606\n",
      "iter 4490: loss 1.0830\n",
      "iter 4500: loss 1.0570\n",
      "step 4500: train loss 0.9862, val loss 0.9892\n",
      "iter 4510: loss 1.0693\n",
      "iter 4520: loss 1.0762\n",
      "iter 4530: loss 1.0629\n",
      "iter 4540: loss 1.0649\n",
      "iter 4550: loss 1.0662\n",
      "iter 4560: loss 1.0759\n",
      "iter 4570: loss 1.0803\n",
      "iter 4580: loss 1.0618\n",
      "iter 4590: loss 1.0443\n",
      "iter 4600: loss 1.0681\n",
      "iter 4610: loss 1.0546\n",
      "iter 4620: loss 1.0605\n",
      "iter 4630: loss 1.0620\n",
      "iter 4640: loss 1.0361\n",
      "iter 4650: loss 1.0508\n",
      "iter 4660: loss 1.0619\n",
      "iter 4670: loss 1.0460\n",
      "iter 4680: loss 1.0649\n",
      "iter 4690: loss 1.0650\n",
      "iter 4700: loss 1.0478\n",
      "iter 4710: loss 1.0487\n",
      "iter 4720: loss 1.0456\n",
      "iter 4730: loss 1.0521\n",
      "iter 4740: loss 1.0473\n",
      "iter 4750: loss 1.0423\n",
      "iter 4760: loss 1.0423\n",
      "iter 4770: loss 1.0426\n",
      "iter 4780: loss 1.0301\n",
      "iter 4790: loss 1.0425\n",
      "iter 4800: loss 1.0540\n",
      "iter 4810: loss 1.0375\n",
      "iter 4820: loss 1.0366\n",
      "iter 4830: loss 1.0558\n",
      "iter 4840: loss 1.0434\n",
      "iter 4850: loss 1.0439\n",
      "iter 4860: loss 1.0375\n",
      "iter 4870: loss 1.0299\n",
      "iter 4880: loss 1.0476\n",
      "iter 4890: loss 1.0392\n",
      "iter 4900: loss 1.0380\n",
      "iter 4910: loss 1.0548\n",
      "iter 4920: loss 1.0401\n",
      "iter 4930: loss 1.0617\n",
      "iter 4940: loss 1.0424\n",
      "iter 4950: loss 1.0341\n",
      "iter 4960: loss 1.0529\n",
      "iter 4970: loss 1.0210\n",
      "iter 4980: loss 1.0337\n",
      "iter 4990: loss 1.0207\n",
      "iter 5000: loss 1.0251\n",
      "step 5000: train loss 0.9431, val loss 0.9452\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.1339\n",
      "step 0: train loss 4.1581, val loss 4.1583\n",
      "iter 10: loss 3.4328\n",
      "iter 20: loss 3.2376\n",
      "iter 30: loss 3.0787\n",
      "iter 40: loss 3.0750\n",
      "iter 50: loss 3.0616\n",
      "iter 60: loss 2.9808\n",
      "iter 70: loss 2.9261\n",
      "iter 80: loss 2.8435\n",
      "iter 90: loss 2.8157\n",
      "iter 100: loss 2.7584\n",
      "iter 110: loss 2.7211\n",
      "iter 120: loss 2.7054\n",
      "iter 130: loss 2.6672\n",
      "iter 140: loss 2.6685\n",
      "iter 150: loss 2.6604\n",
      "iter 160: loss 2.6290\n",
      "iter 170: loss 2.6167\n",
      "iter 180: loss 2.6054\n",
      "iter 190: loss 2.5926\n",
      "iter 200: loss 2.5798\n",
      "iter 210: loss 2.5832\n",
      "iter 220: loss 2.5612\n",
      "iter 230: loss 2.5823\n",
      "iter 240: loss 2.5562\n",
      "iter 250: loss 2.5460\n",
      "iter 260: loss 2.5430\n",
      "iter 270: loss 2.5277\n",
      "iter 280: loss 2.5339\n",
      "iter 290: loss 2.5309\n",
      "iter 300: loss 2.5145\n",
      "iter 310: loss 2.5183\n",
      "iter 320: loss 2.5197\n",
      "iter 330: loss 2.5157\n",
      "iter 340: loss 2.4949\n",
      "iter 350: loss 2.4965\n",
      "iter 360: loss 2.4988\n",
      "iter 370: loss 2.4700\n",
      "iter 380: loss 2.5002\n",
      "iter 390: loss 2.4787\n",
      "iter 400: loss 2.4900\n",
      "iter 410: loss 2.4525\n",
      "iter 420: loss 2.4562\n",
      "iter 430: loss 2.4438\n",
      "iter 440: loss 2.4398\n",
      "iter 450: loss 2.4090\n",
      "iter 460: loss 2.4028\n",
      "iter 470: loss 2.3994\n",
      "iter 480: loss 2.3741\n",
      "iter 490: loss 2.3678\n",
      "iter 500: loss 2.3298\n",
      "step 500: train loss 2.2917, val loss 2.2926\n",
      "iter 510: loss 2.3221\n",
      "iter 520: loss 2.2944\n",
      "iter 530: loss 2.2861\n",
      "iter 540: loss 2.2602\n",
      "iter 550: loss 2.2597\n",
      "iter 560: loss 2.2251\n",
      "iter 570: loss 2.2076\n",
      "iter 580: loss 2.1950\n",
      "iter 590: loss 2.2037\n",
      "iter 600: loss 2.1506\n",
      "iter 610: loss 2.1411\n",
      "iter 620: loss 2.1353\n",
      "iter 630: loss 2.1088\n",
      "iter 640: loss 2.0992\n",
      "iter 650: loss 2.0724\n",
      "iter 660: loss 2.0453\n",
      "iter 670: loss 2.0617\n",
      "iter 680: loss 2.0247\n",
      "iter 690: loss 2.0155\n",
      "iter 700: loss 2.0159\n",
      "iter 710: loss 1.9783\n",
      "iter 720: loss 1.9713\n",
      "iter 730: loss 1.9705\n",
      "iter 740: loss 1.9747\n",
      "iter 750: loss 1.9384\n",
      "iter 760: loss 1.9427\n",
      "iter 770: loss 1.9129\n",
      "iter 780: loss 1.9035\n",
      "iter 790: loss 1.8953\n",
      "iter 800: loss 1.8983\n",
      "iter 810: loss 1.8660\n",
      "iter 820: loss 1.8737\n",
      "iter 830: loss 1.8606\n",
      "iter 840: loss 1.8516\n",
      "iter 850: loss 1.8127\n",
      "iter 860: loss 1.8220\n",
      "iter 870: loss 1.8011\n",
      "iter 880: loss 1.7969\n",
      "iter 890: loss 1.7870\n",
      "iter 900: loss 1.7638\n",
      "iter 910: loss 1.7673\n",
      "iter 920: loss 1.7875\n",
      "iter 930: loss 1.7937\n",
      "iter 940: loss 1.7587\n",
      "iter 950: loss 1.7451\n",
      "iter 960: loss 1.7329\n",
      "iter 970: loss 1.7245\n",
      "iter 980: loss 1.7273\n",
      "iter 990: loss 1.7227\n",
      "iter 1000: loss 1.6959\n",
      "step 1000: train loss 1.6480, val loss 1.6457\n",
      "iter 1010: loss 1.7087\n",
      "iter 1020: loss 1.7320\n",
      "iter 1030: loss 1.7043\n",
      "iter 1040: loss 1.6850\n",
      "iter 1050: loss 1.6657\n",
      "iter 1060: loss 1.6613\n",
      "iter 1070: loss 1.6625\n",
      "iter 1080: loss 1.6439\n",
      "iter 1090: loss 1.6636\n",
      "iter 1100: loss 1.6358\n",
      "iter 1110: loss 1.6575\n",
      "iter 1120: loss 1.6147\n",
      "iter 1130: loss 1.6244\n",
      "iter 1140: loss 1.6280\n",
      "iter 1150: loss 1.6385\n",
      "iter 1160: loss 1.6017\n",
      "iter 1170: loss 1.5807\n",
      "iter 1180: loss 1.6160\n",
      "iter 1190: loss 1.6346\n",
      "iter 1200: loss 1.5941\n",
      "iter 1210: loss 1.5817\n",
      "iter 1220: loss 1.6080\n",
      "iter 1230: loss 1.5816\n",
      "iter 1240: loss 1.5780\n",
      "iter 1250: loss 1.5715\n",
      "iter 1260: loss 1.5586\n",
      "iter 1270: loss 1.5734\n",
      "iter 1280: loss 1.5455\n",
      "iter 1290: loss 1.5485\n",
      "iter 1300: loss 1.5482\n",
      "iter 1310: loss 1.5295\n",
      "iter 1320: loss 1.5420\n",
      "iter 1330: loss 1.5461\n",
      "iter 1340: loss 1.5421\n",
      "iter 1350: loss 1.5259\n",
      "iter 1360: loss 1.5282\n",
      "iter 1370: loss 1.5277\n",
      "iter 1380: loss 1.5116\n",
      "iter 1390: loss 1.5158\n",
      "iter 1400: loss 1.5186\n",
      "iter 1410: loss 1.5162\n",
      "iter 1420: loss 1.5006\n",
      "iter 1430: loss 1.5083\n",
      "iter 1440: loss 1.5116\n",
      "iter 1450: loss 1.4892\n",
      "iter 1460: loss 1.5109\n",
      "iter 1470: loss 1.4732\n",
      "iter 1480: loss 1.4884\n",
      "iter 1490: loss 1.4738\n",
      "iter 1500: loss 1.4847\n",
      "step 1500: train loss 1.4183, val loss 1.4229\n",
      "iter 1510: loss 1.5173\n",
      "iter 1520: loss 1.4749\n",
      "iter 1530: loss 1.4400\n",
      "iter 1540: loss 1.4520\n",
      "iter 1550: loss 1.4620\n",
      "iter 1560: loss 1.4515\n",
      "iter 1570: loss 1.4448\n",
      "iter 1580: loss 1.4453\n",
      "iter 1590: loss 1.4607\n",
      "iter 1600: loss 1.4482\n",
      "iter 1610: loss 1.4456\n",
      "iter 1620: loss 1.4509\n",
      "iter 1630: loss 1.4459\n",
      "iter 1640: loss 1.4439\n",
      "iter 1650: loss 1.4261\n",
      "iter 1660: loss 1.4302\n",
      "iter 1670: loss 1.4270\n",
      "iter 1680: loss 1.4334\n",
      "iter 1690: loss 1.4276\n",
      "iter 1700: loss 1.4069\n",
      "iter 1710: loss 1.4223\n",
      "iter 1720: loss 1.4314\n",
      "iter 1730: loss 1.4159\n",
      "iter 1740: loss 1.4179\n",
      "iter 1750: loss 1.4156\n",
      "iter 1760: loss 1.4169\n",
      "iter 1770: loss 1.3988\n",
      "iter 1780: loss 1.4071\n",
      "iter 1790: loss 1.4101\n",
      "iter 1800: loss 1.3932\n",
      "iter 1810: loss 1.4244\n",
      "iter 1820: loss 1.4034\n",
      "iter 1830: loss 1.3986\n",
      "iter 1840: loss 1.3779\n",
      "iter 1850: loss 1.4017\n",
      "iter 1860: loss 1.3851\n",
      "iter 1870: loss 1.4017\n",
      "iter 1880: loss 1.3937\n",
      "iter 1890: loss 1.4049\n",
      "iter 1900: loss 1.3921\n",
      "iter 1910: loss 1.3857\n",
      "iter 1920: loss 1.3742\n",
      "iter 1930: loss 1.3654\n",
      "iter 1940: loss 1.3732\n",
      "iter 1950: loss 1.3711\n",
      "iter 1960: loss 1.3626\n",
      "iter 1970: loss 1.3803\n",
      "iter 1980: loss 1.3514\n",
      "iter 1990: loss 1.3598\n",
      "iter 2000: loss 1.3454\n",
      "step 2000: train loss 1.3051, val loss 1.3102\n",
      "iter 2010: loss 1.3539\n",
      "iter 2020: loss 1.3473\n",
      "iter 2030: loss 1.3538\n",
      "iter 2040: loss 1.3476\n",
      "iter 2050: loss 1.3626\n",
      "iter 2060: loss 1.3533\n",
      "iter 2070: loss 1.3524\n",
      "iter 2080: loss 1.3537\n",
      "iter 2090: loss 1.3518\n",
      "iter 2100: loss 1.3439\n",
      "iter 2110: loss 1.3518\n",
      "iter 2120: loss 1.3276\n",
      "iter 2130: loss 1.3338\n",
      "iter 2140: loss 1.3218\n",
      "iter 2150: loss 1.3401\n",
      "iter 2160: loss 1.3531\n",
      "iter 2170: loss 1.3307\n",
      "iter 2180: loss 1.3314\n",
      "iter 2190: loss 1.3028\n",
      "iter 2200: loss 1.3438\n",
      "iter 2210: loss 1.3199\n",
      "iter 2220: loss 1.3351\n",
      "iter 2230: loss 1.3089\n",
      "iter 2240: loss 1.3118\n",
      "iter 2250: loss 1.3112\n",
      "iter 2260: loss 1.3088\n",
      "iter 2270: loss 1.3043\n",
      "iter 2280: loss 1.3067\n",
      "iter 2290: loss 1.3243\n",
      "iter 2300: loss 1.3118\n",
      "iter 2310: loss 1.3051\n",
      "iter 2320: loss 1.3158\n",
      "iter 2330: loss 1.2849\n",
      "iter 2340: loss 1.3196\n",
      "iter 2350: loss 1.3309\n",
      "iter 2360: loss 1.2987\n",
      "iter 2370: loss 1.2945\n",
      "iter 2380: loss 1.2823\n",
      "iter 2390: loss 1.2963\n",
      "iter 2400: loss 1.2858\n",
      "iter 2410: loss 1.3025\n",
      "iter 2420: loss 1.2951\n",
      "iter 2430: loss 1.3115\n",
      "iter 2440: loss 1.2903\n",
      "iter 2450: loss 1.3016\n",
      "iter 2460: loss 1.2740\n",
      "iter 2470: loss 1.2984\n",
      "iter 2480: loss 1.3063\n",
      "iter 2490: loss 1.2776\n",
      "iter 2500: loss 1.2737\n",
      "step 2500: train loss 1.2325, val loss 1.2352\n",
      "iter 2510: loss 1.2775\n",
      "iter 2520: loss 1.2976\n",
      "iter 2530: loss 1.2901\n",
      "iter 2540: loss 1.2696\n",
      "iter 2550: loss 1.2660\n",
      "iter 2560: loss 1.2740\n",
      "iter 2570: loss 1.2698\n",
      "iter 2580: loss 1.2863\n",
      "iter 2590: loss 1.2719\n",
      "iter 2600: loss 1.2863\n",
      "iter 2610: loss 1.2639\n",
      "iter 2620: loss 1.2799\n",
      "iter 2630: loss 1.2802\n",
      "iter 2640: loss 1.2784\n",
      "iter 2650: loss 1.2810\n",
      "iter 2660: loss 1.2538\n",
      "iter 2670: loss 1.2927\n",
      "iter 2680: loss 1.2544\n",
      "iter 2690: loss 1.2541\n",
      "iter 2700: loss 1.2567\n",
      "iter 2710: loss 1.2752\n",
      "iter 2720: loss 1.2482\n",
      "iter 2730: loss 1.2488\n",
      "iter 2740: loss 1.2692\n",
      "iter 2750: loss 1.2566\n",
      "iter 2760: loss 1.2558\n",
      "iter 2770: loss 1.2601\n",
      "iter 2780: loss 1.2682\n",
      "iter 2790: loss 1.2414\n",
      "iter 2800: loss 1.2360\n",
      "iter 2810: loss 1.2634\n",
      "iter 2820: loss 1.2429\n",
      "iter 2830: loss 1.2519\n",
      "iter 2840: loss 1.2127\n",
      "iter 2850: loss 1.2414\n",
      "iter 2860: loss 1.2472\n",
      "iter 2870: loss 1.2524\n",
      "iter 2880: loss 1.2557\n",
      "iter 2890: loss 1.2425\n",
      "iter 2900: loss 1.2074\n",
      "iter 2910: loss 1.2365\n",
      "iter 2920: loss 1.2576\n",
      "iter 2930: loss 1.2307\n",
      "iter 2940: loss 1.2142\n",
      "iter 2950: loss 1.2490\n",
      "iter 2960: loss 1.2547\n",
      "iter 2970: loss 1.2446\n",
      "iter 2980: loss 1.2299\n",
      "iter 2990: loss 1.2327\n",
      "iter 3000: loss 1.2296\n",
      "step 3000: train loss 1.1724, val loss 1.1729\n",
      "iter 3010: loss 1.2228\n",
      "iter 3020: loss 1.2211\n",
      "iter 3030: loss 1.2175\n",
      "iter 3040: loss 1.2107\n",
      "iter 3050: loss 1.2343\n",
      "iter 3060: loss 1.2216\n",
      "iter 3070: loss 1.2091\n",
      "iter 3080: loss 1.2188\n",
      "iter 3090: loss 1.1891\n",
      "iter 3100: loss 1.2326\n",
      "iter 3110: loss 1.2220\n",
      "iter 3120: loss 1.2173\n",
      "iter 3130: loss 1.1976\n",
      "iter 3140: loss 1.1928\n",
      "iter 3150: loss 1.2175\n",
      "iter 3160: loss 1.2158\n",
      "iter 3170: loss 1.2278\n",
      "iter 3180: loss 1.2073\n",
      "iter 3190: loss 1.2044\n",
      "iter 3200: loss 1.2017\n",
      "iter 3210: loss 1.2070\n",
      "iter 3220: loss 1.2049\n",
      "iter 3230: loss 1.1861\n",
      "iter 3240: loss 1.2117\n",
      "iter 3250: loss 1.2042\n",
      "iter 3260: loss 1.1997\n",
      "iter 3270: loss 1.1809\n",
      "iter 3280: loss 1.2086\n",
      "iter 3290: loss 1.1788\n",
      "iter 3300: loss 1.2183\n",
      "iter 3310: loss 1.1936\n",
      "iter 3320: loss 1.1876\n",
      "iter 3330: loss 1.2095\n",
      "iter 3340: loss 1.1797\n",
      "iter 3350: loss 1.1939\n",
      "iter 3360: loss 1.1772\n",
      "iter 3370: loss 1.2059\n",
      "iter 3380: loss 1.1801\n",
      "iter 3390: loss 1.1929\n",
      "iter 3400: loss 1.1886\n",
      "iter 3410: loss 1.1833\n",
      "iter 3420: loss 1.1784\n",
      "iter 3430: loss 1.1844\n",
      "iter 3440: loss 1.1864\n",
      "iter 3450: loss 1.1803\n",
      "iter 3460: loss 1.1777\n",
      "iter 3470: loss 1.1745\n",
      "iter 3480: loss 1.1684\n",
      "iter 3490: loss 1.1649\n",
      "iter 3500: loss 1.1848\n",
      "step 3500: train loss 1.1206, val loss 1.1214\n",
      "iter 3510: loss 1.1759\n",
      "iter 3520: loss 1.1815\n",
      "iter 3530: loss 1.1776\n",
      "iter 3540: loss 1.1848\n",
      "iter 3550: loss 1.1847\n",
      "iter 3560: loss 1.1752\n",
      "iter 3570: loss 1.1787\n",
      "iter 3580: loss 1.1566\n",
      "iter 3590: loss 1.1790\n",
      "iter 3600: loss 1.1714\n",
      "iter 3610: loss 1.1646\n",
      "iter 3620: loss 1.1678\n",
      "iter 3630: loss 1.1548\n",
      "iter 3640: loss 1.1697\n",
      "iter 3650: loss 1.1596\n",
      "iter 3660: loss 1.1788\n",
      "iter 3670: loss 1.1665\n",
      "iter 3680: loss 1.1615\n",
      "iter 3690: loss 1.1538\n",
      "iter 3700: loss 1.1644\n",
      "iter 3710: loss 1.1601\n",
      "iter 3720: loss 1.1710\n",
      "iter 3730: loss 1.1711\n",
      "iter 3740: loss 1.1300\n",
      "iter 3750: loss 1.1574\n",
      "iter 3760: loss 1.1397\n",
      "iter 3770: loss 1.1640\n",
      "iter 3780: loss 1.1442\n",
      "iter 3790: loss 1.1343\n",
      "iter 3800: loss 1.1551\n",
      "iter 3810: loss 1.1507\n",
      "iter 3820: loss 1.1323\n",
      "iter 3830: loss 1.1224\n",
      "iter 3840: loss 1.1671\n",
      "iter 3850: loss 1.1459\n",
      "iter 3860: loss 1.1542\n",
      "iter 3870: loss 1.1360\n",
      "iter 3880: loss 1.1655\n",
      "iter 3890: loss 1.1332\n",
      "iter 3900: loss 1.1414\n",
      "iter 3910: loss 1.1492\n",
      "iter 3920: loss 1.1238\n",
      "iter 3930: loss 1.1292\n",
      "iter 3940: loss 1.1201\n",
      "iter 3950: loss 1.1347\n",
      "iter 3960: loss 1.1518\n",
      "iter 3970: loss 1.1466\n",
      "iter 3980: loss 1.1285\n",
      "iter 3990: loss 1.1289\n",
      "iter 4000: loss 1.1308\n",
      "step 4000: train loss 1.0704, val loss 1.0754\n",
      "iter 4010: loss 1.1302\n",
      "iter 4020: loss 1.1258\n",
      "iter 4030: loss 1.1315\n",
      "iter 4040: loss 1.1257\n",
      "iter 4050: loss 1.1050\n",
      "iter 4060: loss 1.1423\n",
      "iter 4070: loss 1.1334\n",
      "iter 4080: loss 1.1361\n",
      "iter 4090: loss 1.1194\n",
      "iter 4100: loss 1.1151\n",
      "iter 4110: loss 1.1236\n",
      "iter 4120: loss 1.1370\n",
      "iter 4130: loss 1.1270\n",
      "iter 4140: loss 1.1095\n",
      "iter 4150: loss 1.1140\n",
      "iter 4160: loss 1.1367\n",
      "iter 4170: loss 1.1344\n",
      "iter 4180: loss 1.1296\n",
      "iter 4190: loss 1.1122\n",
      "iter 4200: loss 1.1305\n",
      "iter 4210: loss 1.1079\n",
      "iter 4220: loss 1.1109\n",
      "iter 4230: loss 1.1245\n",
      "iter 4240: loss 1.1044\n",
      "iter 4250: loss 1.0984\n",
      "iter 4260: loss 1.1226\n",
      "iter 4270: loss 1.1224\n",
      "iter 4280: loss 1.1143\n",
      "iter 4290: loss 1.1166\n",
      "iter 4300: loss 1.1105\n",
      "iter 4310: loss 1.1116\n",
      "iter 4320: loss 1.1093\n",
      "iter 4330: loss 1.1259\n",
      "iter 4340: loss 1.0976\n",
      "iter 4350: loss 1.1215\n",
      "iter 4360: loss 1.1043\n",
      "iter 4370: loss 1.1352\n",
      "iter 4380: loss 1.0991\n",
      "iter 4390: loss 1.0971\n",
      "iter 4400: loss 1.1110\n",
      "iter 4410: loss 1.1014\n",
      "iter 4420: loss 1.1056\n",
      "iter 4430: loss 1.1106\n",
      "iter 4440: loss 1.0954\n",
      "iter 4450: loss 1.1059\n",
      "iter 4460: loss 1.1053\n",
      "iter 4470: loss 1.0983\n",
      "iter 4480: loss 1.1091\n",
      "iter 4490: loss 1.1040\n",
      "iter 4500: loss 1.0989\n",
      "step 4500: train loss 1.0231, val loss 1.0293\n",
      "iter 4510: loss 1.0877\n",
      "iter 4520: loss 1.1030\n",
      "iter 4530: loss 1.0867\n",
      "iter 4540: loss 1.0959\n",
      "iter 4550: loss 1.0825\n",
      "iter 4560: loss 1.0970\n",
      "iter 4570: loss 1.0933\n",
      "iter 4580: loss 1.0961\n",
      "iter 4590: loss 1.1082\n",
      "iter 4600: loss 1.0874\n",
      "iter 4610: loss 1.0695\n",
      "iter 4620: loss 1.1007\n",
      "iter 4630: loss 1.0814\n",
      "iter 4640: loss 1.0814\n",
      "iter 4650: loss 1.0854\n",
      "iter 4660: loss 1.0778\n",
      "iter 4670: loss 1.0934\n",
      "iter 4680: loss 1.0981\n",
      "iter 4690: loss 1.0925\n",
      "iter 4700: loss 1.0886\n",
      "iter 4710: loss 1.0756\n",
      "iter 4720: loss 1.0816\n",
      "iter 4730: loss 1.0787\n",
      "iter 4740: loss 1.0958\n",
      "iter 4750: loss 1.0705\n",
      "iter 4760: loss 1.0831\n",
      "iter 4770: loss 1.0757\n",
      "iter 4780: loss 1.0799\n",
      "iter 4790: loss 1.0820\n",
      "iter 4800: loss 1.0937\n",
      "iter 4810: loss 1.0965\n",
      "iter 4820: loss 1.0631\n",
      "iter 4830: loss 1.0615\n",
      "iter 4840: loss 1.0860\n",
      "iter 4850: loss 1.0619\n",
      "iter 4860: loss 1.0764\n",
      "iter 4870: loss 1.0776\n",
      "iter 4880: loss 1.0757\n",
      "iter 4890: loss 1.0734\n",
      "iter 4900: loss 1.0709\n",
      "iter 4910: loss 1.0801\n",
      "iter 4920: loss 1.0633\n",
      "iter 4930: loss 1.0702\n",
      "iter 4940: loss 1.0492\n",
      "iter 4950: loss 1.0577\n",
      "iter 4960: loss 1.0706\n",
      "iter 4970: loss 1.0621\n",
      "iter 4980: loss 1.0770\n",
      "iter 4990: loss 1.0420\n",
      "iter 5000: loss 1.0513\n",
      "step 5000: train loss 0.9806, val loss 0.9835\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.0996\n",
      "step 0: train loss 4.1240, val loss 4.1255\n",
      "iter 10: loss 3.1486\n",
      "iter 20: loss 3.0953\n",
      "iter 30: loss 3.0387\n",
      "iter 40: loss 2.9435\n",
      "iter 50: loss 2.7800\n",
      "iter 60: loss 2.7256\n",
      "iter 70: loss 2.6925\n",
      "iter 80: loss 2.6634\n",
      "iter 90: loss 2.6315\n",
      "iter 100: loss 2.6239\n",
      "iter 110: loss 2.5968\n",
      "iter 120: loss 2.5880\n",
      "iter 130: loss 2.5705\n",
      "iter 140: loss 2.5543\n",
      "iter 150: loss 2.5540\n",
      "iter 160: loss 2.5496\n",
      "iter 170: loss 2.5505\n",
      "iter 180: loss 2.5164\n",
      "iter 190: loss 2.5192\n",
      "iter 200: loss 2.5506\n",
      "iter 210: loss 2.5225\n",
      "iter 220: loss 2.5325\n",
      "iter 230: loss 2.5219\n",
      "iter 240: loss 2.4963\n",
      "iter 250: loss 2.5110\n",
      "iter 260: loss 2.5010\n",
      "iter 270: loss 2.4851\n",
      "iter 280: loss 2.4834\n",
      "iter 290: loss 2.4750\n",
      "iter 300: loss 2.4528\n",
      "iter 310: loss 2.5036\n",
      "iter 320: loss 2.4632\n",
      "iter 330: loss 2.4504\n",
      "iter 340: loss 2.4094\n",
      "iter 350: loss 2.4258\n",
      "iter 360: loss 2.3763\n",
      "iter 370: loss 2.3652\n",
      "iter 380: loss 2.3134\n",
      "iter 390: loss 2.2732\n",
      "iter 400: loss 2.2694\n",
      "iter 410: loss 2.2237\n",
      "iter 420: loss 2.2083\n",
      "iter 430: loss 2.1640\n",
      "iter 440: loss 2.1777\n",
      "iter 450: loss 2.1210\n",
      "iter 460: loss 2.1033\n",
      "iter 470: loss 2.0888\n",
      "iter 480: loss 2.0476\n",
      "iter 490: loss 2.0258\n",
      "iter 500: loss 1.9924\n",
      "step 500: train loss 1.9689, val loss 1.9659\n",
      "iter 510: loss 1.9921\n",
      "iter 520: loss 1.9503\n",
      "iter 530: loss 1.9429\n",
      "iter 540: loss 1.9289\n",
      "iter 550: loss 1.9038\n",
      "iter 560: loss 1.8588\n",
      "iter 570: loss 1.8537\n",
      "iter 580: loss 1.8368\n",
      "iter 590: loss 1.8121\n",
      "iter 600: loss 1.8169\n",
      "iter 610: loss 1.7862\n",
      "iter 620: loss 1.7701\n",
      "iter 630: loss 1.7709\n",
      "iter 640: loss 1.7489\n",
      "iter 650: loss 1.7064\n",
      "iter 660: loss 1.6912\n",
      "iter 670: loss 1.6957\n",
      "iter 680: loss 1.6661\n",
      "iter 690: loss 1.6488\n",
      "iter 700: loss 1.6487\n",
      "iter 710: loss 1.6289\n",
      "iter 720: loss 1.6219\n",
      "iter 730: loss 1.6196\n",
      "iter 740: loss 1.6043\n",
      "iter 750: loss 1.5974\n",
      "iter 760: loss 1.5626\n",
      "iter 770: loss 1.5621\n",
      "iter 780: loss 1.5317\n",
      "iter 790: loss 1.5633\n",
      "iter 800: loss 1.5462\n",
      "iter 810: loss 1.5544\n",
      "iter 820: loss 1.5774\n",
      "iter 830: loss 1.5418\n",
      "iter 840: loss 1.5085\n",
      "iter 850: loss 1.5093\n",
      "iter 860: loss 1.4970\n",
      "iter 870: loss 1.4832\n",
      "iter 880: loss 1.4886\n",
      "iter 890: loss 1.4726\n",
      "iter 900: loss 1.4612\n",
      "iter 910: loss 1.4675\n",
      "iter 920: loss 1.4308\n",
      "iter 930: loss 1.4612\n",
      "iter 940: loss 1.4355\n",
      "iter 950: loss 1.4360\n",
      "iter 960: loss 1.4387\n",
      "iter 970: loss 1.4246\n",
      "iter 980: loss 1.4220\n",
      "iter 990: loss 1.4083\n",
      "iter 1000: loss 1.4177\n",
      "step 1000: train loss 1.3708, val loss 1.3703\n",
      "iter 1010: loss 1.4011\n",
      "iter 1020: loss 1.3763\n",
      "iter 1030: loss 1.3778\n",
      "iter 1040: loss 1.3973\n",
      "iter 1050: loss 1.4037\n",
      "iter 1060: loss 1.3765\n",
      "iter 1070: loss 1.3699\n",
      "iter 1080: loss 1.4015\n",
      "iter 1090: loss 1.3644\n",
      "iter 1100: loss 1.3464\n",
      "iter 1110: loss 1.3548\n",
      "iter 1120: loss 1.3620\n",
      "iter 1130: loss 1.3431\n",
      "iter 1140: loss 1.3537\n",
      "iter 1150: loss 1.3489\n",
      "iter 1160: loss 1.3541\n",
      "iter 1170: loss 1.3421\n",
      "iter 1180: loss 1.3489\n",
      "iter 1190: loss 1.3296\n",
      "iter 1200: loss 1.3456\n",
      "iter 1210: loss 1.3330\n",
      "iter 1220: loss 1.3218\n",
      "iter 1230: loss 1.3302\n",
      "iter 1240: loss 1.3216\n",
      "iter 1250: loss 1.3154\n",
      "iter 1260: loss 1.3415\n",
      "iter 1270: loss 1.2906\n",
      "iter 1280: loss 1.3023\n",
      "iter 1290: loss 1.2966\n",
      "iter 1300: loss 1.3138\n",
      "iter 1310: loss 1.3031\n",
      "iter 1320: loss 1.2954\n",
      "iter 1330: loss 1.3083\n",
      "iter 1340: loss 1.3248\n",
      "iter 1350: loss 1.2813\n",
      "iter 1360: loss 1.3012\n",
      "iter 1370: loss 1.2884\n",
      "iter 1380: loss 1.2834\n",
      "iter 1390: loss 1.2906\n",
      "iter 1400: loss 1.2726\n",
      "iter 1410: loss 1.2604\n",
      "iter 1420: loss 1.2722\n",
      "iter 1430: loss 1.2703\n",
      "iter 1440: loss 1.3061\n",
      "iter 1450: loss 1.2624\n",
      "iter 1460: loss 1.2664\n",
      "iter 1470: loss 1.2760\n",
      "iter 1480: loss 1.2620\n",
      "iter 1490: loss 1.2617\n",
      "iter 1500: loss 1.2529\n",
      "step 1500: train loss 1.2148, val loss 1.2156\n",
      "iter 1510: loss 1.2454\n",
      "iter 1520: loss 1.2294\n",
      "iter 1530: loss 1.2231\n",
      "iter 1540: loss 1.2219\n",
      "iter 1550: loss 1.2564\n",
      "iter 1560: loss 1.2674\n",
      "iter 1570: loss 1.2427\n",
      "iter 1580: loss 1.2369\n",
      "iter 1590: loss 1.2406\n",
      "iter 1600: loss 1.2399\n",
      "iter 1610: loss 1.2363\n",
      "iter 1620: loss 1.2445\n",
      "iter 1630: loss 1.2211\n",
      "iter 1640: loss 1.2246\n",
      "iter 1650: loss 1.2363\n",
      "iter 1660: loss 1.2356\n",
      "iter 1670: loss 1.2166\n",
      "iter 1680: loss 1.2020\n",
      "iter 1690: loss 1.2064\n",
      "iter 1700: loss 1.2250\n",
      "iter 1710: loss 1.2396\n",
      "iter 1720: loss 1.2160\n",
      "iter 1730: loss 1.2176\n",
      "iter 1740: loss 1.2102\n",
      "iter 1750: loss 1.2196\n",
      "iter 1760: loss 1.2096\n",
      "iter 1770: loss 1.2063\n",
      "iter 1780: loss 1.2008\n",
      "iter 1790: loss 1.2247\n",
      "iter 1800: loss 1.2086\n",
      "iter 1810: loss 1.2155\n",
      "iter 1820: loss 1.2046\n",
      "iter 1830: loss 1.2175\n",
      "iter 1840: loss 1.2167\n",
      "iter 1850: loss 1.1857\n",
      "iter 1860: loss 1.2025\n",
      "iter 1870: loss 1.2004\n",
      "iter 1880: loss 1.1797\n",
      "iter 1890: loss 1.1814\n",
      "iter 1900: loss 1.1988\n",
      "iter 1910: loss 1.1851\n",
      "iter 1920: loss 1.1880\n",
      "iter 1930: loss 1.1770\n",
      "iter 1940: loss 1.1741\n",
      "iter 1950: loss 1.1851\n",
      "iter 1960: loss 1.1739\n",
      "iter 1970: loss 1.1929\n",
      "iter 1980: loss 1.1682\n",
      "iter 1990: loss 1.1769\n",
      "iter 2000: loss 1.1664\n",
      "step 2000: train loss 1.1235, val loss 1.1252\n",
      "iter 2010: loss 1.1747\n",
      "iter 2020: loss 1.1788\n",
      "iter 2030: loss 1.1889\n",
      "iter 2040: loss 1.1472\n",
      "iter 2050: loss 1.1606\n",
      "iter 2060: loss 1.1745\n",
      "iter 2070: loss 1.1666\n",
      "iter 2080: loss 1.1587\n",
      "iter 2090: loss 1.1473\n",
      "iter 2100: loss 1.1468\n",
      "iter 2110: loss 1.1613\n",
      "iter 2120: loss 1.1628\n",
      "iter 2130: loss 1.1524\n",
      "iter 2140: loss 1.1506\n",
      "iter 2150: loss 1.1402\n",
      "iter 2160: loss 1.1287\n",
      "iter 2170: loss 1.1327\n",
      "iter 2180: loss 1.1553\n",
      "iter 2190: loss 1.1588\n",
      "iter 2200: loss 1.1505\n",
      "iter 2210: loss 1.1255\n",
      "iter 2220: loss 1.1537\n",
      "iter 2230: loss 1.1499\n",
      "iter 2240: loss 1.1507\n",
      "iter 2250: loss 1.1404\n",
      "iter 2260: loss 1.1260\n",
      "iter 2270: loss 1.1367\n",
      "iter 2280: loss 1.1291\n",
      "iter 2290: loss 1.1415\n",
      "iter 2300: loss 1.1377\n",
      "iter 2310: loss 1.1334\n",
      "iter 2320: loss 1.1358\n",
      "iter 2330: loss 1.1044\n",
      "iter 2340: loss 1.1520\n",
      "iter 2350: loss 1.1368\n",
      "iter 2360: loss 1.1149\n",
      "iter 2370: loss 1.1345\n",
      "iter 2380: loss 1.1090\n",
      "iter 2390: loss 1.1311\n",
      "iter 2400: loss 1.1249\n",
      "iter 2410: loss 1.1240\n",
      "iter 2420: loss 1.1391\n",
      "iter 2430: loss 1.1210\n",
      "iter 2440: loss 1.1005\n",
      "iter 2450: loss 1.1064\n",
      "iter 2460: loss 1.1078\n",
      "iter 2470: loss 1.1090\n",
      "iter 2480: loss 1.1008\n",
      "iter 2490: loss 1.1033\n",
      "iter 2500: loss 1.1062\n",
      "step 2500: train loss 1.0525, val loss 1.0531\n",
      "iter 2510: loss 1.1221\n",
      "iter 2520: loss 1.1184\n",
      "iter 2530: loss 1.0999\n",
      "iter 2540: loss 1.1081\n",
      "iter 2550: loss 1.1124\n",
      "iter 2560: loss 1.0954\n",
      "iter 2570: loss 1.0901\n",
      "iter 2580: loss 1.1149\n",
      "iter 2590: loss 1.0906\n",
      "iter 2600: loss 1.0837\n",
      "iter 2610: loss 1.0814\n",
      "iter 2620: loss 1.0903\n",
      "iter 2630: loss 1.1094\n",
      "iter 2640: loss 1.0929\n",
      "iter 2650: loss 1.0906\n",
      "iter 2660: loss 1.1140\n",
      "iter 2670: loss 1.0770\n",
      "iter 2680: loss 1.0904\n",
      "iter 2690: loss 1.1008\n",
      "iter 2700: loss 1.0903\n",
      "iter 2710: loss 1.0829\n",
      "iter 2720: loss 1.0833\n",
      "iter 2730: loss 1.0831\n",
      "iter 2740: loss 1.0830\n",
      "iter 2750: loss 1.0696\n",
      "iter 2760: loss 1.0984\n",
      "iter 2770: loss 1.0870\n",
      "iter 2780: loss 1.0648\n",
      "iter 2790: loss 1.0442\n",
      "iter 2800: loss 1.0839\n",
      "iter 2810: loss 1.0785\n",
      "iter 2820: loss 1.0555\n",
      "iter 2830: loss 1.0692\n",
      "iter 2840: loss 1.0584\n",
      "iter 2850: loss 1.0612\n",
      "iter 2860: loss 1.0620\n",
      "iter 2870: loss 1.0707\n",
      "iter 2880: loss 1.0851\n",
      "iter 2890: loss 1.0818\n",
      "iter 2900: loss 1.0558\n",
      "iter 2910: loss 1.0442\n",
      "iter 2920: loss 1.0526\n",
      "iter 2930: loss 1.0764\n",
      "iter 2940: loss 1.0707\n",
      "iter 2950: loss 1.0526\n",
      "iter 2960: loss 1.0732\n",
      "iter 2970: loss 1.0448\n",
      "iter 2980: loss 1.0559\n",
      "iter 2990: loss 1.0426\n",
      "iter 3000: loss 1.0535\n",
      "step 3000: train loss 0.9862, val loss 0.9861\n",
      "iter 3010: loss 1.0513\n",
      "iter 3020: loss 1.0562\n",
      "iter 3030: loss 1.0558\n",
      "iter 3040: loss 1.0590\n",
      "iter 3050: loss 1.0596\n",
      "iter 3060: loss 1.0606\n",
      "iter 3070: loss 1.0621\n",
      "iter 3080: loss 1.0523\n",
      "iter 3090: loss 1.0428\n",
      "iter 3100: loss 1.0455\n",
      "iter 3110: loss 1.0562\n",
      "iter 3120: loss 1.0464\n",
      "iter 3130: loss 1.0654\n",
      "iter 3140: loss 1.0435\n",
      "iter 3150: loss 1.0391\n",
      "iter 3160: loss 1.0499\n",
      "iter 3170: loss 1.0296\n",
      "iter 3180: loss 1.0325\n",
      "iter 3190: loss 1.0248\n",
      "iter 3200: loss 1.0332\n",
      "iter 3210: loss 1.0385\n",
      "iter 3220: loss 1.0305\n",
      "iter 3230: loss 1.0513\n",
      "iter 3240: loss 1.0390\n",
      "iter 3250: loss 1.0261\n",
      "iter 3260: loss 1.0391\n",
      "iter 3270: loss 1.0376\n",
      "iter 3280: loss 1.0453\n",
      "iter 3290: loss 1.0142\n",
      "iter 3300: loss 1.0109\n",
      "iter 3310: loss 1.0302\n",
      "iter 3320: loss 1.0319\n",
      "iter 3330: loss 1.0140\n",
      "iter 3340: loss 1.0074\n",
      "iter 3350: loss 1.0308\n",
      "iter 3360: loss 1.0304\n",
      "iter 3370: loss 1.0302\n",
      "iter 3380: loss 1.0109\n",
      "iter 3390: loss 1.0244\n",
      "iter 3400: loss 1.0099\n",
      "iter 3410: loss 1.0154\n",
      "iter 3420: loss 1.0075\n",
      "iter 3430: loss 1.0023\n",
      "iter 3440: loss 1.0272\n",
      "iter 3450: loss 1.0243\n",
      "iter 3460: loss 1.0009\n",
      "iter 3470: loss 1.0156\n",
      "iter 3480: loss 1.0231\n",
      "iter 3490: loss 1.0061\n",
      "iter 3500: loss 1.0173\n",
      "step 3500: train loss 0.9275, val loss 0.9299\n",
      "iter 3510: loss 1.0081\n",
      "iter 3520: loss 1.0067\n",
      "iter 3530: loss 0.9958\n",
      "iter 3540: loss 1.0186\n",
      "iter 3550: loss 0.9892\n",
      "iter 3560: loss 0.9969\n",
      "iter 3570: loss 1.0002\n",
      "iter 3580: loss 0.9922\n",
      "iter 3590: loss 0.9946\n",
      "iter 3600: loss 0.9931\n",
      "iter 3610: loss 1.0020\n",
      "iter 3620: loss 1.0052\n",
      "iter 3630: loss 0.9915\n",
      "iter 3640: loss 0.9992\n",
      "iter 3650: loss 0.9872\n",
      "iter 3660: loss 1.0136\n",
      "iter 3670: loss 0.9734\n",
      "iter 3680: loss 1.0064\n",
      "iter 3690: loss 1.0044\n",
      "iter 3700: loss 0.9992\n",
      "iter 3710: loss 0.9859\n",
      "iter 3720: loss 1.0064\n",
      "iter 3730: loss 0.9748\n",
      "iter 3740: loss 0.9835\n",
      "iter 3750: loss 0.9701\n",
      "iter 3760: loss 1.0050\n",
      "iter 3770: loss 0.9819\n",
      "iter 3780: loss 0.9658\n",
      "iter 3790: loss 0.9724\n",
      "iter 3800: loss 0.9881\n",
      "iter 3810: loss 0.9900\n",
      "iter 3820: loss 0.9864\n",
      "iter 3830: loss 0.9710\n",
      "iter 3840: loss 0.9742\n",
      "iter 3850: loss 0.9603\n",
      "iter 3860: loss 0.9825\n",
      "iter 3870: loss 0.9708\n",
      "iter 3880: loss 0.9822\n",
      "iter 3890: loss 0.9593\n",
      "iter 3900: loss 0.9715\n",
      "iter 3910: loss 0.9839\n",
      "iter 3920: loss 0.9887\n",
      "iter 3930: loss 0.9667\n",
      "iter 3940: loss 0.9765\n",
      "iter 3950: loss 0.9811\n",
      "iter 3960: loss 0.9783\n",
      "iter 3970: loss 0.9737\n",
      "iter 3980: loss 0.9712\n",
      "iter 3990: loss 0.9669\n",
      "iter 4000: loss 0.9932\n",
      "step 4000: train loss 0.8712, val loss 0.8729\n",
      "iter 4010: loss 0.9588\n",
      "iter 4020: loss 0.9530\n",
      "iter 4030: loss 0.9752\n",
      "iter 4040: loss 0.9688\n",
      "iter 4050: loss 0.9690\n",
      "iter 4060: loss 0.9563\n",
      "iter 4070: loss 0.9499\n",
      "iter 4080: loss 0.9507\n",
      "iter 4090: loss 0.9540\n",
      "iter 4100: loss 0.9539\n",
      "iter 4110: loss 0.9571\n",
      "iter 4120: loss 0.9620\n",
      "iter 4130: loss 0.9643\n",
      "iter 4140: loss 0.9571\n",
      "iter 4150: loss 0.9442\n",
      "iter 4160: loss 0.9466\n",
      "iter 4170: loss 0.9575\n",
      "iter 4180: loss 0.9518\n",
      "iter 4190: loss 0.9582\n",
      "iter 4200: loss 0.9413\n",
      "iter 4210: loss 0.9579\n",
      "iter 4220: loss 0.9402\n",
      "iter 4230: loss 0.9388\n",
      "iter 4240: loss 0.9413\n",
      "iter 4250: loss 0.9279\n",
      "iter 4260: loss 0.9313\n",
      "iter 4270: loss 0.9405\n",
      "iter 4280: loss 0.9466\n",
      "iter 4290: loss 0.9318\n",
      "iter 4300: loss 0.9386\n",
      "iter 4310: loss 0.9344\n",
      "iter 4320: loss 0.9426\n",
      "iter 4330: loss 0.9476\n",
      "iter 4340: loss 0.9405\n",
      "iter 4350: loss 0.9378\n",
      "iter 4360: loss 0.9331\n",
      "iter 4370: loss 0.9169\n",
      "iter 4380: loss 0.9466\n",
      "iter 4390: loss 0.9210\n",
      "iter 4400: loss 0.9500\n",
      "iter 4410: loss 0.9213\n",
      "iter 4420: loss 0.9349\n",
      "iter 4430: loss 0.9077\n",
      "iter 4440: loss 0.9407\n",
      "iter 4450: loss 0.9340\n",
      "iter 4460: loss 0.9230\n",
      "iter 4470: loss 0.9277\n",
      "iter 4480: loss 0.9220\n",
      "iter 4490: loss 0.9221\n",
      "iter 4500: loss 0.9160\n",
      "step 4500: train loss 0.8146, val loss 0.8218\n",
      "iter 4510: loss 0.9270\n",
      "iter 4520: loss 0.9319\n",
      "iter 4530: loss 0.9172\n",
      "iter 4540: loss 0.9197\n",
      "iter 4550: loss 0.9344\n",
      "iter 4560: loss 0.9248\n",
      "iter 4570: loss 0.9293\n",
      "iter 4580: loss 0.9374\n",
      "iter 4590: loss 0.9314\n",
      "iter 4600: loss 0.9171\n",
      "iter 4610: loss 0.9338\n",
      "iter 4620: loss 0.9129\n",
      "iter 4630: loss 0.9053\n",
      "iter 4640: loss 0.9088\n",
      "iter 4650: loss 0.9164\n",
      "iter 4660: loss 0.9118\n",
      "iter 4670: loss 0.9046\n",
      "iter 4680: loss 0.9149\n",
      "iter 4690: loss 0.9104\n",
      "iter 4700: loss 0.9035\n",
      "iter 4710: loss 0.9229\n",
      "iter 4720: loss 0.8899\n",
      "iter 4730: loss 0.9102\n",
      "iter 4740: loss 0.8958\n",
      "iter 4750: loss 0.8872\n",
      "iter 4760: loss 0.9103\n",
      "iter 4770: loss 0.9048\n",
      "iter 4780: loss 0.9070\n",
      "iter 4790: loss 0.9048\n",
      "iter 4800: loss 0.9115\n",
      "iter 4810: loss 0.9013\n",
      "iter 4820: loss 0.9079\n",
      "iter 4830: loss 0.9151\n",
      "iter 4840: loss 0.9111\n",
      "iter 4850: loss 0.8912\n",
      "iter 4860: loss 0.8988\n",
      "iter 4870: loss 0.8984\n",
      "iter 4880: loss 0.9022\n",
      "iter 4890: loss 0.9006\n",
      "iter 4900: loss 0.8867\n",
      "iter 4910: loss 0.8943\n",
      "iter 4920: loss 0.9133\n",
      "iter 4930: loss 0.8960\n",
      "iter 4940: loss 0.9030\n",
      "iter 4950: loss 0.8933\n",
      "iter 4960: loss 0.8932\n",
      "iter 4970: loss 0.8911\n",
      "iter 4980: loss 0.8808\n",
      "iter 4990: loss 0.8904\n",
      "iter 5000: loss 0.8919\n",
      "step 5000: train loss 0.7686, val loss 0.7713\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "training done\n",
      "Number of parameters in GPT: 18.87M\n",
      "iter 0: loss 5.2590\n",
      "step 0: train loss 4.1674, val loss 4.1667\n",
      "iter 10: loss 3.1405\n",
      "iter 20: loss 3.1252\n",
      "iter 30: loss 3.1325\n",
      "iter 40: loss 3.1021\n",
      "iter 50: loss 3.1227\n",
      "iter 60: loss 9.2055\n",
      "iter 70: loss 3.1269\n",
      "iter 80: loss 3.1099\n",
      "iter 90: loss 3.1127\n",
      "iter 100: loss 3.1207\n",
      "iter 110: loss 3.1209\n",
      "iter 120: loss 3.1201\n",
      "iter 130: loss 3.1141\n",
      "iter 140: loss 3.1174\n",
      "iter 150: loss 3.1121\n",
      "iter 160: loss 3.1260\n",
      "iter 170: loss 3.1103\n",
      "iter 180: loss 3.1277\n",
      "iter 190: loss 3.1208\n",
      "iter 200: loss 3.1203\n",
      "iter 210: loss 3.1082\n",
      "iter 220: loss 3.1103\n",
      "iter 230: loss 3.1192\n",
      "iter 240: loss 3.1185\n",
      "iter 250: loss 3.1164\n",
      "iter 260: loss 3.1139\n",
      "iter 270: loss 3.1113\n",
      "iter 280: loss 3.1004\n",
      "iter 290: loss 3.0391\n",
      "iter 300: loss 2.9936\n",
      "iter 310: loss 2.9357\n",
      "iter 320: loss 2.9161\n",
      "iter 330: loss 2.8653\n",
      "iter 340: loss 2.7970\n",
      "iter 350: loss 2.7469\n",
      "iter 360: loss 2.7176\n",
      "iter 370: loss 2.6915\n",
      "iter 380: loss 2.6897\n",
      "iter 390: loss 2.6711\n",
      "iter 400: loss 2.6431\n",
      "iter 410: loss 2.6092\n",
      "iter 420: loss 2.6402\n",
      "iter 430: loss 2.5915\n",
      "iter 440: loss 2.5895\n",
      "iter 450: loss 2.5730\n",
      "iter 460: loss 2.5809\n",
      "iter 470: loss 2.5664\n",
      "iter 480: loss 2.5295\n",
      "iter 490: loss 2.5625\n",
      "iter 500: loss 2.5349\n",
      "step 500: train loss 2.5362, val loss 2.5358\n",
      "iter 510: loss 2.5423\n",
      "iter 520: loss 2.5263\n",
      "iter 530: loss 2.5263\n",
      "iter 540: loss 2.5277\n",
      "iter 550: loss 2.5205\n",
      "iter 560: loss 2.5098\n",
      "iter 570: loss 2.5068\n",
      "iter 580: loss 2.4940\n",
      "iter 590: loss 2.4923\n",
      "iter 600: loss 2.4685\n",
      "iter 610: loss 2.4889\n",
      "iter 620: loss 2.4749\n",
      "iter 630: loss 2.4883\n",
      "iter 640: loss 2.4538\n",
      "iter 650: loss 2.4361\n",
      "iter 660: loss 2.4689\n",
      "iter 670: loss 2.4035\n",
      "iter 680: loss 2.3695\n",
      "iter 690: loss 2.3389\n",
      "iter 700: loss 2.3333\n",
      "iter 710: loss 2.3048\n",
      "iter 720: loss 2.2904\n",
      "iter 730: loss 2.2229\n",
      "iter 740: loss 2.2234\n",
      "iter 750: loss 2.2206\n",
      "iter 760: loss 2.1995\n",
      "iter 770: loss 2.1692\n",
      "iter 780: loss 2.1315\n",
      "iter 790: loss 2.2975\n",
      "iter 800: loss 2.1647\n",
      "iter 810: loss 2.1026\n",
      "iter 820: loss 2.0673\n",
      "iter 830: loss 2.0445\n",
      "iter 840: loss 2.0430\n",
      "iter 850: loss 2.0075\n",
      "iter 860: loss 2.0150\n",
      "iter 870: loss 1.9684\n",
      "iter 880: loss 1.9661\n",
      "iter 890: loss 1.9401\n",
      "iter 900: loss 1.9160\n",
      "iter 910: loss 1.8776\n",
      "iter 920: loss 1.8861\n",
      "iter 930: loss 1.8555\n",
      "iter 940: loss 1.8508\n",
      "iter 950: loss 1.8309\n",
      "iter 960: loss 1.8179\n",
      "iter 970: loss 1.7952\n",
      "iter 980: loss 1.7672\n",
      "iter 990: loss 1.7660\n",
      "iter 1000: loss 1.7524\n",
      "step 1000: train loss 1.7067, val loss 1.7055\n",
      "iter 1010: loss 1.7425\n",
      "iter 1020: loss 1.7392\n",
      "iter 1030: loss 1.7083\n",
      "iter 1040: loss 1.7265\n",
      "iter 1050: loss 1.7121\n",
      "iter 1060: loss 1.7042\n",
      "iter 1070: loss 1.6692\n",
      "iter 1080: loss 1.6580\n",
      "iter 1090: loss 1.6539\n",
      "iter 1100: loss 1.6213\n",
      "iter 1110: loss 1.6040\n",
      "iter 1120: loss 1.6184\n",
      "iter 1130: loss 1.6039\n",
      "iter 1140: loss 1.5701\n",
      "iter 1150: loss 1.5894\n",
      "iter 1160: loss 1.5791\n",
      "iter 1170: loss 1.5793\n",
      "iter 1180: loss 1.5672\n",
      "iter 1190: loss 1.5576\n",
      "iter 1200: loss 1.5233\n",
      "iter 1210: loss 1.5498\n",
      "iter 1220: loss 1.5252\n",
      "iter 1230: loss 1.5197\n",
      "iter 1240: loss 1.5313\n",
      "iter 1250: loss 1.5079\n",
      "iter 1260: loss 1.4910\n",
      "iter 1270: loss 1.4833\n",
      "iter 1280: loss 1.5048\n",
      "iter 1290: loss 1.4455\n",
      "iter 1300: loss 1.4959\n",
      "iter 1310: loss 1.4552\n",
      "iter 1320: loss 1.4621\n",
      "iter 1330: loss 1.5183\n",
      "iter 1340: loss 1.4764\n",
      "iter 1350: loss 1.4638\n",
      "iter 1360: loss 1.4471\n",
      "iter 1370: loss 1.4488\n",
      "iter 1380: loss 1.4335\n",
      "iter 1390: loss 1.4545\n",
      "iter 1400: loss 1.4132\n",
      "iter 1410: loss 1.4115\n",
      "iter 1420: loss 1.3908\n",
      "iter 1430: loss 1.4321\n",
      "iter 1440: loss 1.4100\n",
      "iter 1450: loss 1.3882\n",
      "iter 1460: loss 1.4168\n",
      "iter 1470: loss 1.4046\n",
      "iter 1480: loss 1.3792\n",
      "iter 1490: loss 1.4016\n",
      "iter 1500: loss 1.3874\n",
      "step 1500: train loss 1.3424, val loss 1.3462\n",
      "iter 1510: loss 1.3772\n",
      "iter 1520: loss 1.3717\n",
      "iter 1530: loss 1.3925\n",
      "iter 1540: loss 1.3659\n",
      "iter 1550: loss 1.3639\n",
      "iter 1560: loss 1.3762\n",
      "iter 1570: loss 1.3433\n",
      "iter 1580: loss 1.3643\n",
      "iter 1590: loss 1.3630\n",
      "iter 1600: loss 1.3658\n",
      "iter 1610: loss 1.3473\n",
      "iter 1620: loss 1.3539\n",
      "iter 1630: loss 1.3421\n",
      "iter 1640: loss 1.3280\n",
      "iter 1650: loss 1.3493\n",
      "iter 1660: loss 1.3275\n",
      "iter 1670: loss 1.3119\n",
      "iter 1680: loss 1.3151\n",
      "iter 1690: loss 1.3315\n",
      "iter 1700: loss 1.3293\n",
      "iter 1710: loss 1.3362\n",
      "iter 1720: loss 1.3221\n",
      "iter 1730: loss 1.3061\n",
      "iter 1740: loss 1.2933\n",
      "iter 1750: loss 1.3057\n",
      "iter 1760: loss 1.2988\n",
      "iter 1770: loss 1.3030\n",
      "iter 1780: loss 1.3089\n",
      "iter 1790: loss 1.2925\n",
      "iter 1800: loss 1.3182\n",
      "iter 1810: loss 1.2886\n",
      "iter 1820: loss 1.2914\n",
      "iter 1830: loss 1.2705\n",
      "iter 1840: loss 1.3094\n",
      "iter 1850: loss 1.2918\n",
      "iter 1860: loss 1.2910\n",
      "iter 1870: loss 1.3024\n",
      "iter 1880: loss 1.2864\n",
      "iter 1890: loss 1.2537\n",
      "iter 1900: loss 1.2593\n",
      "iter 1910: loss 1.2616\n",
      "iter 1920: loss 1.2740\n",
      "iter 1930: loss 1.2842\n",
      "iter 1940: loss 1.2536\n",
      "iter 1950: loss 1.2610\n",
      "iter 1960: loss 1.2729\n",
      "iter 1970: loss 1.2512\n",
      "iter 1980: loss 1.2671\n",
      "iter 1990: loss 1.2723\n",
      "iter 2000: loss 1.2713\n",
      "step 2000: train loss 1.2108, val loss 1.2113\n",
      "iter 2010: loss 1.2691\n",
      "iter 2020: loss 1.2639\n",
      "iter 2030: loss 1.2395\n",
      "iter 2040: loss 1.2345\n",
      "iter 2050: loss 1.2412\n",
      "iter 2060: loss 1.2378\n",
      "iter 2070: loss 1.2680\n",
      "iter 2080: loss 1.2378\n",
      "iter 2090: loss 1.2459\n",
      "iter 2100: loss 1.2478\n",
      "iter 2110: loss 1.2479\n",
      "iter 2120: loss 1.2490\n",
      "iter 2130: loss 1.2222\n",
      "iter 2140: loss 1.2384\n",
      "iter 2150: loss 1.2143\n",
      "iter 2160: loss 1.2037\n",
      "iter 2170: loss 1.2252\n",
      "iter 2180: loss 1.2265\n",
      "iter 2190: loss 1.2074\n",
      "iter 2200: loss 1.1981\n",
      "iter 2210: loss 1.2104\n",
      "iter 2220: loss 1.2139\n",
      "iter 2230: loss 1.2116\n",
      "iter 2240: loss 1.2266\n",
      "iter 2250: loss 1.1943\n",
      "iter 2260: loss 1.1979\n",
      "iter 2270: loss 1.2082\n",
      "iter 2280: loss 1.1990\n",
      "iter 2290: loss 1.1971\n",
      "iter 2300: loss 1.1941\n",
      "iter 2310: loss 1.1901\n",
      "iter 2320: loss 1.1960\n",
      "iter 2330: loss 1.2099\n",
      "iter 2340: loss 1.1932\n",
      "iter 2350: loss 1.2110\n",
      "iter 2360: loss 1.1844\n",
      "iter 2370: loss 1.1740\n",
      "iter 2380: loss 1.1943\n",
      "iter 2390: loss 1.1957\n",
      "iter 2400: loss 1.1643\n",
      "iter 2410: loss 1.1789\n",
      "iter 2420: loss 1.1780\n",
      "iter 2430: loss 1.1920\n",
      "iter 2440: loss 1.1912\n",
      "iter 2450: loss 1.1764\n",
      "iter 2460: loss 1.1848\n",
      "iter 2470: loss 1.1969\n",
      "iter 2480: loss 1.1764\n",
      "iter 2490: loss 1.1665\n",
      "iter 2500: loss 1.1621\n",
      "step 2500: train loss 1.1170, val loss 1.1214\n",
      "iter 2510: loss 1.1719\n",
      "iter 2520: loss 1.1708\n",
      "iter 2530: loss 1.1866\n",
      "iter 2540: loss 1.1775\n",
      "iter 2550: loss 1.1544\n",
      "iter 2560: loss 1.1519\n",
      "iter 2570: loss 1.1729\n",
      "iter 2580: loss 1.1634\n",
      "iter 2590: loss 1.1773\n",
      "iter 2600: loss 1.1488\n",
      "iter 2610: loss 1.1548\n",
      "iter 2620: loss 1.1621\n",
      "iter 2630: loss 1.1584\n",
      "iter 2640: loss 1.1489\n",
      "iter 2650: loss 1.1506\n",
      "iter 2660: loss 1.1455\n",
      "iter 2670: loss 1.1608\n",
      "iter 2680: loss 1.1478\n",
      "iter 2690: loss 1.1355\n",
      "iter 2700: loss 1.1241\n",
      "iter 2710: loss 1.1598\n",
      "iter 2720: loss 1.1425\n",
      "iter 2730: loss 1.1392\n",
      "iter 2740: loss 1.1465\n",
      "iter 2750: loss 1.1256\n",
      "iter 2760: loss 1.1488\n",
      "iter 2770: loss 1.1390\n",
      "iter 2780: loss 1.1286\n",
      "iter 2790: loss 1.1373\n",
      "iter 2800: loss 1.1278\n",
      "iter 2810: loss 1.1419\n",
      "iter 2820: loss 1.1340\n",
      "iter 2830: loss 1.1208\n",
      "iter 2840: loss 1.1368\n",
      "iter 2850: loss 1.1285\n",
      "iter 2860: loss 1.1252\n",
      "iter 2870: loss 1.1152\n",
      "iter 2880: loss 1.1145\n",
      "iter 2890: loss 1.1319\n",
      "iter 2900: loss 1.1207\n",
      "iter 2910: loss 1.1214\n",
      "iter 2920: loss 1.1121\n",
      "iter 2930: loss 1.1051\n",
      "iter 2940: loss 1.1281\n",
      "iter 2950: loss 1.1248\n",
      "iter 2960: loss 1.1181\n",
      "iter 2970: loss 1.1239\n",
      "iter 2980: loss 1.1032\n",
      "iter 2990: loss 1.1223\n",
      "iter 3000: loss 1.1004\n",
      "step 3000: train loss 1.0449, val loss 1.0484\n",
      "iter 3010: loss 1.1075\n",
      "iter 3020: loss 1.1043\n",
      "iter 3030: loss 1.1050\n",
      "iter 3040: loss 1.0897\n",
      "iter 3050: loss 1.1192\n",
      "iter 3060: loss 1.0937\n",
      "iter 3070: loss 1.0962\n",
      "iter 3080: loss 1.1077\n",
      "iter 3090: loss 1.1105\n",
      "iter 3100: loss 1.0933\n",
      "iter 3110: loss 1.0987\n",
      "iter 3120: loss 1.1180\n",
      "iter 3130: loss 1.0856\n",
      "iter 3140: loss 1.0791\n",
      "iter 3150: loss 1.1083\n",
      "iter 3160: loss 1.0844\n",
      "iter 3170: loss 1.0878\n",
      "iter 3180: loss 1.0988\n",
      "iter 3190: loss 1.0953\n",
      "iter 3200: loss 1.0738\n",
      "iter 3210: loss 1.0885\n",
      "iter 3220: loss 1.0832\n",
      "iter 3230: loss 1.0631\n",
      "iter 3240: loss 1.0802\n",
      "iter 3250: loss 1.0963\n",
      "iter 3260: loss 1.0661\n",
      "iter 3270: loss 1.0571\n",
      "iter 3280: loss 1.0837\n",
      "iter 3290: loss 1.0706\n",
      "iter 3300: loss 1.0881\n",
      "iter 3310: loss 1.0633\n",
      "iter 3320: loss 1.0831\n",
      "iter 3330: loss 1.0621\n",
      "iter 3340: loss 1.0661\n",
      "iter 3350: loss 1.0638\n",
      "iter 3360: loss 1.0654\n",
      "iter 3370: loss 1.0728\n",
      "iter 3380: loss 1.0612\n",
      "iter 3390: loss 1.0501\n",
      "iter 3400: loss 1.0581\n",
      "iter 3410: loss 1.0627\n",
      "iter 3420: loss 1.0572\n",
      "iter 3430: loss 1.0673\n",
      "iter 3440: loss 1.0628\n",
      "iter 3450: loss 1.0672\n",
      "iter 3460: loss 1.0743\n",
      "iter 3470: loss 1.0680\n",
      "iter 3480: loss 1.0618\n",
      "iter 3490: loss 1.0455\n",
      "iter 3500: loss 1.0478\n",
      "step 3500: train loss 0.9880, val loss 0.9863\n",
      "iter 3510: loss 1.0699\n",
      "iter 3520: loss 1.0457\n",
      "iter 3530: loss 1.0627\n",
      "iter 3540: loss 1.0580\n",
      "iter 3550: loss 1.0671\n",
      "iter 3560: loss 1.0384\n",
      "iter 3570: loss 1.0385\n",
      "iter 3580: loss 1.0428\n",
      "iter 3590: loss 1.0400\n",
      "iter 3600: loss 1.0415\n",
      "iter 3610: loss 1.0411\n",
      "iter 3620: loss 1.0453\n",
      "iter 3630: loss 1.0486\n",
      "iter 3640: loss 1.0468\n",
      "iter 3650: loss 1.0495\n",
      "iter 3660: loss 1.0439\n",
      "iter 3670: loss 1.0572\n",
      "iter 3680: loss 1.0582\n",
      "iter 3690: loss 1.0335\n",
      "iter 3700: loss 1.0307\n",
      "iter 3710: loss 1.0396\n",
      "iter 3720: loss 1.0520\n",
      "iter 3730: loss 1.0431\n",
      "iter 3740: loss 1.0321\n",
      "iter 3750: loss 1.0247\n",
      "iter 3760: loss 1.0401\n",
      "iter 3770: loss 1.0101\n",
      "iter 3780: loss 1.0248\n",
      "iter 3790: loss 1.0253\n",
      "iter 3800: loss 1.0228\n",
      "iter 3810: loss 1.0279\n",
      "iter 3820: loss 1.0147\n",
      "iter 3830: loss 1.0154\n",
      "iter 3840: loss 1.0181\n",
      "iter 3850: loss 1.0303\n",
      "iter 3860: loss 1.0365\n",
      "iter 3870: loss 1.0192\n",
      "iter 3880: loss 1.0295\n",
      "iter 3890: loss 1.0160\n",
      "iter 3900: loss 1.0299\n",
      "iter 3910: loss 1.0135\n",
      "iter 3920: loss 1.0222\n",
      "iter 3930: loss 1.0286\n",
      "iter 3940: loss 1.0265\n",
      "iter 3950: loss 1.0184\n",
      "iter 3960: loss 0.9985\n",
      "iter 3970: loss 1.0047\n",
      "iter 3980: loss 1.0337\n",
      "iter 3990: loss 0.9991\n",
      "iter 4000: loss 1.0147\n",
      "step 4000: train loss 0.9272, val loss 0.9266\n",
      "iter 4010: loss 1.0066\n",
      "iter 4020: loss 1.0123\n",
      "iter 4030: loss 1.0074\n",
      "iter 4040: loss 1.0288\n",
      "iter 4050: loss 1.0072\n",
      "iter 4060: loss 1.0123\n",
      "iter 4070: loss 1.0016\n",
      "iter 4080: loss 0.9990\n",
      "iter 4090: loss 1.0133\n",
      "iter 4100: loss 0.9920\n",
      "iter 4110: loss 0.9986\n",
      "iter 4120: loss 1.0064\n",
      "iter 4130: loss 1.0079\n",
      "iter 4140: loss 0.9925\n",
      "iter 4150: loss 1.0054\n",
      "iter 4160: loss 0.9925\n",
      "iter 4170: loss 1.0095\n",
      "iter 4180: loss 1.0126\n",
      "iter 4190: loss 0.9864\n",
      "iter 4200: loss 1.0119\n",
      "iter 4210: loss 0.9866\n",
      "iter 4220: loss 1.0171\n",
      "iter 4230: loss 0.9783\n",
      "iter 4240: loss 0.9946\n",
      "iter 4250: loss 0.9933\n",
      "iter 4260: loss 1.0095\n",
      "iter 4270: loss 0.9827\n",
      "iter 4280: loss 1.0019\n",
      "iter 4290: loss 0.9758\n",
      "iter 4300: loss 0.9863\n",
      "iter 4310: loss 0.9914\n",
      "iter 4320: loss 0.9896\n",
      "iter 4330: loss 0.9808\n",
      "iter 4340: loss 0.9738\n",
      "iter 4350: loss 0.9910\n",
      "iter 4360: loss 0.9825\n",
      "iter 4370: loss 0.9805\n",
      "iter 4380: loss 0.9677\n",
      "iter 4390: loss 0.9775\n",
      "iter 4400: loss 0.9681\n",
      "iter 4410: loss 0.9706\n",
      "iter 4420: loss 0.9584\n",
      "iter 4430: loss 0.9726\n",
      "iter 4440: loss 0.9812\n",
      "iter 4450: loss 0.9861\n",
      "iter 4460: loss 0.9809\n",
      "iter 4470: loss 0.9912\n",
      "iter 4480: loss 0.9767\n",
      "iter 4490: loss 0.9481\n",
      "iter 4500: loss 0.9697\n",
      "step 4500: train loss 0.8713, val loss 0.8717\n",
      "iter 4510: loss 0.9473\n",
      "iter 4520: loss 0.9506\n",
      "iter 4530: loss 0.9566\n",
      "iter 4540: loss 0.9604\n",
      "iter 4550: loss 0.9715\n",
      "iter 4560: loss 0.9672\n",
      "iter 4570: loss 0.9533\n",
      "iter 4580: loss 0.9613\n",
      "iter 4590: loss 0.9595\n",
      "iter 4600: loss 0.9585\n",
      "iter 4610: loss 0.9570\n",
      "iter 4620: loss 0.9545\n",
      "iter 4630: loss 0.9414\n",
      "iter 4640: loss 0.9520\n",
      "iter 4650: loss 0.9575\n",
      "iter 4660: loss 0.9549\n",
      "iter 4670: loss 0.9525\n",
      "iter 4680: loss 0.9521\n",
      "iter 4690: loss 0.9628\n",
      "iter 4700: loss 0.9450\n",
      "iter 4710: loss 0.9550\n",
      "iter 4720: loss 0.9483\n",
      "iter 4730: loss 0.9536\n",
      "iter 4740: loss 0.9592\n",
      "iter 4750: loss 0.9442\n",
      "iter 4760: loss 0.9400\n",
      "iter 4770: loss 0.9398\n",
      "iter 4780: loss 0.9461\n",
      "iter 4790: loss 0.9278\n",
      "iter 4800: loss 0.9407\n",
      "iter 4810: loss 0.9374\n",
      "iter 4820: loss 0.9328\n",
      "iter 4830: loss 0.9587\n",
      "iter 4840: loss 0.9512\n",
      "iter 4850: loss 0.9288\n",
      "iter 4860: loss 0.9269\n",
      "iter 4870: loss 0.9334\n",
      "iter 4880: loss 0.9352\n",
      "iter 4890: loss 0.9395\n",
      "iter 4900: loss 0.9213\n",
      "iter 4910: loss 0.9161\n",
      "iter 4920: loss 0.9443\n",
      "iter 4930: loss 0.9348\n",
      "iter 4940: loss 0.9326\n",
      "iter 4950: loss 0.9271\n",
      "iter 4960: loss 0.9432\n",
      "iter 4970: loss 0.9197\n",
      "iter 4980: loss 0.9303\n",
      "iter 4990: loss 0.9392\n",
      "iter 5000: loss 0.9283\n",
      "step 5000: train loss 0.8142, val loss 0.8155\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.1073\n",
      "step 0: train loss 4.4118, val loss 4.4115\n",
      "iter 10: loss 3.6775\n",
      "iter 20: loss 3.2180\n",
      "iter 30: loss 2.9502\n",
      "iter 40: loss 2.7981\n",
      "iter 50: loss 2.7036\n",
      "iter 60: loss 2.6539\n",
      "iter 70: loss 2.6290\n",
      "iter 80: loss 2.5862\n",
      "iter 90: loss 2.5713\n",
      "iter 100: loss 2.5443\n",
      "iter 110: loss 2.5513\n",
      "iter 120: loss 2.5293\n",
      "iter 130: loss 2.5327\n",
      "iter 140: loss 2.5285\n",
      "iter 150: loss 2.5283\n",
      "iter 160: loss 2.5211\n",
      "iter 170: loss 2.4976\n",
      "iter 180: loss 2.4976\n",
      "iter 190: loss 2.4903\n",
      "iter 200: loss 2.4783\n",
      "iter 210: loss 2.4943\n",
      "iter 220: loss 2.4794\n",
      "iter 230: loss 2.4685\n",
      "iter 240: loss 2.4442\n",
      "iter 250: loss 2.4365\n",
      "iter 260: loss 2.4449\n",
      "iter 270: loss 2.4179\n",
      "iter 280: loss 2.4121\n",
      "iter 290: loss 2.3959\n",
      "iter 300: loss 2.3744\n",
      "iter 310: loss 2.3726\n",
      "iter 320: loss 2.3744\n",
      "iter 330: loss 2.3678\n",
      "iter 340: loss 2.3257\n",
      "iter 350: loss 2.3125\n",
      "iter 360: loss 2.3186\n",
      "iter 370: loss 2.2896\n",
      "iter 380: loss 2.2906\n",
      "iter 390: loss 2.2759\n",
      "iter 400: loss 2.2469\n",
      "iter 410: loss 2.2296\n",
      "iter 420: loss 2.2133\n",
      "iter 430: loss 2.2008\n",
      "iter 440: loss 2.1810\n",
      "iter 450: loss 2.1800\n",
      "iter 460: loss 2.1577\n",
      "iter 470: loss 2.1435\n",
      "iter 480: loss 2.1290\n",
      "iter 490: loss 2.1284\n",
      "iter 500: loss 2.1101\n",
      "step 500: train loss 2.0394, val loss 2.0382\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 510: loss 2.0914\n",
      "iter 520: loss 2.0873\n",
      "iter 530: loss 2.0513\n",
      "iter 540: loss 2.0506\n",
      "iter 550: loss 2.0452\n",
      "iter 560: loss 2.0213\n",
      "iter 570: loss 2.0388\n",
      "iter 580: loss 2.0188\n",
      "iter 590: loss 2.0176\n",
      "iter 600: loss 1.9717\n",
      "iter 610: loss 1.9631\n",
      "iter 620: loss 1.9597\n",
      "iter 630: loss 1.9531\n",
      "iter 640: loss 1.9492\n",
      "iter 650: loss 1.9258\n",
      "iter 660: loss 1.9345\n",
      "iter 670: loss 1.8992\n",
      "iter 680: loss 1.9017\n",
      "iter 690: loss 1.8999\n",
      "iter 700: loss 1.9047\n",
      "iter 710: loss 1.8830\n",
      "iter 720: loss 1.8758\n",
      "iter 730: loss 1.8624\n",
      "iter 740: loss 1.8570\n",
      "iter 750: loss 1.8543\n",
      "iter 760: loss 1.8575\n",
      "iter 770: loss 1.8240\n",
      "iter 780: loss 1.8195\n",
      "iter 790: loss 1.8156\n",
      "iter 800: loss 1.8290\n",
      "iter 810: loss 1.8073\n",
      "iter 820: loss 1.8012\n",
      "iter 830: loss 1.7877\n",
      "iter 840: loss 1.7885\n",
      "iter 850: loss 1.8136\n",
      "iter 860: loss 1.7546\n",
      "iter 870: loss 1.7674\n",
      "iter 880: loss 1.7572\n",
      "iter 890: loss 1.7454\n",
      "iter 900: loss 1.7247\n",
      "iter 910: loss 1.7429\n",
      "iter 920: loss 1.7275\n",
      "iter 930: loss 1.7238\n",
      "iter 940: loss 1.7115\n",
      "iter 950: loss 1.7188\n",
      "iter 960: loss 1.6740\n",
      "iter 970: loss 1.7138\n",
      "iter 980: loss 1.7079\n",
      "iter 990: loss 1.7058\n",
      "iter 1000: loss 1.6738\n",
      "step 1000: train loss 1.6134, val loss 1.6176\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 1010: loss 1.6870\n",
      "iter 1020: loss 1.6823\n",
      "iter 1030: loss 1.6875\n",
      "iter 1040: loss 1.6849\n",
      "iter 1050: loss 1.6670\n",
      "iter 1060: loss 1.6710\n",
      "iter 1070: loss 1.6256\n",
      "iter 1080: loss 1.6282\n",
      "iter 1090: loss 1.6432\n",
      "iter 1100: loss 1.6533\n",
      "iter 1110: loss 1.6327\n",
      "iter 1120: loss 1.6213\n",
      "iter 1130: loss 1.6431\n",
      "iter 1140: loss 1.6607\n",
      "iter 1150: loss 1.6324\n",
      "iter 1160: loss 1.6148\n",
      "iter 1170: loss 1.5946\n",
      "iter 1180: loss 1.5737\n",
      "iter 1190: loss 1.5932\n",
      "iter 1200: loss 1.6197\n",
      "iter 1210: loss 1.6000\n",
      "iter 1220: loss 1.6068\n",
      "iter 1230: loss 1.5819\n",
      "iter 1240: loss 1.5827\n",
      "iter 1250: loss 1.5830\n",
      "iter 1260: loss 1.5951\n",
      "iter 1270: loss 1.5691\n",
      "iter 1280: loss 1.5666\n",
      "iter 1290: loss 1.5708\n",
      "iter 1300: loss 1.5747\n",
      "iter 1310: loss 1.5729\n",
      "iter 1320: loss 1.5486\n",
      "iter 1330: loss 1.5633\n",
      "iter 1340: loss 1.5653\n",
      "iter 1350: loss 1.5648\n",
      "iter 1360: loss 1.5746\n",
      "iter 1370: loss 1.5439\n",
      "iter 1380: loss 1.5544\n",
      "iter 1390: loss 1.5158\n",
      "iter 1400: loss 1.5287\n",
      "iter 1410: loss 1.5332\n",
      "iter 1420: loss 1.5002\n",
      "iter 1430: loss 1.4995\n",
      "iter 1440: loss 1.5161\n",
      "iter 1450: loss 1.5397\n",
      "iter 1460: loss 1.5086\n",
      "iter 1470: loss 1.5178\n",
      "iter 1480: loss 1.4996\n",
      "iter 1490: loss 1.4908\n",
      "iter 1500: loss 1.5137\n",
      "step 1500: train loss 1.4369, val loss 1.4389\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 1510: loss 1.5213\n",
      "iter 1520: loss 1.4999\n",
      "iter 1530: loss 1.5075\n",
      "iter 1540: loss 1.4952\n",
      "iter 1550: loss 1.4884\n",
      "iter 1560: loss 1.4863\n",
      "iter 1570: loss 1.4779\n",
      "iter 1580: loss 1.4914\n",
      "iter 1590: loss 1.4729\n",
      "iter 1600: loss 1.4934\n",
      "iter 1610: loss 1.4717\n",
      "iter 1620: loss 1.4704\n",
      "iter 1630: loss 1.4825\n",
      "iter 1640: loss 1.4747\n",
      "iter 1650: loss 1.4828\n",
      "iter 1660: loss 1.4588\n",
      "iter 1670: loss 1.4665\n",
      "iter 1680: loss 1.4562\n",
      "iter 1690: loss 1.4754\n",
      "iter 1700: loss 1.4566\n",
      "iter 1710: loss 1.4574\n",
      "iter 1720: loss 1.4457\n",
      "iter 1730: loss 1.4420\n",
      "iter 1740: loss 1.4741\n",
      "iter 1750: loss 1.4427\n",
      "iter 1760: loss 1.4515\n",
      "iter 1770: loss 1.4281\n",
      "iter 1780: loss 1.4501\n",
      "iter 1790: loss 1.4536\n",
      "iter 1800: loss 1.4380\n",
      "iter 1810: loss 1.4448\n",
      "iter 1820: loss 1.4216\n",
      "iter 1830: loss 1.4238\n",
      "iter 1840: loss 1.4370\n",
      "iter 1850: loss 1.4211\n",
      "iter 1860: loss 1.4141\n",
      "iter 1870: loss 1.4114\n",
      "iter 1880: loss 1.4390\n",
      "iter 1890: loss 1.4385\n",
      "iter 1900: loss 1.4276\n",
      "iter 1910: loss 1.4150\n",
      "iter 1920: loss 1.4150\n",
      "iter 1930: loss 1.4341\n",
      "iter 1940: loss 1.3989\n",
      "iter 1950: loss 1.4019\n",
      "iter 1960: loss 1.4145\n",
      "iter 1970: loss 1.4214\n",
      "iter 1980: loss 1.4093\n",
      "iter 1990: loss 1.3992\n",
      "iter 2000: loss 1.4024\n",
      "step 2000: train loss 1.3379, val loss 1.3406\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 2010: loss 1.4048\n",
      "iter 2020: loss 1.3895\n",
      "iter 2030: loss 1.4129\n",
      "iter 2040: loss 1.4110\n",
      "iter 2050: loss 1.4083\n",
      "iter 2060: loss 1.3816\n",
      "iter 2070: loss 1.3775\n",
      "iter 2080: loss 1.3729\n",
      "iter 2090: loss 1.3776\n",
      "iter 2100: loss 1.4132\n",
      "iter 2110: loss 1.3840\n",
      "iter 2120: loss 1.3581\n",
      "iter 2130: loss 1.3581\n",
      "iter 2140: loss 1.3699\n",
      "iter 2150: loss 1.3737\n",
      "iter 2160: loss 1.3662\n",
      "iter 2170: loss 1.3779\n",
      "iter 2180: loss 1.3598\n",
      "iter 2190: loss 1.3763\n",
      "iter 2200: loss 1.3854\n",
      "iter 2210: loss 1.3808\n",
      "iter 2220: loss 1.3775\n",
      "iter 2230: loss 1.3796\n",
      "iter 2240: loss 1.3893\n",
      "iter 2250: loss 1.3656\n",
      "iter 2260: loss 1.3851\n",
      "iter 2270: loss 1.3591\n",
      "iter 2280: loss 1.3586\n",
      "iter 2290: loss 1.3584\n",
      "iter 2300: loss 1.3593\n",
      "iter 2310: loss 1.3472\n",
      "iter 2320: loss 1.3640\n",
      "iter 2330: loss 1.3506\n",
      "iter 2340: loss 1.3706\n",
      "iter 2350: loss 1.3562\n",
      "iter 2360: loss 1.3397\n",
      "iter 2370: loss 1.3311\n",
      "iter 2380: loss 1.3531\n",
      "iter 2390: loss 1.3563\n",
      "iter 2400: loss 1.3420\n",
      "iter 2410: loss 1.3632\n",
      "iter 2420: loss 1.3422\n",
      "iter 2430: loss 1.3418\n",
      "iter 2440: loss 1.3212\n",
      "iter 2450: loss 1.3312\n",
      "iter 2460: loss 1.3515\n",
      "iter 2470: loss 1.3395\n",
      "iter 2480: loss 1.3378\n",
      "iter 2490: loss 1.3340\n",
      "iter 2500: loss 1.3416\n",
      "step 2500: train loss 1.2723, val loss 1.2743\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 2510: loss 1.3087\n",
      "iter 2520: loss 1.3435\n",
      "iter 2530: loss 1.3095\n",
      "iter 2540: loss 1.3163\n",
      "iter 2550: loss 1.3309\n",
      "iter 2560: loss 1.3203\n",
      "iter 2570: loss 1.2881\n",
      "iter 2580: loss 1.3052\n",
      "iter 2590: loss 1.3303\n",
      "iter 2600: loss 1.3094\n",
      "iter 2610: loss 1.3067\n",
      "iter 2620: loss 1.3055\n",
      "iter 2630: loss 1.3316\n",
      "iter 2640: loss 1.3422\n",
      "iter 2650: loss 1.2925\n",
      "iter 2660: loss 1.3056\n",
      "iter 2670: loss 1.3328\n",
      "iter 2680: loss 1.3191\n",
      "iter 2690: loss 1.3147\n",
      "iter 2700: loss 1.3240\n",
      "iter 2710: loss 1.2828\n",
      "iter 2720: loss 1.3136\n",
      "iter 2730: loss 1.2990\n",
      "iter 2740: loss 1.2916\n",
      "iter 2750: loss 1.3200\n",
      "iter 2760: loss 1.3000\n",
      "iter 2770: loss 1.3144\n",
      "iter 2780: loss 1.2950\n",
      "iter 2790: loss 1.3098\n",
      "iter 2800: loss 1.3169\n",
      "iter 2810: loss 1.2896\n",
      "iter 2820: loss 1.3021\n",
      "iter 2830: loss 1.3057\n",
      "iter 2840: loss 1.2934\n",
      "iter 2850: loss 1.3018\n",
      "iter 2860: loss 1.2900\n",
      "iter 2870: loss 1.2862\n",
      "iter 2880: loss 1.3008\n",
      "iter 2890: loss 1.2834\n",
      "iter 2900: loss 1.3056\n",
      "iter 2910: loss 1.3087\n",
      "iter 2920: loss 1.2884\n",
      "iter 2930: loss 1.2823\n",
      "iter 2940: loss 1.2833\n",
      "iter 2950: loss 1.2930\n",
      "iter 2960: loss 1.2536\n",
      "iter 2970: loss 1.2931\n",
      "iter 2980: loss 1.2920\n",
      "iter 2990: loss 1.2766\n",
      "iter 3000: loss 1.2793\n",
      "step 3000: train loss 1.2176, val loss 1.2246\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3010: loss 1.2879\n",
      "iter 3020: loss 1.2789\n",
      "iter 3030: loss 1.2740\n",
      "iter 3040: loss 1.3049\n",
      "iter 3050: loss 1.2806\n",
      "iter 3060: loss 1.2635\n",
      "iter 3070: loss 1.2887\n",
      "iter 3080: loss 1.2711\n",
      "iter 3090: loss 1.2863\n",
      "iter 3100: loss 1.2899\n",
      "iter 3110: loss 1.3054\n",
      "iter 3120: loss 1.2776\n",
      "iter 3130: loss 1.2889\n",
      "iter 3140: loss 1.2749\n",
      "iter 3150: loss 1.2753\n",
      "iter 3160: loss 1.2776\n",
      "iter 3170: loss 1.2730\n",
      "iter 3180: loss 1.2545\n",
      "iter 3190: loss 1.2446\n",
      "iter 3200: loss 1.2897\n",
      "iter 3210: loss 1.2798\n",
      "iter 3220: loss 1.2689\n",
      "iter 3230: loss 1.2839\n",
      "iter 3240: loss 1.2729\n",
      "iter 3250: loss 1.2697\n",
      "iter 3260: loss 1.2523\n",
      "iter 3270: loss 1.2664\n",
      "iter 3280: loss 1.2804\n",
      "iter 3290: loss 1.2549\n",
      "iter 3300: loss 1.2620\n",
      "iter 3310: loss 1.2523\n",
      "iter 3320: loss 1.2687\n",
      "iter 3330: loss 1.2775\n",
      "iter 3340: loss 1.2512\n",
      "iter 3350: loss 1.2540\n",
      "iter 3360: loss 1.2340\n",
      "iter 3370: loss 1.2366\n",
      "iter 3380: loss 1.2490\n",
      "iter 3390: loss 1.2712\n",
      "iter 3400: loss 1.2532\n",
      "iter 3410: loss 1.2422\n",
      "iter 3420: loss 1.2548\n",
      "iter 3430: loss 1.2622\n",
      "iter 3440: loss 1.2436\n",
      "iter 3450: loss 1.2401\n",
      "iter 3460: loss 1.2322\n",
      "iter 3470: loss 1.2462\n",
      "iter 3480: loss 1.2409\n",
      "iter 3490: loss 1.2306\n",
      "iter 3500: loss 1.2577\n",
      "step 3500: train loss 1.1814, val loss 1.1870\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3510: loss 1.2350\n",
      "iter 3520: loss 1.2423\n",
      "iter 3530: loss 1.2465\n",
      "iter 3540: loss 1.2315\n",
      "iter 3550: loss 1.2275\n",
      "iter 3560: loss 1.2294\n",
      "iter 3570: loss 1.2406\n",
      "iter 3580: loss 1.2300\n",
      "iter 3590: loss 1.2397\n",
      "iter 3600: loss 1.2579\n",
      "iter 3610: loss 1.2432\n",
      "iter 3620: loss 1.2330\n",
      "iter 3630: loss 1.2394\n",
      "iter 3640: loss 1.2135\n",
      "iter 3650: loss 1.2365\n",
      "iter 3660: loss 1.2297\n",
      "iter 3670: loss 1.2081\n",
      "iter 3680: loss 1.2382\n",
      "iter 3690: loss 1.2278\n",
      "iter 3700: loss 1.2335\n",
      "iter 3710: loss 1.2298\n",
      "iter 3720: loss 1.2303\n",
      "iter 3730: loss 1.2291\n",
      "iter 3740: loss 1.2059\n",
      "iter 3750: loss 1.2156\n",
      "iter 3760: loss 1.2112\n",
      "iter 3770: loss 1.2264\n",
      "iter 3780: loss 1.2376\n",
      "iter 3790: loss 1.2315\n",
      "iter 3800: loss 1.2265\n",
      "iter 3810: loss 1.2354\n",
      "iter 3820: loss 1.2282\n",
      "iter 3830: loss 1.2145\n",
      "iter 3840: loss 1.2189\n",
      "iter 3850: loss 1.2141\n",
      "iter 3860: loss 1.2077\n",
      "iter 3870: loss 1.2128\n",
      "iter 3880: loss 1.2160\n",
      "iter 3890: loss 1.2246\n",
      "iter 3900: loss 1.2209\n",
      "iter 3910: loss 1.2460\n",
      "iter 3920: loss 1.2161\n",
      "iter 3930: loss 1.2187\n",
      "iter 3940: loss 1.2127\n",
      "iter 3950: loss 1.2186\n",
      "iter 3960: loss 1.2127\n",
      "iter 3970: loss 1.2142\n",
      "iter 3980: loss 1.2074\n",
      "iter 3990: loss 1.2242\n",
      "iter 4000: loss 1.1952\n",
      "step 4000: train loss 1.1480, val loss 1.1489\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4010: loss 1.2162\n",
      "iter 4020: loss 1.2135\n",
      "iter 4030: loss 1.2189\n",
      "iter 4040: loss 1.2087\n",
      "iter 4050: loss 1.2097\n",
      "iter 4060: loss 1.2090\n",
      "iter 4070: loss 1.1907\n",
      "iter 4080: loss 1.2274\n",
      "iter 4090: loss 1.1996\n",
      "iter 4100: loss 1.2058\n",
      "iter 4110: loss 1.1975\n",
      "iter 4120: loss 1.2092\n",
      "iter 4130: loss 1.1940\n",
      "iter 4140: loss 1.1876\n",
      "iter 4150: loss 1.1827\n",
      "iter 4160: loss 1.2006\n",
      "iter 4170: loss 1.1851\n",
      "iter 4180: loss 1.2122\n",
      "iter 4190: loss 1.2017\n",
      "iter 4200: loss 1.1930\n",
      "iter 4210: loss 1.1792\n",
      "iter 4220: loss 1.1786\n",
      "iter 4230: loss 1.1900\n",
      "iter 4240: loss 1.1941\n",
      "iter 4250: loss 1.2113\n",
      "iter 4260: loss 1.2025\n",
      "iter 4270: loss 1.1910\n",
      "iter 4280: loss 1.1954\n",
      "iter 4290: loss 1.1932\n",
      "iter 4300: loss 1.1906\n",
      "iter 4310: loss 1.1871\n",
      "iter 4320: loss 1.1887\n",
      "iter 4330: loss 1.1869\n",
      "iter 4340: loss 1.2019\n",
      "iter 4350: loss 1.1882\n",
      "iter 4360: loss 1.1920\n",
      "iter 4370: loss 1.2008\n",
      "iter 4380: loss 1.1838\n",
      "iter 4390: loss 1.1683\n",
      "iter 4400: loss 1.1568\n",
      "iter 4410: loss 1.2011\n",
      "iter 4420: loss 1.1840\n",
      "iter 4430: loss 1.1760\n",
      "iter 4440: loss 1.1831\n",
      "iter 4450: loss 1.1834\n",
      "iter 4460: loss 1.1785\n",
      "iter 4470: loss 1.1886\n",
      "iter 4480: loss 1.1848\n",
      "iter 4490: loss 1.1725\n",
      "iter 4500: loss 1.1831\n",
      "step 4500: train loss 1.1136, val loss 1.1175\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4510: loss 1.1846\n",
      "iter 4520: loss 1.1847\n",
      "iter 4530: loss 1.1948\n",
      "iter 4540: loss 1.1755\n",
      "iter 4550: loss 1.1779\n",
      "iter 4560: loss 1.1912\n",
      "iter 4570: loss 1.1669\n",
      "iter 4580: loss 1.2105\n",
      "iter 4590: loss 1.1672\n",
      "iter 4600: loss 1.1766\n",
      "iter 4610: loss 1.1766\n",
      "iter 4620: loss 1.1693\n",
      "iter 4630: loss 1.1825\n",
      "iter 4640: loss 1.1780\n",
      "iter 4650: loss 1.1707\n",
      "iter 4660: loss 1.1759\n",
      "iter 4670: loss 1.1719\n",
      "iter 4680: loss 1.1771\n",
      "iter 4690: loss 1.1945\n",
      "iter 4700: loss 1.1737\n",
      "iter 4710: loss 1.1443\n",
      "iter 4720: loss 1.1625\n",
      "iter 4730: loss 1.1848\n",
      "iter 4740: loss 1.1573\n",
      "iter 4750: loss 1.1453\n",
      "iter 4760: loss 1.1555\n",
      "iter 4770: loss 1.1588\n",
      "iter 4780: loss 1.1758\n",
      "iter 4790: loss 1.1484\n",
      "iter 4800: loss 1.1577\n",
      "iter 4810: loss 1.1626\n",
      "iter 4820: loss 1.1497\n",
      "iter 4830: loss 1.1414\n",
      "iter 4840: loss 1.1668\n",
      "iter 4850: loss 1.1726\n",
      "iter 4860: loss 1.1566\n",
      "iter 4870: loss 1.1602\n",
      "iter 4880: loss 1.1436\n",
      "iter 4890: loss 1.1540\n",
      "iter 4900: loss 1.1640\n",
      "iter 4910: loss 1.1677\n",
      "iter 4920: loss 1.1638\n",
      "iter 4930: loss 1.1554\n",
      "iter 4940: loss 1.1399\n",
      "iter 4950: loss 1.1515\n",
      "iter 4960: loss 1.1564\n",
      "iter 4970: loss 1.1556\n",
      "iter 4980: loss 1.1647\n",
      "iter 4990: loss 1.1568\n",
      "iter 5000: loss 1.1527\n",
      "step 5000: train loss 1.0871, val loss 1.0875\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.1052\n",
      "step 0: train loss 4.3726, val loss 4.3720\n",
      "iter 10: loss 3.6869\n",
      "iter 20: loss 3.3916\n",
      "iter 30: loss 3.1468\n",
      "iter 40: loss 2.9671\n",
      "iter 50: loss 2.8487\n",
      "iter 60: loss 2.7691\n",
      "iter 70: loss 2.7208\n",
      "iter 80: loss 2.6849\n",
      "iter 90: loss 2.6410\n",
      "iter 100: loss 2.6278\n",
      "iter 110: loss 2.5991\n",
      "iter 120: loss 2.6069\n",
      "iter 130: loss 2.5630\n",
      "iter 140: loss 2.5709\n",
      "iter 150: loss 2.5608\n",
      "iter 160: loss 2.5497\n",
      "iter 170: loss 2.5503\n",
      "iter 180: loss 2.5400\n",
      "iter 190: loss 2.5276\n",
      "iter 200: loss 2.5148\n",
      "iter 210: loss 2.5180\n",
      "iter 220: loss 2.5148\n",
      "iter 230: loss 2.5061\n",
      "iter 240: loss 2.4880\n",
      "iter 250: loss 2.5038\n",
      "iter 260: loss 2.4881\n",
      "iter 270: loss 2.4823\n",
      "iter 280: loss 2.4898\n",
      "iter 290: loss 2.4596\n",
      "iter 300: loss 2.4622\n",
      "iter 310: loss 2.4734\n",
      "iter 320: loss 2.4524\n",
      "iter 330: loss 2.4258\n",
      "iter 340: loss 2.4157\n",
      "iter 350: loss 2.4207\n",
      "iter 360: loss 2.3958\n",
      "iter 370: loss 2.3910\n",
      "iter 380: loss 2.3602\n",
      "iter 390: loss 2.3607\n",
      "iter 400: loss 2.3406\n",
      "iter 410: loss 2.3353\n",
      "iter 420: loss 2.2908\n",
      "iter 430: loss 2.2961\n",
      "iter 440: loss 2.2787\n",
      "iter 450: loss 2.2806\n",
      "iter 460: loss 2.2456\n",
      "iter 470: loss 2.2551\n",
      "iter 480: loss 2.2230\n",
      "iter 490: loss 2.1982\n",
      "iter 500: loss 2.2047\n",
      "step 500: train loss 2.1363, val loss 2.1352\n",
      "iter 510: loss 2.1800\n",
      "iter 520: loss 2.1720\n",
      "iter 530: loss 2.1574\n",
      "iter 540: loss 2.1582\n",
      "iter 550: loss 2.1341\n",
      "iter 560: loss 2.0993\n",
      "iter 570: loss 2.1033\n",
      "iter 580: loss 2.0954\n",
      "iter 590: loss 2.0793\n",
      "iter 600: loss 2.0571\n",
      "iter 610: loss 2.0315\n",
      "iter 620: loss 2.0452\n",
      "iter 630: loss 2.0561\n",
      "iter 640: loss 2.0127\n",
      "iter 650: loss 1.9958\n",
      "iter 660: loss 2.0143\n",
      "iter 670: loss 1.9654\n",
      "iter 680: loss 1.9754\n",
      "iter 690: loss 1.9690\n",
      "iter 700: loss 1.9333\n",
      "iter 710: loss 1.9667\n",
      "iter 720: loss 1.9564\n",
      "iter 730: loss 1.9128\n",
      "iter 740: loss 1.9155\n",
      "iter 750: loss 1.9187\n",
      "iter 760: loss 1.9078\n",
      "iter 770: loss 1.8770\n",
      "iter 780: loss 1.8873\n",
      "iter 790: loss 1.8976\n",
      "iter 800: loss 1.8562\n",
      "iter 810: loss 1.8443\n",
      "iter 820: loss 1.8317\n",
      "iter 830: loss 1.8442\n",
      "iter 840: loss 1.8376\n",
      "iter 850: loss 1.8234\n",
      "iter 860: loss 1.8151\n",
      "iter 870: loss 1.8310\n",
      "iter 880: loss 1.8090\n",
      "iter 890: loss 1.7856\n",
      "iter 900: loss 1.7938\n",
      "iter 910: loss 1.7690\n",
      "iter 920: loss 1.7620\n",
      "iter 930: loss 1.7527\n",
      "iter 940: loss 1.7577\n",
      "iter 950: loss 1.7515\n",
      "iter 960: loss 1.7381\n",
      "iter 970: loss 1.7261\n",
      "iter 980: loss 1.7347\n",
      "iter 990: loss 1.7401\n",
      "iter 1000: loss 1.7541\n",
      "step 1000: train loss 1.6474, val loss 1.6486\n",
      "iter 1010: loss 1.7209\n",
      "iter 1020: loss 1.7259\n",
      "iter 1030: loss 1.6899\n",
      "iter 1040: loss 1.6858\n",
      "iter 1050: loss 1.6834\n",
      "iter 1060: loss 1.6849\n",
      "iter 1070: loss 1.6646\n",
      "iter 1080: loss 1.6839\n",
      "iter 1090: loss 1.6801\n",
      "iter 1100: loss 1.6764\n",
      "iter 1110: loss 1.6853\n",
      "iter 1120: loss 1.6491\n",
      "iter 1130: loss 1.6406\n",
      "iter 1140: loss 1.6566\n",
      "iter 1150: loss 1.6606\n",
      "iter 1160: loss 1.6392\n",
      "iter 1170: loss 1.6517\n",
      "iter 1180: loss 1.6286\n",
      "iter 1190: loss 1.6433\n",
      "iter 1200: loss 1.6486\n",
      "iter 1210: loss 1.6229\n",
      "iter 1220: loss 1.6119\n",
      "iter 1230: loss 1.6115\n",
      "iter 1240: loss 1.5793\n",
      "iter 1250: loss 1.6040\n",
      "iter 1260: loss 1.5972\n",
      "iter 1270: loss 1.5853\n",
      "iter 1280: loss 1.5795\n",
      "iter 1290: loss 1.6054\n",
      "iter 1300: loss 1.5875\n",
      "iter 1310: loss 1.5791\n",
      "iter 1320: loss 1.5788\n",
      "iter 1330: loss 1.5800\n",
      "iter 1340: loss 1.5688\n",
      "iter 1350: loss 1.5733\n",
      "iter 1360: loss 1.5884\n",
      "iter 1370: loss 1.5733\n",
      "iter 1380: loss 1.5521\n",
      "iter 1390: loss 1.5437\n",
      "iter 1400: loss 1.5596\n",
      "iter 1410: loss 1.5430\n",
      "iter 1420: loss 1.5473\n",
      "iter 1430: loss 1.5709\n",
      "iter 1440: loss 1.5507\n",
      "iter 1450: loss 1.5565\n",
      "iter 1460: loss 1.5315\n",
      "iter 1470: loss 1.5155\n",
      "iter 1480: loss 1.5154\n",
      "iter 1490: loss 1.5115\n",
      "iter 1500: loss 1.5013\n",
      "step 1500: train loss 1.4496, val loss 1.4539\n",
      "iter 1510: loss 1.5297\n",
      "iter 1520: loss 1.5102\n",
      "iter 1530: loss 1.5283\n",
      "iter 1540: loss 1.5127\n",
      "iter 1550: loss 1.4886\n",
      "iter 1560: loss 1.5064\n",
      "iter 1570: loss 1.5086\n",
      "iter 1580: loss 1.4901\n",
      "iter 1590: loss 1.5040\n",
      "iter 1600: loss 1.4951\n",
      "iter 1610: loss 1.5140\n",
      "iter 1620: loss 1.4812\n",
      "iter 1630: loss 1.4999\n",
      "iter 1640: loss 1.4958\n",
      "iter 1650: loss 1.4795\n",
      "iter 1660: loss 1.4811\n",
      "iter 1670: loss 1.4843\n",
      "iter 1680: loss 1.4867\n",
      "iter 1690: loss 1.4697\n",
      "iter 1700: loss 1.4426\n",
      "iter 1710: loss 1.4714\n",
      "iter 1720: loss 1.4679\n",
      "iter 1730: loss 1.4419\n",
      "iter 1740: loss 1.4418\n",
      "iter 1750: loss 1.4602\n",
      "iter 1760: loss 1.4609\n",
      "iter 1770: loss 1.4688\n",
      "iter 1780: loss 1.4646\n",
      "iter 1790: loss 1.4691\n",
      "iter 1800: loss 1.4603\n",
      "iter 1810: loss 1.4467\n",
      "iter 1820: loss 1.4509\n",
      "iter 1830: loss 1.4517\n",
      "iter 1840: loss 1.4401\n",
      "iter 1850: loss 1.4411\n",
      "iter 1860: loss 1.4437\n",
      "iter 1870: loss 1.4245\n",
      "iter 1880: loss 1.4456\n",
      "iter 1890: loss 1.4405\n",
      "iter 1900: loss 1.4452\n",
      "iter 1910: loss 1.4287\n",
      "iter 1920: loss 1.4264\n",
      "iter 1930: loss 1.4179\n",
      "iter 1940: loss 1.4179\n",
      "iter 1950: loss 1.4286\n",
      "iter 1960: loss 1.4072\n",
      "iter 1970: loss 1.4122\n",
      "iter 1980: loss 1.4115\n",
      "iter 1990: loss 1.4166\n",
      "iter 2000: loss 1.4017\n",
      "step 2000: train loss 1.3519, val loss 1.3489\n",
      "iter 2010: loss 1.4274\n",
      "iter 2020: loss 1.3781\n",
      "iter 2030: loss 1.4136\n",
      "iter 2040: loss 1.3968\n",
      "iter 2050: loss 1.4191\n",
      "iter 2060: loss 1.3923\n",
      "iter 2070: loss 1.4183\n",
      "iter 2080: loss 1.4109\n",
      "iter 2090: loss 1.4013\n",
      "iter 2100: loss 1.4087\n",
      "iter 2110: loss 1.3990\n",
      "iter 2120: loss 1.3830\n",
      "iter 2130: loss 1.3898\n",
      "iter 2140: loss 1.4093\n",
      "iter 2150: loss 1.3830\n",
      "iter 2160: loss 1.3896\n",
      "iter 2170: loss 1.3799\n",
      "iter 2180: loss 1.3695\n",
      "iter 2190: loss 1.3713\n",
      "iter 2200: loss 1.3706\n",
      "iter 2210: loss 1.3741\n",
      "iter 2220: loss 1.3883\n",
      "iter 2230: loss 1.3763\n",
      "iter 2240: loss 1.3567\n",
      "iter 2250: loss 1.3759\n",
      "iter 2260: loss 1.3601\n",
      "iter 2270: loss 1.3838\n",
      "iter 2280: loss 1.3937\n",
      "iter 2290: loss 1.3785\n",
      "iter 2300: loss 1.3464\n",
      "iter 2310: loss 1.3673\n",
      "iter 2320: loss 1.3464\n",
      "iter 2330: loss 1.3680\n",
      "iter 2340: loss 1.3621\n",
      "iter 2350: loss 1.3734\n",
      "iter 2360: loss 1.3380\n",
      "iter 2370: loss 1.3635\n",
      "iter 2380: loss 1.3394\n",
      "iter 2390: loss 1.3445\n",
      "iter 2400: loss 1.3487\n",
      "iter 2410: loss 1.3229\n",
      "iter 2420: loss 1.3543\n",
      "iter 2430: loss 1.3404\n",
      "iter 2440: loss 1.3503\n",
      "iter 2450: loss 1.3284\n",
      "iter 2460: loss 1.3418\n",
      "iter 2470: loss 1.3408\n",
      "iter 2480: loss 1.3533\n",
      "iter 2490: loss 1.3380\n",
      "iter 2500: loss 1.3291\n",
      "step 2500: train loss 1.2792, val loss 1.2804\n",
      "iter 2510: loss 1.3414\n",
      "iter 2520: loss 1.3291\n",
      "iter 2530: loss 1.3829\n",
      "iter 2540: loss 1.3230\n",
      "iter 2550: loss 1.3454\n",
      "iter 2560: loss 1.3347\n",
      "iter 2570: loss 1.3531\n",
      "iter 2580: loss 1.3321\n",
      "iter 2590: loss 1.3222\n",
      "iter 2600: loss 1.3467\n",
      "iter 2610: loss 1.3287\n",
      "iter 2620: loss 1.3219\n",
      "iter 2630: loss 1.3135\n",
      "iter 2640: loss 1.3508\n",
      "iter 2650: loss 1.3147\n",
      "iter 2660: loss 1.3270\n",
      "iter 2670: loss 1.3124\n",
      "iter 2680: loss 1.3242\n",
      "iter 2690: loss 1.3318\n",
      "iter 2700: loss 1.3142\n",
      "iter 2710: loss 1.3280\n",
      "iter 2720: loss 1.3349\n",
      "iter 2730: loss 1.3124\n",
      "iter 2740: loss 1.2905\n",
      "iter 2750: loss 1.3315\n",
      "iter 2760: loss 1.3242\n",
      "iter 2770: loss 1.3041\n",
      "iter 2780: loss 1.2985\n",
      "iter 2790: loss 1.3011\n",
      "iter 2800: loss 1.3125\n",
      "iter 2810: loss 1.3223\n",
      "iter 2820: loss 1.2970\n",
      "iter 2830: loss 1.3000\n",
      "iter 2840: loss 1.3028\n",
      "iter 2850: loss 1.2930\n",
      "iter 2860: loss 1.3021\n",
      "iter 2870: loss 1.2827\n",
      "iter 2880: loss 1.3144\n",
      "iter 2890: loss 1.2835\n",
      "iter 2900: loss 1.2961\n",
      "iter 2910: loss 1.3068\n",
      "iter 2920: loss 1.3021\n",
      "iter 2930: loss 1.2777\n",
      "iter 2940: loss 1.2878\n",
      "iter 2950: loss 1.3149\n",
      "iter 2960: loss 1.2612\n",
      "iter 2970: loss 1.3047\n",
      "iter 2980: loss 1.2884\n",
      "iter 2990: loss 1.2742\n",
      "iter 3000: loss 1.2972\n",
      "step 3000: train loss 1.2309, val loss 1.2329\n",
      "iter 3010: loss 1.2887\n",
      "iter 3020: loss 1.2777\n",
      "iter 3030: loss 1.2746\n",
      "iter 3040: loss 1.2900\n",
      "iter 3050: loss 1.2684\n",
      "iter 3060: loss 1.2913\n",
      "iter 3070: loss 1.2726\n",
      "iter 3080: loss 1.2922\n",
      "iter 3090: loss 1.2809\n",
      "iter 3100: loss 1.2703\n",
      "iter 3110: loss 1.3033\n",
      "iter 3120: loss 1.2925\n",
      "iter 3130: loss 1.2722\n",
      "iter 3140: loss 1.2813\n",
      "iter 3150: loss 1.2651\n",
      "iter 3160: loss 1.2775\n",
      "iter 3170: loss 1.2665\n",
      "iter 3180: loss 1.2718\n",
      "iter 3190: loss 1.2470\n",
      "iter 3200: loss 1.2656\n",
      "iter 3210: loss 1.2618\n",
      "iter 3220: loss 1.2752\n",
      "iter 3230: loss 1.2730\n",
      "iter 3240: loss 1.2865\n",
      "iter 3250: loss 1.2771\n",
      "iter 3260: loss 1.2739\n",
      "iter 3270: loss 1.2816\n",
      "iter 3280: loss 1.2663\n",
      "iter 3290: loss 1.2601\n",
      "iter 3300: loss 1.2804\n",
      "iter 3310: loss 1.2706\n",
      "iter 3320: loss 1.2539\n",
      "iter 3330: loss 1.2569\n",
      "iter 3340: loss 1.2465\n",
      "iter 3350: loss 1.2622\n",
      "iter 3360: loss 1.2333\n",
      "iter 3370: loss 1.2448\n",
      "iter 3380: loss 1.2681\n",
      "iter 3390: loss 1.2658\n",
      "iter 3400: loss 1.2521\n",
      "iter 3410: loss 1.2453\n",
      "iter 3420: loss 1.2436\n",
      "iter 3430: loss 1.2531\n",
      "iter 3440: loss 1.2411\n",
      "iter 3450: loss 1.2351\n",
      "iter 3460: loss 1.2487\n",
      "iter 3470: loss 1.2694\n",
      "iter 3480: loss 1.2559\n",
      "iter 3490: loss 1.2545\n",
      "iter 3500: loss 1.2685\n",
      "step 3500: train loss 1.1855, val loss 1.1892\n",
      "iter 3510: loss 1.2481\n",
      "iter 3520: loss 1.2349\n",
      "iter 3530: loss 1.2486\n",
      "iter 3540: loss 1.2576\n",
      "iter 3550: loss 1.2446\n",
      "iter 3560: loss 1.2526\n",
      "iter 3570: loss 1.2386\n",
      "iter 3580: loss 1.2666\n",
      "iter 3590: loss 1.2342\n",
      "iter 3600: loss 1.2234\n",
      "iter 3610: loss 1.2478\n",
      "iter 3620: loss 1.2419\n",
      "iter 3630: loss 1.2454\n",
      "iter 3640: loss 1.2711\n",
      "iter 3650: loss 1.2204\n",
      "iter 3660: loss 1.2529\n",
      "iter 3670: loss 1.2382\n",
      "iter 3680: loss 1.2410\n",
      "iter 3690: loss 1.2305\n",
      "iter 3700: loss 1.2311\n",
      "iter 3710: loss 1.2246\n",
      "iter 3720: loss 1.2393\n",
      "iter 3730: loss 1.2238\n",
      "iter 3740: loss 1.2339\n",
      "iter 3750: loss 1.2401\n",
      "iter 3760: loss 1.2146\n",
      "iter 3770: loss 1.2308\n",
      "iter 3780: loss 1.2238\n",
      "iter 3790: loss 1.2411\n",
      "iter 3800: loss 1.2297\n",
      "iter 3810: loss 1.2160\n",
      "iter 3820: loss 1.2338\n",
      "iter 3830: loss 1.2284\n",
      "iter 3840: loss 1.2564\n",
      "iter 3850: loss 1.2266\n",
      "iter 3860: loss 1.2211\n",
      "iter 3870: loss 1.2258\n",
      "iter 3880: loss 1.2181\n",
      "iter 3890: loss 1.2081\n",
      "iter 3900: loss 1.2024\n",
      "iter 3910: loss 1.2249\n",
      "iter 3920: loss 1.2261\n",
      "iter 3930: loss 1.1982\n",
      "iter 3940: loss 1.2163\n",
      "iter 3950: loss 1.1968\n",
      "iter 3960: loss 1.2277\n",
      "iter 3970: loss 1.2164\n",
      "iter 3980: loss 1.2034\n",
      "iter 3990: loss 1.2066\n",
      "iter 4000: loss 1.1973\n",
      "step 4000: train loss 1.1501, val loss 1.1524\n",
      "iter 4010: loss 1.2005\n",
      "iter 4020: loss 1.2165\n",
      "iter 4030: loss 1.2188\n",
      "iter 4040: loss 1.1951\n",
      "iter 4050: loss 1.2061\n",
      "iter 4060: loss 1.2108\n",
      "iter 4070: loss 1.1995\n",
      "iter 4080: loss 1.2063\n",
      "iter 4090: loss 1.1958\n",
      "iter 4100: loss 1.2032\n",
      "iter 4110: loss 1.2217\n",
      "iter 4120: loss 1.2096\n",
      "iter 4130: loss 1.2155\n",
      "iter 4140: loss 1.2058\n",
      "iter 4150: loss 1.2007\n",
      "iter 4160: loss 1.2145\n",
      "iter 4170: loss 1.1985\n",
      "iter 4180: loss 1.2025\n",
      "iter 4190: loss 1.1977\n",
      "iter 4200: loss 1.2042\n",
      "iter 4210: loss 1.1809\n",
      "iter 4220: loss 1.2044\n",
      "iter 4230: loss 1.1863\n",
      "iter 4240: loss 1.1939\n",
      "iter 4250: loss 1.1905\n",
      "iter 4260: loss 1.1962\n",
      "iter 4270: loss 1.2045\n",
      "iter 4280: loss 1.1930\n",
      "iter 4290: loss 1.1795\n",
      "iter 4300: loss 1.1956\n",
      "iter 4310: loss 1.1935\n",
      "iter 4320: loss 1.1864\n",
      "iter 4330: loss 1.1958\n",
      "iter 4340: loss 1.2006\n",
      "iter 4350: loss 1.1949\n",
      "iter 4360: loss 1.1948\n",
      "iter 4370: loss 1.1976\n",
      "iter 4380: loss 1.1959\n",
      "iter 4390: loss 1.1968\n",
      "iter 4400: loss 1.1821\n",
      "iter 4410: loss 1.2023\n",
      "iter 4420: loss 1.1937\n",
      "iter 4430: loss 1.1874\n",
      "iter 4440: loss 1.1768\n",
      "iter 4450: loss 1.1896\n",
      "iter 4460: loss 1.1744\n",
      "iter 4470: loss 1.1856\n",
      "iter 4480: loss 1.1838\n",
      "iter 4490: loss 1.1878\n",
      "iter 4500: loss 1.1626\n",
      "step 4500: train loss 1.1192, val loss 1.1209\n",
      "iter 4510: loss 1.1680\n",
      "iter 4520: loss 1.1785\n",
      "iter 4530: loss 1.2183\n",
      "iter 4540: loss 1.1817\n",
      "iter 4550: loss 1.1997\n",
      "iter 4560: loss 1.1881\n",
      "iter 4570: loss 1.1744\n",
      "iter 4580: loss 1.1818\n",
      "iter 4590: loss 1.2059\n",
      "iter 4600: loss 1.1601\n",
      "iter 4610: loss 1.1761\n",
      "iter 4620: loss 1.1774\n",
      "iter 4630: loss 1.1850\n",
      "iter 4640: loss 1.1618\n",
      "iter 4650: loss 1.1672\n",
      "iter 4660: loss 1.1732\n",
      "iter 4670: loss 1.1791\n",
      "iter 4680: loss 1.1714\n",
      "iter 4690: loss 1.1597\n",
      "iter 4700: loss 1.1674\n",
      "iter 4710: loss 1.1700\n",
      "iter 4720: loss 1.1488\n",
      "iter 4730: loss 1.1779\n",
      "iter 4740: loss 1.1549\n",
      "iter 4750: loss 1.1632\n",
      "iter 4760: loss 1.1541\n",
      "iter 4770: loss 1.1684\n",
      "iter 4780: loss 1.1628\n",
      "iter 4790: loss 1.1685\n",
      "iter 4800: loss 1.1519\n",
      "iter 4810: loss 1.1717\n",
      "iter 4820: loss 1.1651\n",
      "iter 4830: loss 1.1543\n",
      "iter 4840: loss 1.1727\n",
      "iter 4850: loss 1.1737\n",
      "iter 4860: loss 1.1545\n",
      "iter 4870: loss 1.1555\n",
      "iter 4880: loss 1.1498\n",
      "iter 4890: loss 1.1724\n",
      "iter 4900: loss 1.1636\n",
      "iter 4910: loss 1.1421\n",
      "iter 4920: loss 1.1605\n",
      "iter 4930: loss 1.1530\n",
      "iter 4940: loss 1.1645\n",
      "iter 4950: loss 1.1659\n",
      "iter 4960: loss 1.1614\n",
      "iter 4970: loss 1.1528\n",
      "iter 4980: loss 1.1692\n",
      "iter 4990: loss 1.1428\n",
      "iter 5000: loss 1.1545\n",
      "step 5000: train loss 1.0865, val loss 1.0892\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.3077\n",
      "step 0: train loss 4.5958, val loss 4.5958\n",
      "iter 10: loss 3.1609\n",
      "iter 20: loss 3.0995\n",
      "iter 30: loss 3.0397\n",
      "iter 40: loss 2.9480\n",
      "iter 50: loss 2.7967\n",
      "iter 60: loss 2.7364\n",
      "iter 70: loss 2.7071\n",
      "iter 80: loss 2.6765\n",
      "iter 90: loss 2.6629\n",
      "iter 100: loss 2.6375\n",
      "iter 110: loss 2.6227\n",
      "iter 120: loss 2.5881\n",
      "iter 130: loss 2.5936\n",
      "iter 140: loss 2.5815\n",
      "iter 150: loss 2.5597\n",
      "iter 160: loss 2.5536\n",
      "iter 170: loss 2.5606\n",
      "iter 180: loss 2.5353\n",
      "iter 190: loss 2.5380\n",
      "iter 200: loss 2.5233\n",
      "iter 210: loss 2.5114\n",
      "iter 220: loss 2.5031\n",
      "iter 230: loss 2.4977\n",
      "iter 240: loss 2.4899\n",
      "iter 250: loss 2.4788\n",
      "iter 260: loss 2.4772\n",
      "iter 270: loss 2.4654\n",
      "iter 280: loss 2.4484\n",
      "iter 290: loss 2.4467\n",
      "iter 300: loss 2.4397\n",
      "iter 310: loss 2.4146\n",
      "iter 320: loss 2.3976\n",
      "iter 330: loss 2.3723\n",
      "iter 340: loss 2.3709\n",
      "iter 350: loss 2.3516\n",
      "iter 360: loss 2.3402\n",
      "iter 370: loss 2.3095\n",
      "iter 380: loss 2.3146\n",
      "iter 390: loss 2.3019\n",
      "iter 400: loss 2.2718\n",
      "iter 410: loss 2.2622\n",
      "iter 420: loss 2.2332\n",
      "iter 430: loss 2.2134\n",
      "iter 440: loss 2.1970\n",
      "iter 450: loss 2.1771\n",
      "iter 460: loss 2.1434\n",
      "iter 470: loss 2.1057\n",
      "iter 480: loss 2.1044\n",
      "iter 490: loss 2.0804\n",
      "iter 500: loss 2.0660\n",
      "step 500: train loss 2.0065, val loss 2.0076\n",
      "iter 510: loss 2.0334\n",
      "iter 520: loss 2.0141\n",
      "iter 530: loss 1.9426\n",
      "iter 540: loss 1.9642\n",
      "iter 550: loss 1.9181\n",
      "iter 560: loss 1.8950\n",
      "iter 570: loss 1.8801\n",
      "iter 580: loss 1.8445\n",
      "iter 590: loss 1.8388\n",
      "iter 600: loss 1.8192\n",
      "iter 610: loss 1.8433\n",
      "iter 620: loss 1.7660\n",
      "iter 630: loss 1.7784\n",
      "iter 640: loss 1.7222\n",
      "iter 650: loss 1.7165\n",
      "iter 660: loss 1.7270\n",
      "iter 670: loss 1.6901\n",
      "iter 680: loss 1.7073\n",
      "iter 690: loss 1.6481\n",
      "iter 700: loss 1.6618\n",
      "iter 710: loss 1.6318\n",
      "iter 720: loss 1.6242\n",
      "iter 730: loss 1.5855\n",
      "iter 740: loss 1.5959\n",
      "iter 750: loss 1.6173\n",
      "iter 760: loss 1.5865\n",
      "iter 770: loss 1.5588\n",
      "iter 780: loss 1.5496\n",
      "iter 790: loss 1.5305\n",
      "iter 800: loss 1.5241\n",
      "iter 810: loss 1.5214\n",
      "iter 820: loss 1.5039\n",
      "iter 830: loss 1.4797\n",
      "iter 840: loss 1.4982\n",
      "iter 850: loss 1.4989\n",
      "iter 860: loss 1.4724\n",
      "iter 870: loss 1.4772\n",
      "iter 880: loss 1.4441\n",
      "iter 890: loss 1.4665\n",
      "iter 900: loss 1.4625\n",
      "iter 910: loss 1.4519\n",
      "iter 920: loss 1.4354\n",
      "iter 930: loss 1.4479\n",
      "iter 940: loss 1.4297\n",
      "iter 950: loss 1.4243\n",
      "iter 960: loss 1.4128\n",
      "iter 970: loss 1.4006\n",
      "iter 980: loss 1.4232\n",
      "iter 990: loss 1.3973\n",
      "iter 1000: loss 1.4153\n",
      "step 1000: train loss 1.3439, val loss 1.3493\n",
      "iter 1010: loss 1.3968\n",
      "iter 1020: loss 1.3668\n",
      "iter 1030: loss 1.3985\n",
      "iter 1040: loss 1.3654\n",
      "iter 1050: loss 1.3759\n",
      "iter 1060: loss 1.3663\n",
      "iter 1070: loss 1.3350\n",
      "iter 1080: loss 1.3611\n",
      "iter 1090: loss 1.3706\n",
      "iter 1100: loss 1.3461\n",
      "iter 1110: loss 1.3628\n",
      "iter 1120: loss 1.3388\n",
      "iter 1130: loss 1.3432\n",
      "iter 1140: loss 1.3337\n",
      "iter 1150: loss 1.3203\n",
      "iter 1160: loss 1.3377\n",
      "iter 1170: loss 1.3366\n",
      "iter 1180: loss 1.3114\n",
      "iter 1190: loss 1.3134\n",
      "iter 1200: loss 1.2987\n",
      "iter 1210: loss 1.3105\n",
      "iter 1220: loss 1.3132\n",
      "iter 1230: loss 1.3005\n",
      "iter 1240: loss 1.2907\n",
      "iter 1250: loss 1.3086\n",
      "iter 1260: loss 1.2922\n",
      "iter 1270: loss 1.2988\n",
      "iter 1280: loss 1.3005\n",
      "iter 1290: loss 1.3018\n",
      "iter 1300: loss 1.2726\n",
      "iter 1310: loss 1.2692\n",
      "iter 1320: loss 1.3052\n",
      "iter 1330: loss 1.3086\n",
      "iter 1340: loss 1.2637\n",
      "iter 1350: loss 1.2919\n",
      "iter 1360: loss 1.2720\n",
      "iter 1370: loss 1.2825\n",
      "iter 1380: loss 1.2733\n",
      "iter 1390: loss 1.2562\n",
      "iter 1400: loss 1.2750\n",
      "iter 1410: loss 1.2656\n",
      "iter 1420: loss 1.2583\n",
      "iter 1430: loss 1.2503\n",
      "iter 1440: loss 1.2620\n",
      "iter 1450: loss 1.2434\n",
      "iter 1460: loss 1.2424\n",
      "iter 1470: loss 1.2535\n",
      "iter 1480: loss 1.2445\n",
      "iter 1490: loss 1.2407\n",
      "iter 1500: loss 1.2451\n",
      "step 1500: train loss 1.1966, val loss 1.2012\n",
      "iter 1510: loss 1.2515\n",
      "iter 1520: loss 1.2362\n",
      "iter 1530: loss 1.2395\n",
      "iter 1540: loss 1.2353\n",
      "iter 1550: loss 1.2289\n",
      "iter 1560: loss 1.2214\n",
      "iter 1570: loss 1.2400\n",
      "iter 1580: loss 1.2144\n",
      "iter 1590: loss 1.2368\n",
      "iter 1600: loss 1.2163\n",
      "iter 1610: loss 1.2304\n",
      "iter 1620: loss 1.2277\n",
      "iter 1630: loss 1.2210\n",
      "iter 1640: loss 1.2089\n",
      "iter 1650: loss 1.2089\n",
      "iter 1660: loss 1.2171\n",
      "iter 1670: loss 1.2080\n",
      "iter 1680: loss 1.2319\n",
      "iter 1690: loss 1.2232\n",
      "iter 1700: loss 1.2038\n",
      "iter 1710: loss 1.1781\n",
      "iter 1720: loss 1.2185\n",
      "iter 1730: loss 1.2125\n",
      "iter 1740: loss 1.1890\n",
      "iter 1750: loss 1.1865\n",
      "iter 1760: loss 1.1791\n",
      "iter 1770: loss 1.2239\n",
      "iter 1780: loss 1.1895\n",
      "iter 1790: loss 1.1910\n",
      "iter 1800: loss 1.1831\n",
      "iter 1810: loss 1.2082\n",
      "iter 1820: loss 1.1733\n",
      "iter 1830: loss 1.1885\n",
      "iter 1840: loss 1.2020\n",
      "iter 1850: loss 1.1945\n",
      "iter 1860: loss 1.1664\n",
      "iter 1870: loss 1.1951\n",
      "iter 1880: loss 1.1786\n",
      "iter 1890: loss 1.1795\n",
      "iter 1900: loss 1.1863\n",
      "iter 1910: loss 1.1826\n",
      "iter 1920: loss 1.1668\n",
      "iter 1930: loss 1.1987\n",
      "iter 1940: loss 1.1919\n",
      "iter 1950: loss 1.1510\n",
      "iter 1960: loss 1.1743\n",
      "iter 1970: loss 1.1733\n",
      "iter 1980: loss 1.1638\n",
      "iter 1990: loss 1.1693\n",
      "iter 2000: loss 1.1672\n",
      "step 2000: train loss 1.1134, val loss 1.1161\n",
      "iter 2010: loss 1.1407\n",
      "iter 2020: loss 1.1424\n",
      "iter 2030: loss 1.1421\n",
      "iter 2040: loss 1.1503\n",
      "iter 2050: loss 1.1611\n",
      "iter 2060: loss 1.1532\n",
      "iter 2070: loss 1.1679\n",
      "iter 2080: loss 1.1494\n",
      "iter 2090: loss 1.1325\n",
      "iter 2100: loss 1.1426\n",
      "iter 2110: loss 1.1484\n",
      "iter 2120: loss 1.1515\n",
      "iter 2130: loss 1.1719\n",
      "iter 2140: loss 1.1463\n",
      "iter 2150: loss 1.1434\n",
      "iter 2160: loss 1.1558\n",
      "iter 2170: loss 1.1548\n",
      "iter 2180: loss 1.1429\n",
      "iter 2190: loss 1.1304\n",
      "iter 2200: loss 1.1533\n",
      "iter 2210: loss 1.1307\n",
      "iter 2220: loss 1.1317\n",
      "iter 2230: loss 1.1294\n",
      "iter 2240: loss 1.1180\n",
      "iter 2250: loss 1.1218\n",
      "iter 2260: loss 1.1320\n",
      "iter 2270: loss 1.1339\n",
      "iter 2280: loss 1.1190\n",
      "iter 2290: loss 1.1237\n",
      "iter 2300: loss 1.1340\n",
      "iter 2310: loss 1.1186\n",
      "iter 2320: loss 1.1279\n",
      "iter 2330: loss 1.1025\n",
      "iter 2340: loss 1.1229\n",
      "iter 2350: loss 1.1085\n",
      "iter 2360: loss 1.1011\n",
      "iter 2370: loss 1.1138\n",
      "iter 2380: loss 1.1272\n",
      "iter 2390: loss 1.1048\n",
      "iter 2400: loss 1.1121\n",
      "iter 2410: loss 1.1240\n",
      "iter 2420: loss 1.1111\n",
      "iter 2430: loss 1.1311\n",
      "iter 2440: loss 1.1173\n",
      "iter 2450: loss 1.1164\n",
      "iter 2460: loss 1.1349\n",
      "iter 2470: loss 1.1166\n",
      "iter 2480: loss 1.0912\n",
      "iter 2490: loss 1.1160\n",
      "iter 2500: loss 1.1194\n",
      "step 2500: train loss 1.0466, val loss 1.0507\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 2510: loss 1.1092\n",
      "iter 2520: loss 1.0898\n",
      "iter 2530: loss 1.0939\n",
      "iter 2540: loss 1.1227\n",
      "iter 2550: loss 1.0999\n",
      "iter 2560: loss 1.0950\n",
      "iter 2570: loss 1.1189\n",
      "iter 2580: loss 1.0917\n",
      "iter 2590: loss 1.0963\n",
      "iter 2600: loss 1.1028\n",
      "iter 2610: loss 1.0830\n",
      "iter 2620: loss 1.0790\n",
      "iter 2630: loss 1.1010\n",
      "iter 2640: loss 1.0869\n",
      "iter 2650: loss 1.0925\n",
      "iter 2660: loss 1.0916\n",
      "iter 2670: loss 1.0750\n",
      "iter 2680: loss 1.0774\n",
      "iter 2690: loss 1.0844\n",
      "iter 2700: loss 1.0862\n",
      "iter 2710: loss 1.0815\n",
      "iter 2720: loss 1.0924\n",
      "iter 2730: loss 1.0857\n",
      "iter 2740: loss 1.0929\n",
      "iter 2750: loss 1.0653\n",
      "iter 2760: loss 1.0776\n",
      "iter 2770: loss 1.0653\n",
      "iter 2780: loss 1.0854\n",
      "iter 2790: loss 1.0842\n",
      "iter 2800: loss 1.0812\n",
      "iter 2810: loss 1.0972\n",
      "iter 2820: loss 1.0912\n",
      "iter 2830: loss 1.0818\n",
      "iter 2840: loss 1.0737\n",
      "iter 2850: loss 1.0996\n",
      "iter 2860: loss 1.0781\n",
      "iter 2870: loss 1.0727\n",
      "iter 2880: loss 1.0563\n",
      "iter 2890: loss 1.0579\n",
      "iter 2900: loss 1.0585\n",
      "iter 2910: loss 1.0630\n",
      "iter 2920: loss 1.0451\n",
      "iter 2930: loss 1.0665\n",
      "iter 2940: loss 1.0573\n",
      "iter 2950: loss 1.0636\n",
      "iter 2960: loss 1.0585\n",
      "iter 2970: loss 1.0577\n",
      "iter 2980: loss 1.0743\n",
      "iter 2990: loss 1.0428\n",
      "iter 3000: loss 1.0462\n",
      "step 3000: train loss 0.9960, val loss 0.9931\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3010: loss 1.0572\n",
      "iter 3020: loss 1.0571\n",
      "iter 3030: loss 1.0525\n",
      "iter 3040: loss 1.0465\n",
      "iter 3050: loss 1.0504\n",
      "iter 3060: loss 1.0470\n",
      "iter 3070: loss 1.0609\n",
      "iter 3080: loss 1.0758\n",
      "iter 3090: loss 1.0480\n",
      "iter 3100: loss 1.0523\n",
      "iter 3110: loss 1.0569\n",
      "iter 3120: loss 1.0393\n",
      "iter 3130: loss 1.0235\n",
      "iter 3140: loss 1.0418\n",
      "iter 3150: loss 1.0495\n",
      "iter 3160: loss 1.0332\n",
      "iter 3170: loss 1.0548\n",
      "iter 3180: loss 1.0244\n",
      "iter 3190: loss 1.0503\n",
      "iter 3200: loss 1.0364\n",
      "iter 3210: loss 1.0260\n",
      "iter 3220: loss 1.0606\n",
      "iter 3230: loss 1.0328\n",
      "iter 3240: loss 1.0374\n",
      "iter 3250: loss 1.0416\n",
      "iter 3260: loss 1.0328\n",
      "iter 3270: loss 1.0340\n",
      "iter 3280: loss 1.0261\n",
      "iter 3290: loss 1.0203\n",
      "iter 3300: loss 1.0238\n",
      "iter 3310: loss 1.0201\n",
      "iter 3320: loss 1.0183\n",
      "iter 3330: loss 1.0228\n",
      "iter 3340: loss 1.0309\n",
      "iter 3350: loss 1.0115\n",
      "iter 3360: loss 1.0334\n",
      "iter 3370: loss 1.0271\n",
      "iter 3380: loss 1.0245\n",
      "iter 3390: loss 1.0341\n",
      "iter 3400: loss 1.0050\n",
      "iter 3410: loss 1.0416\n",
      "iter 3420: loss 1.0345\n",
      "iter 3430: loss 1.0404\n",
      "iter 3440: loss 1.0285\n",
      "iter 3450: loss 1.0295\n",
      "iter 3460: loss 1.0158\n",
      "iter 3470: loss 1.0150\n",
      "iter 3480: loss 1.0191\n",
      "iter 3490: loss 1.0187\n",
      "iter 3500: loss 1.0062\n",
      "step 3500: train loss 0.9394, val loss 0.9414\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 3510: loss 1.0343\n",
      "iter 3520: loss 1.0033\n",
      "iter 3530: loss 1.0099\n",
      "iter 3540: loss 0.9936\n",
      "iter 3550: loss 0.9979\n",
      "iter 3560: loss 1.0134\n",
      "iter 3570: loss 1.0156\n",
      "iter 3580: loss 1.0189\n",
      "iter 3590: loss 1.0058\n",
      "iter 3600: loss 0.9981\n",
      "iter 3610: loss 0.9983\n",
      "iter 3620: loss 1.0257\n",
      "iter 3630: loss 1.0057\n",
      "iter 3640: loss 0.9785\n",
      "iter 3650: loss 1.0084\n",
      "iter 3660: loss 1.0117\n",
      "iter 3670: loss 1.0163\n",
      "iter 3680: loss 1.0115\n",
      "iter 3690: loss 0.9940\n",
      "iter 3700: loss 0.9832\n",
      "iter 3710: loss 0.9822\n",
      "iter 3720: loss 1.0181\n",
      "iter 3730: loss 1.0092\n",
      "iter 3740: loss 0.9858\n",
      "iter 3750: loss 1.0147\n",
      "iter 3760: loss 0.9884\n",
      "iter 3770: loss 0.9968\n",
      "iter 3780: loss 0.9864\n",
      "iter 3790: loss 0.9937\n",
      "iter 3800: loss 0.9832\n",
      "iter 3810: loss 0.9812\n",
      "iter 3820: loss 1.0144\n",
      "iter 3830: loss 1.0015\n",
      "iter 3840: loss 0.9969\n",
      "iter 3850: loss 1.0088\n",
      "iter 3860: loss 0.9758\n",
      "iter 3870: loss 0.9883\n",
      "iter 3880: loss 0.9802\n",
      "iter 3890: loss 0.9952\n",
      "iter 3900: loss 0.9988\n",
      "iter 3910: loss 0.9977\n",
      "iter 3920: loss 1.0048\n",
      "iter 3930: loss 0.9829\n",
      "iter 3940: loss 0.9691\n",
      "iter 3950: loss 0.9777\n",
      "iter 3960: loss 0.9975\n",
      "iter 3970: loss 0.9744\n",
      "iter 3980: loss 0.9632\n",
      "iter 3990: loss 0.9914\n",
      "iter 4000: loss 0.9773\n",
      "step 4000: train loss 0.8924, val loss 0.8979\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4010: loss 0.9913\n",
      "iter 4020: loss 0.9661\n",
      "iter 4030: loss 0.9777\n",
      "iter 4040: loss 0.9693\n",
      "iter 4050: loss 0.9697\n",
      "iter 4060: loss 0.9622\n",
      "iter 4070: loss 0.9793\n",
      "iter 4080: loss 0.9763\n",
      "iter 4090: loss 0.9712\n",
      "iter 4100: loss 0.9785\n",
      "iter 4110: loss 0.9725\n",
      "iter 4120: loss 0.9751\n",
      "iter 4130: loss 0.9610\n",
      "iter 4140: loss 0.9694\n",
      "iter 4150: loss 0.9740\n",
      "iter 4160: loss 0.9798\n",
      "iter 4170: loss 0.9710\n",
      "iter 4180: loss 0.9544\n",
      "iter 4190: loss 0.9621\n",
      "iter 4200: loss 0.9519\n",
      "iter 4210: loss 0.9710\n",
      "iter 4220: loss 0.9696\n",
      "iter 4230: loss 0.9501\n",
      "iter 4240: loss 0.9745\n",
      "iter 4250: loss 0.9642\n",
      "iter 4260: loss 0.9688\n",
      "iter 4270: loss 0.9602\n",
      "iter 4280: loss 0.9630\n",
      "iter 4290: loss 0.9608\n",
      "iter 4300: loss 0.9580\n",
      "iter 4310: loss 0.9610\n",
      "iter 4320: loss 0.9398\n",
      "iter 4330: loss 0.9442\n",
      "iter 4340: loss 0.9558\n",
      "iter 4350: loss 0.9509\n",
      "iter 4360: loss 0.9506\n",
      "iter 4370: loss 0.9407\n",
      "iter 4380: loss 0.9597\n",
      "iter 4390: loss 0.9725\n",
      "iter 4400: loss 0.9411\n",
      "iter 4410: loss 0.9551\n",
      "iter 4420: loss 0.9347\n",
      "iter 4430: loss 0.9682\n",
      "iter 4440: loss 0.9535\n",
      "iter 4450: loss 0.9473\n",
      "iter 4460: loss 0.9479\n",
      "iter 4470: loss 0.9693\n",
      "iter 4480: loss 0.9533\n",
      "iter 4490: loss 0.9454\n",
      "iter 4500: loss 0.9435\n",
      "step 4500: train loss 0.8526, val loss 0.8540\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "iter 4510: loss 0.9391\n",
      "iter 4520: loss 0.9397\n",
      "iter 4530: loss 0.9430\n",
      "iter 4540: loss 0.9458\n",
      "iter 4550: loss 0.9439\n",
      "iter 4560: loss 0.9548\n",
      "iter 4570: loss 0.9335\n",
      "iter 4580: loss 0.9305\n",
      "iter 4590: loss 0.9374\n",
      "iter 4600: loss 0.9348\n",
      "iter 4610: loss 0.9171\n",
      "iter 4620: loss 0.9374\n",
      "iter 4630: loss 0.9386\n",
      "iter 4640: loss 0.9377\n",
      "iter 4650: loss 0.9399\n",
      "iter 4660: loss 0.9327\n",
      "iter 4670: loss 0.9319\n",
      "iter 4680: loss 0.9292\n",
      "iter 4690: loss 0.9107\n",
      "iter 4700: loss 0.9366\n",
      "iter 4710: loss 0.9396\n",
      "iter 4720: loss 0.9272\n",
      "iter 4730: loss 0.9219\n",
      "iter 4740: loss 0.9389\n",
      "iter 4750: loss 0.9200\n",
      "iter 4760: loss 0.9240\n",
      "iter 4770: loss 0.9413\n",
      "iter 4780: loss 0.9287\n",
      "iter 4790: loss 0.9420\n",
      "iter 4800: loss 0.9321\n",
      "iter 4810: loss 0.9347\n",
      "iter 4820: loss 0.9273\n",
      "iter 4830: loss 0.9275\n",
      "iter 4840: loss 0.9193\n",
      "iter 4850: loss 0.9284\n",
      "iter 4860: loss 0.9231\n",
      "iter 4870: loss 0.9212\n",
      "iter 4880: loss 0.9380\n",
      "iter 4890: loss 0.9395\n",
      "iter 4900: loss 0.9112\n",
      "iter 4910: loss 0.9039\n",
      "iter 4920: loss 0.9177\n",
      "iter 4930: loss 0.9135\n",
      "iter 4940: loss 0.9104\n",
      "iter 4950: loss 0.9256\n",
      "iter 4960: loss 0.9148\n",
      "iter 4970: loss 0.9333\n",
      "iter 4980: loss 0.9210\n",
      "iter 4990: loss 0.9033\n",
      "iter 5000: loss 0.9432\n",
      "step 5000: train loss 0.8158, val loss 0.8169\n",
      "Saving checkpoint to MovieGPT -------------------\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.0918\n",
      "step 0: train loss 4.3125, val loss 4.3125\n",
      "iter 10: loss 3.1883\n",
      "iter 20: loss 3.1215\n",
      "iter 30: loss 3.1135\n",
      "iter 40: loss 3.1149\n",
      "iter 50: loss 3.1224\n",
      "iter 60: loss 3.1150\n",
      "iter 70: loss 3.1151\n",
      "iter 80: loss 3.1139\n",
      "iter 90: loss 3.1177\n",
      "iter 100: loss 3.1899\n",
      "iter 110: loss 3.1117\n",
      "iter 120: loss 3.0844\n",
      "iter 130: loss 2.9981\n",
      "iter 140: loss 2.9499\n",
      "iter 150: loss 2.9256\n",
      "iter 160: loss 2.9117\n",
      "iter 170: loss 2.8888\n",
      "iter 180: loss 2.8699\n",
      "iter 190: loss 2.8686\n",
      "iter 200: loss 2.8472\n",
      "iter 210: loss 2.7967\n",
      "iter 220: loss 2.7000\n",
      "iter 230: loss 2.6653\n",
      "iter 240: loss 2.6428\n",
      "iter 250: loss 2.6280\n",
      "iter 260: loss 2.6072\n",
      "iter 270: loss 2.5969\n",
      "iter 280: loss 2.5883\n",
      "iter 290: loss 2.5773\n",
      "iter 300: loss 2.5722\n",
      "iter 310: loss 2.5623\n",
      "iter 320: loss 2.5376\n",
      "iter 330: loss 2.5417\n",
      "iter 340: loss 2.5381\n",
      "iter 350: loss 2.5234\n",
      "iter 360: loss 2.5318\n",
      "iter 370: loss 2.5150\n",
      "iter 380: loss 2.5026\n",
      "iter 390: loss 2.5016\n",
      "iter 400: loss 2.4936\n",
      "iter 410: loss 2.4897\n",
      "iter 420: loss 2.4830\n",
      "iter 430: loss 2.4888\n",
      "iter 440: loss 2.4934\n",
      "iter 450: loss 2.4815\n",
      "iter 460: loss 2.4652\n",
      "iter 470: loss 2.4607\n",
      "iter 480: loss 2.4525\n",
      "iter 490: loss 2.4678\n",
      "iter 500: loss 2.4363\n",
      "step 500: train loss 2.4304, val loss 2.4319\n",
      "iter 510: loss 2.4130\n",
      "iter 520: loss 2.3604\n",
      "iter 530: loss 2.3811\n",
      "iter 540: loss 2.3135\n",
      "iter 550: loss 2.3065\n",
      "iter 560: loss 2.2557\n",
      "iter 570: loss 2.2348\n",
      "iter 580: loss 2.2034\n",
      "iter 590: loss 2.1805\n",
      "iter 600: loss 2.1727\n",
      "iter 610: loss 2.1293\n",
      "iter 620: loss 2.0931\n",
      "iter 630: loss 2.0754\n",
      "iter 640: loss 2.0522\n",
      "iter 650: loss 2.0633\n",
      "iter 660: loss 2.0501\n",
      "iter 670: loss 2.0209\n",
      "iter 680: loss 1.9908\n",
      "iter 690: loss 1.9533\n",
      "iter 700: loss 1.9106\n",
      "iter 710: loss 1.9090\n",
      "iter 720: loss 1.8932\n",
      "iter 730: loss 1.8837\n",
      "iter 740: loss 1.8602\n",
      "iter 750: loss 1.8225\n",
      "iter 760: loss 1.7809\n",
      "iter 770: loss 1.8195\n",
      "iter 780: loss 1.8570\n",
      "iter 790: loss 1.9561\n",
      "iter 800: loss 1.8234\n",
      "iter 810: loss 1.8059\n",
      "iter 820: loss 1.7420\n",
      "iter 830: loss 1.7236\n",
      "iter 840: loss 1.7160\n",
      "iter 850: loss 1.7284\n",
      "iter 860: loss 1.6711\n",
      "iter 870: loss 1.6793\n",
      "iter 880: loss 1.6832\n",
      "iter 890: loss 1.6705\n",
      "iter 900: loss 1.6580\n",
      "iter 910: loss 1.6343\n",
      "iter 920: loss 1.6217\n",
      "iter 930: loss 1.5842\n",
      "iter 940: loss 1.6142\n",
      "iter 950: loss 1.5938\n",
      "iter 960: loss 1.5850\n",
      "iter 970: loss 1.5743\n",
      "iter 980: loss 1.5794\n",
      "iter 990: loss 1.5533\n",
      "iter 1000: loss 1.5599\n",
      "step 1000: train loss 1.5172, val loss 1.5221\n",
      "iter 1010: loss 1.5512\n",
      "iter 1020: loss 1.5750\n",
      "iter 1030: loss 1.5325\n",
      "iter 1040: loss 1.5357\n",
      "iter 1050: loss 1.5536\n",
      "iter 1060: loss 1.5156\n",
      "iter 1070: loss 1.5419\n",
      "iter 1080: loss 1.5436\n",
      "iter 1090: loss 1.5024\n",
      "iter 1100: loss 1.4839\n",
      "iter 1110: loss 1.5061\n",
      "iter 1120: loss 1.5139\n",
      "iter 1130: loss 1.5084\n",
      "iter 1140: loss 1.4841\n",
      "iter 1150: loss 1.5057\n",
      "iter 1160: loss 1.4992\n",
      "iter 1170: loss 1.4458\n",
      "iter 1180: loss 1.4553\n",
      "iter 1190: loss 1.4519\n",
      "iter 1200: loss 1.4903\n",
      "iter 1210: loss 1.4727\n",
      "iter 1220: loss 1.4476\n",
      "iter 1230: loss 1.4436\n",
      "iter 1240: loss 1.4460\n",
      "iter 1250: loss 1.4106\n",
      "iter 1260: loss 1.4221\n",
      "iter 1270: loss 1.4344\n",
      "iter 1280: loss 1.4075\n",
      "iter 1290: loss 1.4193\n",
      "iter 1300: loss 1.4132\n",
      "iter 1310: loss 1.4067\n",
      "iter 1320: loss 1.4285\n",
      "iter 1330: loss 1.4193\n",
      "iter 1340: loss 1.4080\n",
      "iter 1350: loss 1.3890\n",
      "iter 1360: loss 1.4025\n",
      "iter 1370: loss 1.4233\n",
      "iter 1380: loss 1.3764\n",
      "iter 1390: loss 1.3916\n",
      "iter 1400: loss 1.3777\n",
      "iter 1410: loss 1.3698\n",
      "iter 1420: loss 1.3643\n",
      "iter 1430: loss 1.3949\n",
      "iter 1440: loss 1.3770\n",
      "iter 1450: loss 1.3921\n",
      "iter 1460: loss 1.3558\n",
      "iter 1470: loss 1.3619\n",
      "iter 1480: loss 1.3716\n",
      "iter 1490: loss 1.3828\n",
      "iter 1500: loss 1.3712\n",
      "step 1500: train loss 1.3208, val loss 1.3239\n",
      "iter 1510: loss 1.3699\n",
      "iter 1520: loss 1.3686\n",
      "iter 1530: loss 1.3483\n",
      "iter 1540: loss 1.3515\n",
      "iter 1550: loss 1.3397\n",
      "iter 1560: loss 1.3438\n",
      "iter 1570: loss 1.3437\n",
      "iter 1580: loss 1.3334\n",
      "iter 1590: loss 1.3377\n",
      "iter 1600: loss 1.3586\n",
      "iter 1610: loss 1.3211\n",
      "iter 1620: loss 1.3361\n",
      "iter 1630: loss 1.3407\n",
      "iter 1640: loss 1.3405\n",
      "iter 1650: loss 1.3224\n",
      "iter 1660: loss 1.3444\n",
      "iter 1670: loss 1.3180\n",
      "iter 1680: loss 1.3220\n",
      "iter 1690: loss 1.3333\n",
      "iter 1700: loss 1.3418\n",
      "iter 1710: loss 1.3136\n",
      "iter 1720: loss 1.3005\n",
      "iter 1730: loss 1.3134\n",
      "iter 1740: loss 1.3212\n",
      "iter 1750: loss 1.3103\n",
      "iter 1760: loss 1.3130\n",
      "iter 1770: loss 1.3125\n",
      "iter 1780: loss 1.3154\n",
      "iter 1790: loss 1.2996\n",
      "iter 1800: loss 1.2979\n",
      "iter 1810: loss 1.3063\n",
      "iter 1820: loss 1.3059\n",
      "iter 1830: loss 1.3001\n",
      "iter 1840: loss 1.2761\n",
      "iter 1850: loss 1.2913\n",
      "iter 1860: loss 1.2869\n",
      "iter 1870: loss 1.2842\n",
      "iter 1880: loss 1.2889\n",
      "iter 1890: loss 1.2956\n",
      "iter 1900: loss 1.2950\n",
      "iter 1910: loss 1.2901\n",
      "iter 1920: loss 1.2829\n",
      "iter 1930: loss 1.2911\n",
      "iter 1940: loss 1.2675\n",
      "iter 1950: loss 1.2696\n",
      "iter 1960: loss 1.2833\n",
      "iter 1970: loss 1.2800\n",
      "iter 1980: loss 1.2682\n",
      "iter 1990: loss 1.2798\n",
      "iter 2000: loss 1.2602\n",
      "step 2000: train loss 1.2281, val loss 1.2306\n",
      "iter 2010: loss 1.2615\n",
      "iter 2020: loss 1.2665\n",
      "iter 2030: loss 1.2763\n",
      "iter 2040: loss 1.2569\n",
      "iter 2050: loss 1.2724\n",
      "iter 2060: loss 1.2768\n",
      "iter 2070: loss 1.2603\n",
      "iter 2080: loss 1.2560\n",
      "iter 2090: loss 1.2551\n",
      "iter 2100: loss 1.2266\n",
      "iter 2110: loss 1.2391\n",
      "iter 2120: loss 1.2904\n",
      "iter 2130: loss 1.2541\n",
      "iter 2140: loss 1.2625\n",
      "iter 2150: loss 1.2402\n",
      "iter 2160: loss 1.2459\n",
      "iter 2170: loss 1.2548\n",
      "iter 2180: loss 1.2472\n",
      "iter 2190: loss 1.2607\n",
      "iter 2200: loss 1.2495\n",
      "iter 2210: loss 1.2393\n",
      "iter 2220: loss 1.2391\n",
      "iter 2230: loss 1.2525\n",
      "iter 2240: loss 1.2388\n",
      "iter 2250: loss 1.2363\n",
      "iter 2260: loss 1.2502\n",
      "iter 2270: loss 1.2499\n",
      "iter 2280: loss 1.2367\n",
      "iter 2290: loss 1.2391\n",
      "iter 2300: loss 1.2395\n",
      "iter 2310: loss 1.2569\n",
      "iter 2320: loss 1.2292\n",
      "iter 2330: loss 1.2287\n",
      "iter 2340: loss 1.2234\n",
      "iter 2350: loss 1.2114\n",
      "iter 2360: loss 1.2258\n",
      "iter 2370: loss 1.2398\n",
      "iter 2380: loss 1.2352\n",
      "iter 2390: loss 1.2252\n",
      "iter 2400: loss 1.2423\n",
      "iter 2410: loss 1.2095\n",
      "iter 2420: loss 1.2096\n",
      "iter 2430: loss 1.2011\n",
      "iter 2440: loss 1.2223\n",
      "iter 2450: loss 1.2311\n",
      "iter 2460: loss 1.2271\n",
      "iter 2470: loss 1.2260\n",
      "iter 2480: loss 1.2097\n",
      "iter 2490: loss 1.2085\n",
      "iter 2500: loss 1.2197\n",
      "step 2500: train loss 1.1669, val loss 1.1686\n",
      "iter 2510: loss 1.2231\n",
      "iter 2520: loss 1.2199\n",
      "iter 2530: loss 1.1998\n",
      "iter 2540: loss 1.2147\n",
      "iter 2550: loss 1.2125\n",
      "iter 2560: loss 1.2161\n",
      "iter 2570: loss 1.2010\n",
      "iter 2580: loss 1.1991\n",
      "iter 2590: loss 1.2160\n",
      "iter 2600: loss 1.1920\n",
      "iter 2610: loss 1.2079\n",
      "iter 2620: loss 1.2051\n",
      "iter 2630: loss 1.2073\n",
      "iter 2640: loss 1.2072\n",
      "iter 2650: loss 1.1981\n",
      "iter 2660: loss 1.1976\n",
      "iter 2670: loss 1.1881\n",
      "iter 2680: loss 1.2080\n",
      "iter 2690: loss 1.1776\n",
      "iter 2700: loss 1.1860\n",
      "iter 2710: loss 1.1773\n",
      "iter 2720: loss 1.2053\n",
      "iter 2730: loss 1.1741\n",
      "iter 2740: loss 1.1878\n",
      "iter 2750: loss 1.1809\n",
      "iter 2760: loss 1.1788\n",
      "iter 2770: loss 1.1920\n",
      "iter 2780: loss 1.1792\n",
      "iter 2790: loss 1.2014\n",
      "iter 2800: loss 1.1932\n",
      "iter 2810: loss 1.1856\n",
      "iter 2820: loss 1.1874\n",
      "iter 2830: loss 1.1847\n",
      "iter 2840: loss 1.1915\n",
      "iter 2850: loss 1.1690\n",
      "iter 2860: loss 1.1800\n",
      "iter 2870: loss 1.1799\n",
      "iter 2880: loss 1.1891\n",
      "iter 2890: loss 1.1911\n",
      "iter 2900: loss 1.1611\n",
      "iter 2910: loss 1.1846\n",
      "iter 2920: loss 1.1579\n",
      "iter 2930: loss 1.1657\n",
      "iter 2940: loss 1.1837\n",
      "iter 2950: loss 1.1601\n",
      "iter 2960: loss 1.1811\n",
      "iter 2970: loss 1.1699\n",
      "iter 2980: loss 1.1668\n",
      "iter 2990: loss 1.1743\n",
      "iter 3000: loss 1.1665\n",
      "step 3000: train loss 1.1205, val loss 1.1224\n",
      "iter 3010: loss 1.1606\n",
      "iter 3020: loss 1.1765\n",
      "iter 3030: loss 1.1506\n",
      "iter 3040: loss 1.1682\n",
      "iter 3050: loss 1.1861\n",
      "iter 3060: loss 1.1709\n",
      "iter 3070: loss 1.1706\n",
      "iter 3080: loss 1.1662\n",
      "iter 3090: loss 1.1636\n",
      "iter 3100: loss 1.1779\n",
      "iter 3110: loss 1.1544\n",
      "iter 3120: loss 1.1452\n",
      "iter 3130: loss 1.1528\n",
      "iter 3140: loss 1.1711\n",
      "iter 3150: loss 1.1599\n",
      "iter 3160: loss 1.1319\n",
      "iter 3170: loss 1.1563\n",
      "iter 3180: loss 1.1441\n",
      "iter 3190: loss 1.1731\n",
      "iter 3200: loss 1.1620\n",
      "iter 3210: loss 1.1520\n",
      "iter 3220: loss 1.1585\n",
      "iter 3230: loss 1.1372\n",
      "iter 3240: loss 1.1451\n",
      "iter 3250: loss 1.1685\n",
      "iter 3260: loss 1.1579\n",
      "iter 3270: loss 1.1405\n",
      "iter 3280: loss 1.1391\n",
      "iter 3290: loss 1.1308\n",
      "iter 3300: loss 1.1392\n",
      "iter 3310: loss 1.1521\n",
      "iter 3320: loss 1.1369\n",
      "iter 3330: loss 1.1179\n",
      "iter 3340: loss 1.1452\n",
      "iter 3350: loss 1.1549\n",
      "iter 3360: loss 1.1485\n",
      "iter 3370: loss 1.1303\n",
      "iter 3380: loss 1.1443\n",
      "iter 3390: loss 1.1457\n",
      "iter 3400: loss 1.1504\n",
      "iter 3410: loss 1.1347\n",
      "iter 3420: loss 1.1353\n",
      "iter 3430: loss 1.1343\n",
      "iter 3440: loss 1.1218\n",
      "iter 3450: loss 1.1350\n",
      "iter 3460: loss 1.1341\n",
      "iter 3470: loss 1.1382\n",
      "iter 3480: loss 1.1079\n",
      "iter 3490: loss 1.1085\n",
      "iter 3500: loss 1.1249\n",
      "step 3500: train loss 1.0786, val loss 1.0805\n",
      "iter 3510: loss 1.1512\n",
      "iter 3520: loss 1.1345\n",
      "iter 3530: loss 1.1431\n",
      "iter 3540: loss 1.1343\n",
      "iter 3550: loss 1.1438\n",
      "iter 3560: loss 1.1321\n",
      "iter 3570: loss 1.1431\n",
      "iter 3580: loss 1.1337\n",
      "iter 3590: loss 1.1452\n",
      "iter 3600: loss 1.1180\n",
      "iter 3610: loss 1.1210\n",
      "iter 3620: loss 1.1182\n",
      "iter 3630: loss 1.1072\n",
      "iter 3640: loss 1.1406\n",
      "iter 3650: loss 1.1353\n",
      "iter 3660: loss 1.1248\n",
      "iter 3670: loss 1.1204\n",
      "iter 3680: loss 1.1148\n",
      "iter 3690: loss 1.1305\n",
      "iter 3700: loss 1.1166\n",
      "iter 3710: loss 1.1041\n",
      "iter 3720: loss 1.1142\n",
      "iter 3730: loss 1.1312\n",
      "iter 3740: loss 1.1451\n",
      "iter 3750: loss 1.1092\n",
      "iter 3760: loss 1.1053\n",
      "iter 3770: loss 1.1117\n",
      "iter 3780: loss 1.1026\n",
      "iter 3790: loss 1.1303\n",
      "iter 3800: loss 1.1030\n",
      "iter 3810: loss 1.1340\n",
      "iter 3820: loss 1.1084\n",
      "iter 3830: loss 1.1288\n",
      "iter 3840: loss 1.1093\n",
      "iter 3850: loss 1.1231\n",
      "iter 3860: loss 1.1023\n",
      "iter 3870: loss 1.1083\n",
      "iter 3880: loss 1.1132\n",
      "iter 3890: loss 1.1145\n",
      "iter 3900: loss 1.1193\n",
      "iter 3910: loss 1.1166\n",
      "iter 3920: loss 1.1038\n",
      "iter 3930: loss 1.1059\n",
      "iter 3940: loss 1.1140\n",
      "iter 3950: loss 1.1111\n",
      "iter 3960: loss 1.1056\n",
      "iter 3970: loss 1.1054\n",
      "iter 3980: loss 1.0981\n",
      "iter 3990: loss 1.0925\n",
      "iter 4000: loss 1.1044\n",
      "step 4000: train loss 1.0449, val loss 1.0453\n",
      "iter 4010: loss 1.1054\n",
      "iter 4020: loss 1.1206\n",
      "iter 4030: loss 1.0999\n",
      "iter 4040: loss 1.1091\n",
      "iter 4050: loss 1.0904\n",
      "iter 4060: loss 1.0930\n",
      "iter 4070: loss 1.0926\n",
      "iter 4080: loss 1.0960\n",
      "iter 4090: loss 1.1052\n",
      "iter 4100: loss 1.0786\n",
      "iter 4110: loss 1.1039\n",
      "iter 4120: loss 1.0987\n",
      "iter 4130: loss 1.1065\n",
      "iter 4140: loss 1.0901\n",
      "iter 4150: loss 1.1097\n",
      "iter 4160: loss 1.0936\n",
      "iter 4170: loss 1.0893\n",
      "iter 4180: loss 1.0978\n",
      "iter 4190: loss 1.0919\n",
      "iter 4200: loss 1.0984\n",
      "iter 4210: loss 1.0925\n",
      "iter 4220: loss 1.0928\n",
      "iter 4230: loss 1.1009\n",
      "iter 4240: loss 1.0735\n",
      "iter 4250: loss 1.0914\n",
      "iter 4260: loss 1.0864\n",
      "iter 4270: loss 1.0888\n",
      "iter 4280: loss 1.0820\n",
      "iter 4290: loss 1.1032\n",
      "iter 4300: loss 1.0779\n",
      "iter 4310: loss 1.0896\n",
      "iter 4320: loss 1.0986\n",
      "iter 4330: loss 1.1059\n",
      "iter 4340: loss 1.0830\n",
      "iter 4350: loss 1.0945\n",
      "iter 4360: loss 1.0901\n",
      "iter 4370: loss 1.0752\n",
      "iter 4380: loss 1.0793\n",
      "iter 4390: loss 1.0724\n",
      "iter 4400: loss 1.0960\n",
      "iter 4410: loss 1.0916\n",
      "iter 4420: loss 1.1086\n",
      "iter 4430: loss 1.0810\n",
      "iter 4440: loss 1.0854\n",
      "iter 4450: loss 1.0854\n",
      "iter 4460: loss 1.0767\n",
      "iter 4470: loss 1.0664\n",
      "iter 4480: loss 1.0700\n",
      "iter 4490: loss 1.0828\n",
      "iter 4500: loss 1.0810\n",
      "step 4500: train loss 1.0109, val loss 1.0117\n",
      "iter 4510: loss 1.0697\n",
      "iter 4520: loss 1.0611\n",
      "iter 4530: loss 1.0675\n",
      "iter 4540: loss 1.0622\n",
      "iter 4550: loss 1.0610\n",
      "iter 4560: loss 1.0829\n",
      "iter 4570: loss 1.0761\n",
      "iter 4580: loss 1.0768\n",
      "iter 4590: loss 1.0641\n",
      "iter 4600: loss 1.0715\n",
      "iter 4610: loss 1.0758\n",
      "iter 4620: loss 1.0944\n",
      "iter 4630: loss 1.0760\n",
      "iter 4640: loss 1.0623\n",
      "iter 4650: loss 1.0788\n",
      "iter 4660: loss 1.0745\n",
      "iter 4670: loss 1.0694\n",
      "iter 4680: loss 1.0753\n",
      "iter 4690: loss 1.0704\n",
      "iter 4700: loss 1.0825\n",
      "iter 4710: loss 1.0723\n",
      "iter 4720: loss 1.0484\n",
      "iter 4730: loss 1.0593\n",
      "iter 4740: loss 1.0590\n",
      "iter 4750: loss 1.0490\n",
      "iter 4760: loss 1.0598\n",
      "iter 4770: loss 1.0708\n",
      "iter 4780: loss 1.0640\n",
      "iter 4790: loss 1.0616\n",
      "iter 4800: loss 1.0763\n",
      "iter 4810: loss 1.0823\n",
      "iter 4820: loss 1.0607\n",
      "iter 4830: loss 1.0683\n",
      "iter 4840: loss 1.0509\n",
      "iter 4850: loss 1.0740\n",
      "iter 4860: loss 1.0643\n",
      "iter 4870: loss 1.0427\n",
      "iter 4880: loss 1.0508\n",
      "iter 4890: loss 1.0577\n",
      "iter 4900: loss 1.0651\n",
      "iter 4910: loss 1.0648\n",
      "iter 4920: loss 1.0653\n",
      "iter 4930: loss 1.0532\n",
      "iter 4940: loss 1.0772\n",
      "iter 4950: loss 1.0486\n",
      "iter 4960: loss 1.0499\n",
      "iter 4970: loss 1.0386\n",
      "iter 4980: loss 1.0675\n",
      "iter 4990: loss 1.0676\n",
      "iter 5000: loss 1.0454\n",
      "step 5000: train loss 0.9862, val loss 0.9850\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.1077\n",
      "step 0: train loss 4.4372, val loss 4.4365\n",
      "iter 10: loss 3.7127\n",
      "iter 20: loss 3.2258\n",
      "iter 30: loss 2.9376\n",
      "iter 40: loss 2.7890\n",
      "iter 50: loss 2.6988\n",
      "iter 60: loss 2.6550\n",
      "iter 70: loss 2.6176\n",
      "iter 80: loss 2.5982\n",
      "iter 90: loss 2.5813\n",
      "iter 100: loss 2.5520\n",
      "iter 110: loss 2.5447\n",
      "iter 120: loss 2.5498\n",
      "iter 130: loss 2.5384\n",
      "iter 140: loss 2.5237\n",
      "iter 150: loss 2.5222\n",
      "iter 160: loss 2.5130\n",
      "iter 170: loss 2.4958\n",
      "iter 180: loss 2.5062\n",
      "iter 190: loss 2.5020\n",
      "iter 200: loss 2.4852\n",
      "iter 210: loss 2.4693\n",
      "iter 220: loss 2.4650\n",
      "iter 230: loss 2.4417\n",
      "iter 240: loss 2.4452\n",
      "iter 250: loss 2.4446\n",
      "iter 260: loss 2.4243\n",
      "iter 270: loss 2.4260\n",
      "iter 280: loss 2.4171\n",
      "iter 290: loss 2.4075\n",
      "iter 300: loss 2.3948\n",
      "iter 310: loss 2.3775\n",
      "iter 320: loss 2.3650\n",
      "iter 330: loss 2.3376\n",
      "iter 340: loss 2.3244\n",
      "iter 350: loss 2.3122\n",
      "iter 360: loss 2.3106\n",
      "iter 370: loss 2.2651\n",
      "iter 380: loss 2.2607\n",
      "iter 390: loss 2.2444\n",
      "iter 400: loss 2.2365\n",
      "iter 410: loss 2.2042\n",
      "iter 420: loss 2.1935\n",
      "iter 430: loss 2.2008\n",
      "iter 440: loss 2.1718\n",
      "iter 450: loss 2.1673\n",
      "iter 460: loss 2.1395\n",
      "iter 470: loss 2.1449\n",
      "iter 480: loss 2.1076\n",
      "iter 490: loss 2.1059\n",
      "iter 500: loss 2.0864\n",
      "step 500: train loss 2.0197, val loss 2.0223\n",
      "iter 510: loss 2.0735\n",
      "iter 520: loss 2.0657\n",
      "iter 530: loss 2.0382\n",
      "iter 540: loss 2.0495\n",
      "iter 550: loss 2.0319\n",
      "iter 560: loss 2.0239\n",
      "iter 570: loss 2.0180\n",
      "iter 580: loss 2.0100\n",
      "iter 590: loss 1.9714\n",
      "iter 600: loss 1.9732\n",
      "iter 610: loss 1.9599\n",
      "iter 620: loss 1.9426\n",
      "iter 630: loss 1.9487\n",
      "iter 640: loss 1.9298\n",
      "iter 650: loss 1.9173\n",
      "iter 660: loss 1.9488\n",
      "iter 670: loss 1.9113\n",
      "iter 680: loss 1.8945\n",
      "iter 690: loss 1.8908\n",
      "iter 700: loss 1.8851\n",
      "iter 710: loss 1.8692\n",
      "iter 720: loss 1.8902\n",
      "iter 730: loss 1.8529\n",
      "iter 740: loss 1.8519\n",
      "iter 750: loss 1.8323\n",
      "iter 760: loss 1.8001\n",
      "iter 770: loss 1.8131\n",
      "iter 780: loss 1.8339\n",
      "iter 790: loss 1.7937\n",
      "iter 800: loss 1.8156\n",
      "iter 810: loss 1.7623\n",
      "iter 820: loss 1.7803\n",
      "iter 830: loss 1.7793\n",
      "iter 840: loss 1.7918\n",
      "iter 850: loss 1.7561\n",
      "iter 860: loss 1.7523\n",
      "iter 870: loss 1.7551\n",
      "iter 880: loss 1.7648\n",
      "iter 890: loss 1.7169\n",
      "iter 900: loss 1.7411\n",
      "iter 910: loss 1.7296\n",
      "iter 920: loss 1.7526\n",
      "iter 930: loss 1.7364\n",
      "iter 940: loss 1.6974\n",
      "iter 950: loss 1.7253\n",
      "iter 960: loss 1.6824\n",
      "iter 970: loss 1.6972\n",
      "iter 980: loss 1.7064\n",
      "iter 990: loss 1.6782\n",
      "iter 1000: loss 1.6768\n",
      "step 1000: train loss 1.6043, val loss 1.6062\n",
      "iter 1010: loss 1.6945\n",
      "iter 1020: loss 1.6624\n",
      "iter 1030: loss 1.6590\n",
      "iter 1040: loss 1.6621\n",
      "iter 1050: loss 1.6580\n",
      "iter 1060: loss 1.6429\n",
      "iter 1070: loss 1.6867\n",
      "iter 1080: loss 1.6041\n",
      "iter 1090: loss 1.6376\n",
      "iter 1100: loss 1.6311\n",
      "iter 1110: loss 1.6298\n",
      "iter 1120: loss 1.6112\n",
      "iter 1130: loss 1.6087\n",
      "iter 1140: loss 1.6119\n",
      "iter 1150: loss 1.6098\n",
      "iter 1160: loss 1.5989\n",
      "iter 1170: loss 1.5948\n",
      "iter 1180: loss 1.5933\n",
      "iter 1190: loss 1.5744\n",
      "iter 1200: loss 1.5879\n",
      "iter 1210: loss 1.5817\n",
      "iter 1220: loss 1.5809\n",
      "iter 1230: loss 1.5756\n",
      "iter 1240: loss 1.5600\n",
      "iter 1250: loss 1.5789\n",
      "iter 1260: loss 1.5736\n",
      "iter 1270: loss 1.5493\n",
      "iter 1280: loss 1.5830\n",
      "iter 1290: loss 1.5652\n",
      "iter 1300: loss 1.5570\n",
      "iter 1310: loss 1.5479\n",
      "iter 1320: loss 1.5382\n",
      "iter 1330: loss 1.5234\n",
      "iter 1340: loss 1.5442\n",
      "iter 1350: loss 1.5285\n",
      "iter 1360: loss 1.5492\n",
      "iter 1370: loss 1.5257\n",
      "iter 1380: loss 1.5304\n",
      "iter 1390: loss 1.5196\n",
      "iter 1400: loss 1.5126\n",
      "iter 1410: loss 1.5318\n",
      "iter 1420: loss 1.5103\n",
      "iter 1430: loss 1.5050\n",
      "iter 1440: loss 1.5242\n",
      "iter 1450: loss 1.5048\n",
      "iter 1460: loss 1.5127\n",
      "iter 1470: loss 1.4997\n",
      "iter 1480: loss 1.5006\n",
      "iter 1490: loss 1.4833\n",
      "iter 1500: loss 1.5010\n",
      "step 1500: train loss 1.4340, val loss 1.4354\n",
      "iter 1510: loss 1.5399\n",
      "iter 1520: loss 1.5016\n",
      "iter 1530: loss 1.4766\n",
      "iter 1540: loss 1.5047\n",
      "iter 1550: loss 1.4647\n",
      "iter 1560: loss 1.4570\n",
      "iter 1570: loss 1.4709\n",
      "iter 1580: loss 1.4897\n",
      "iter 1590: loss 1.4778\n",
      "iter 1600: loss 1.4775\n",
      "iter 1610: loss 1.4703\n",
      "iter 1620: loss 1.4438\n",
      "iter 1630: loss 1.4641\n",
      "iter 1640: loss 1.4724\n",
      "iter 1650: loss 1.4855\n",
      "iter 1660: loss 1.4682\n",
      "iter 1670: loss 1.4580\n",
      "iter 1680: loss 1.4455\n",
      "iter 1690: loss 1.4618\n",
      "iter 1700: loss 1.4494\n",
      "iter 1710: loss 1.4496\n",
      "iter 1720: loss 1.4268\n",
      "iter 1730: loss 1.4509\n",
      "iter 1740: loss 1.4400\n",
      "iter 1750: loss 1.4380\n",
      "iter 1760: loss 1.4255\n",
      "iter 1770: loss 1.4431\n",
      "iter 1780: loss 1.4450\n",
      "iter 1790: loss 1.4365\n",
      "iter 1800: loss 1.4081\n",
      "iter 1810: loss 1.4110\n",
      "iter 1820: loss 1.4189\n",
      "iter 1830: loss 1.4369\n",
      "iter 1840: loss 1.4165\n",
      "iter 1850: loss 1.4364\n",
      "iter 1860: loss 1.4240\n",
      "iter 1870: loss 1.4114\n",
      "iter 1880: loss 1.4119\n",
      "iter 1890: loss 1.4364\n",
      "iter 1900: loss 1.4037\n",
      "iter 1910: loss 1.4215\n",
      "iter 1920: loss 1.3925\n",
      "iter 1930: loss 1.4289\n",
      "iter 1940: loss 1.4050\n",
      "iter 1950: loss 1.4045\n",
      "iter 1960: loss 1.4158\n",
      "iter 1970: loss 1.4091\n",
      "iter 1980: loss 1.3734\n",
      "iter 1990: loss 1.4086\n",
      "iter 2000: loss 1.3958\n",
      "step 2000: train loss 1.3385, val loss 1.3398\n",
      "iter 2010: loss 1.4068\n",
      "iter 2020: loss 1.4168\n",
      "iter 2030: loss 1.4115\n",
      "iter 2040: loss 1.3909\n",
      "iter 2050: loss 1.3876\n",
      "iter 2060: loss 1.3924\n",
      "iter 2070: loss 1.3852\n",
      "iter 2080: loss 1.3915\n",
      "iter 2090: loss 1.3782\n",
      "iter 2100: loss 1.3556\n",
      "iter 2110: loss 1.3772\n",
      "iter 2120: loss 1.4041\n",
      "iter 2130: loss 1.3881\n",
      "iter 2140: loss 1.3754\n",
      "iter 2150: loss 1.3816\n",
      "iter 2160: loss 1.3616\n",
      "iter 2170: loss 1.3811\n",
      "iter 2180: loss 1.3648\n",
      "iter 2190: loss 1.3544\n",
      "iter 2200: loss 1.3788\n",
      "iter 2210: loss 1.3483\n",
      "iter 2220: loss 1.3622\n",
      "iter 2230: loss 1.3637\n",
      "iter 2240: loss 1.3564\n",
      "iter 2250: loss 1.3709\n",
      "iter 2260: loss 1.3644\n",
      "iter 2270: loss 1.3590\n",
      "iter 2280: loss 1.3433\n",
      "iter 2290: loss 1.3686\n",
      "iter 2300: loss 1.3604\n",
      "iter 2310: loss 1.3446\n",
      "iter 2320: loss 1.3527\n",
      "iter 2330: loss 1.3364\n",
      "iter 2340: loss 1.3573\n",
      "iter 2350: loss 1.3291\n",
      "iter 2360: loss 1.3577\n",
      "iter 2370: loss 1.3512\n",
      "iter 2380: loss 1.3509\n",
      "iter 2390: loss 1.3548\n",
      "iter 2400: loss 1.3517\n",
      "iter 2410: loss 1.3444\n",
      "iter 2420: loss 1.3428\n",
      "iter 2430: loss 1.3482\n",
      "iter 2440: loss 1.3386\n",
      "iter 2450: loss 1.3300\n",
      "iter 2460: loss 1.3303\n",
      "iter 2470: loss 1.3217\n",
      "iter 2480: loss 1.3298\n",
      "iter 2490: loss 1.3321\n",
      "iter 2500: loss 1.3411\n",
      "step 2500: train loss 1.2763, val loss 1.2730\n",
      "iter 2510: loss 1.3484\n",
      "iter 2520: loss 1.3117\n",
      "iter 2530: loss 1.3166\n",
      "iter 2540: loss 1.3498\n",
      "iter 2550: loss 1.3314\n",
      "iter 2560: loss 1.3260\n",
      "iter 2570: loss 1.3603\n",
      "iter 2580: loss 1.3374\n",
      "iter 2590: loss 1.3162\n",
      "iter 2600: loss 1.3190\n",
      "iter 2610: loss 1.2985\n",
      "iter 2620: loss 1.3120\n",
      "iter 2630: loss 1.3133\n",
      "iter 2640: loss 1.3117\n",
      "iter 2650: loss 1.3101\n",
      "iter 2660: loss 1.3158\n",
      "iter 2670: loss 1.3103\n",
      "iter 2680: loss 1.3252\n",
      "iter 2690: loss 1.3080\n",
      "iter 2700: loss 1.2944\n",
      "iter 2710: loss 1.2942\n",
      "iter 2720: loss 1.3119\n",
      "iter 2730: loss 1.3057\n",
      "iter 2740: loss 1.3044\n",
      "iter 2750: loss 1.3138\n",
      "iter 2760: loss 1.2888\n",
      "iter 2770: loss 1.2965\n",
      "iter 2780: loss 1.3003\n",
      "iter 2790: loss 1.3111\n",
      "iter 2800: loss 1.3004\n",
      "iter 2810: loss 1.2938\n",
      "iter 2820: loss 1.2989\n",
      "iter 2830: loss 1.3119\n",
      "iter 2840: loss 1.2922\n",
      "iter 2850: loss 1.3031\n",
      "iter 2860: loss 1.2667\n",
      "iter 2870: loss 1.3131\n",
      "iter 2880: loss 1.3049\n",
      "iter 2890: loss 1.2862\n",
      "iter 2900: loss 1.2891\n",
      "iter 2910: loss 1.3095\n",
      "iter 2920: loss 1.2585\n",
      "iter 2930: loss 1.2810\n",
      "iter 2940: loss 1.2808\n",
      "iter 2950: loss 1.2862\n",
      "iter 2960: loss 1.2940\n",
      "iter 2970: loss 1.2849\n",
      "iter 2980: loss 1.2920\n",
      "iter 2990: loss 1.2768\n",
      "iter 3000: loss 1.2804\n",
      "step 3000: train loss 1.2231, val loss 1.2228\n",
      "iter 3010: loss 1.2805\n",
      "iter 3020: loss 1.2781\n",
      "iter 3030: loss 1.2543\n",
      "iter 3040: loss 1.3091\n",
      "iter 3050: loss 1.2918\n",
      "iter 3060: loss 1.2868\n",
      "iter 3070: loss 1.2777\n",
      "iter 3080: loss 1.2880\n",
      "iter 3090: loss 1.2558\n",
      "iter 3100: loss 1.2695\n",
      "iter 3110: loss 1.2926\n",
      "iter 3120: loss 1.2666\n",
      "iter 3130: loss 1.2648\n",
      "iter 3140: loss 1.2568\n",
      "iter 3150: loss 1.2611\n",
      "iter 3160: loss 1.2682\n",
      "iter 3170: loss 1.2604\n",
      "iter 3180: loss 1.2616\n",
      "iter 3190: loss 1.2519\n",
      "iter 3200: loss 1.2634\n",
      "iter 3210: loss 1.2736\n",
      "iter 3220: loss 1.2615\n",
      "iter 3230: loss 1.2380\n",
      "iter 3240: loss 1.2702\n",
      "iter 3250: loss 1.2608\n",
      "iter 3260: loss 1.2692\n",
      "iter 3270: loss 1.2395\n",
      "iter 3280: loss 1.2636\n",
      "iter 3290: loss 1.2664\n",
      "iter 3300: loss 1.2372\n",
      "iter 3310: loss 1.2606\n",
      "iter 3320: loss 1.2754\n",
      "iter 3330: loss 1.2859\n",
      "iter 3340: loss 1.2809\n",
      "iter 3350: loss 1.2581\n",
      "iter 3360: loss 1.2850\n",
      "iter 3370: loss 1.2378\n",
      "iter 3380: loss 1.2467\n",
      "iter 3390: loss 1.2453\n",
      "iter 3400: loss 1.2550\n",
      "iter 3410: loss 1.2375\n",
      "iter 3420: loss 1.2507\n",
      "iter 3430: loss 1.2439\n",
      "iter 3440: loss 1.2465\n",
      "iter 3450: loss 1.2224\n",
      "iter 3460: loss 1.2400\n",
      "iter 3470: loss 1.2417\n",
      "iter 3480: loss 1.2510\n",
      "iter 3490: loss 1.2388\n",
      "iter 3500: loss 1.2568\n",
      "step 3500: train loss 1.1824, val loss 1.1800\n",
      "iter 3510: loss 1.2478\n",
      "iter 3520: loss 1.2433\n",
      "iter 3530: loss 1.2210\n",
      "iter 3540: loss 1.2453\n",
      "iter 3550: loss 1.2513\n",
      "iter 3560: loss 1.2217\n",
      "iter 3570: loss 1.2329\n",
      "iter 3580: loss 1.2351\n",
      "iter 3590: loss 1.2117\n",
      "iter 3600: loss 1.2244\n",
      "iter 3610: loss 1.2320\n",
      "iter 3620: loss 1.2270\n",
      "iter 3630: loss 1.2194\n",
      "iter 3640: loss 1.2264\n",
      "iter 3650: loss 1.2159\n",
      "iter 3660: loss 1.2279\n",
      "iter 3670: loss 1.2159\n",
      "iter 3680: loss 1.2168\n",
      "iter 3690: loss 1.2312\n",
      "iter 3700: loss 1.2290\n",
      "iter 3710: loss 1.2312\n",
      "iter 3720: loss 1.2298\n",
      "iter 3730: loss 1.2336\n",
      "iter 3740: loss 1.2220\n",
      "iter 3750: loss 1.2209\n",
      "iter 3760: loss 1.2078\n",
      "iter 3770: loss 1.2201\n",
      "iter 3780: loss 1.2343\n",
      "iter 3790: loss 1.2027\n",
      "iter 3800: loss 1.1980\n",
      "iter 3810: loss 1.2131\n",
      "iter 3820: loss 1.2231\n",
      "iter 3830: loss 1.2215\n",
      "iter 3840: loss 1.2242\n",
      "iter 3850: loss 1.2245\n",
      "iter 3860: loss 1.2104\n",
      "iter 3870: loss 1.2130\n",
      "iter 3880: loss 1.2141\n",
      "iter 3890: loss 1.2093\n",
      "iter 3900: loss 1.2020\n",
      "iter 3910: loss 1.2287\n",
      "iter 3920: loss 1.2265\n",
      "iter 3930: loss 1.2292\n",
      "iter 3940: loss 1.2000\n",
      "iter 3950: loss 1.1962\n",
      "iter 3960: loss 1.2057\n",
      "iter 3970: loss 1.2072\n",
      "iter 3980: loss 1.2147\n",
      "iter 3990: loss 1.2103\n",
      "iter 4000: loss 1.2220\n",
      "step 4000: train loss 1.1427, val loss 1.1445\n",
      "iter 4010: loss 1.2054\n",
      "iter 4020: loss 1.1989\n",
      "iter 4030: loss 1.2121\n",
      "iter 4040: loss 1.2098\n",
      "iter 4050: loss 1.1833\n",
      "iter 4060: loss 1.2075\n",
      "iter 4070: loss 1.1986\n",
      "iter 4080: loss 1.2064\n",
      "iter 4090: loss 1.2058\n",
      "iter 4100: loss 1.2041\n",
      "iter 4110: loss 1.2075\n",
      "iter 4120: loss 1.2059\n",
      "iter 4130: loss 1.1966\n",
      "iter 4140: loss 1.1956\n",
      "iter 4150: loss 1.1953\n",
      "iter 4160: loss 1.1909\n",
      "iter 4170: loss 1.2059\n",
      "iter 4180: loss 1.2071\n",
      "iter 4190: loss 1.2037\n",
      "iter 4200: loss 1.1791\n",
      "iter 4210: loss 1.2024\n",
      "iter 4220: loss 1.1876\n",
      "iter 4230: loss 1.2045\n",
      "iter 4240: loss 1.1713\n",
      "iter 4250: loss 1.1923\n",
      "iter 4260: loss 1.1935\n",
      "iter 4270: loss 1.2045\n",
      "iter 4280: loss 1.2021\n",
      "iter 4290: loss 1.1881\n",
      "iter 4300: loss 1.2117\n",
      "iter 4310: loss 1.1967\n",
      "iter 4320: loss 1.1771\n",
      "iter 4330: loss 1.2122\n",
      "iter 4340: loss 1.1863\n",
      "iter 4350: loss 1.1691\n",
      "iter 4360: loss 1.1791\n",
      "iter 4370: loss 1.2076\n",
      "iter 4380: loss 1.1756\n",
      "iter 4390: loss 1.1894\n",
      "iter 4400: loss 1.1884\n",
      "iter 4410: loss 1.1830\n",
      "iter 4420: loss 1.1843\n",
      "iter 4430: loss 1.1781\n",
      "iter 4440: loss 1.1969\n",
      "iter 4450: loss 1.1690\n",
      "iter 4460: loss 1.1797\n",
      "iter 4470: loss 1.1758\n",
      "iter 4480: loss 1.1766\n",
      "iter 4490: loss 1.1617\n",
      "iter 4500: loss 1.1903\n",
      "step 4500: train loss 1.1080, val loss 1.1122\n",
      "iter 4510: loss 1.1721\n",
      "iter 4520: loss 1.1575\n",
      "iter 4530: loss 1.1655\n",
      "iter 4540: loss 1.1625\n",
      "iter 4550: loss 1.1747\n",
      "iter 4560: loss 1.1763\n",
      "iter 4570: loss 1.1891\n",
      "iter 4580: loss 1.1822\n",
      "iter 4590: loss 1.1679\n",
      "iter 4600: loss 1.1768\n",
      "iter 4610: loss 1.1782\n",
      "iter 4620: loss 1.1871\n",
      "iter 4630: loss 1.1630\n",
      "iter 4640: loss 1.1699\n",
      "iter 4650: loss 1.1545\n",
      "iter 4660: loss 1.1719\n",
      "iter 4670: loss 1.1692\n",
      "iter 4680: loss 1.1604\n",
      "iter 4690: loss 1.1720\n",
      "iter 4700: loss 1.1710\n",
      "iter 4710: loss 1.1687\n",
      "iter 4720: loss 1.1552\n",
      "iter 4730: loss 1.1728\n",
      "iter 4740: loss 1.1612\n",
      "iter 4750: loss 1.1629\n",
      "iter 4760: loss 1.1531\n",
      "iter 4770: loss 1.1486\n",
      "iter 4780: loss 1.1415\n",
      "iter 4790: loss 1.1671\n",
      "iter 4800: loss 1.1583\n",
      "iter 4810: loss 1.1528\n",
      "iter 4820: loss 1.1745\n",
      "iter 4830: loss 1.1627\n",
      "iter 4840: loss 1.1569\n",
      "iter 4850: loss 1.1766\n",
      "iter 4860: loss 1.1607\n",
      "iter 4870: loss 1.1694\n",
      "iter 4880: loss 1.1608\n",
      "iter 4890: loss 1.1464\n",
      "iter 4900: loss 1.1674\n",
      "iter 4910: loss 1.1590\n",
      "iter 4920: loss 1.1488\n",
      "iter 4930: loss 1.1548\n",
      "iter 4940: loss 1.1572\n",
      "iter 4950: loss 1.1618\n",
      "iter 4960: loss 1.1206\n",
      "iter 4970: loss 1.1742\n",
      "iter 4980: loss 1.1600\n",
      "iter 4990: loss 1.1678\n",
      "iter 5000: loss 1.1347\n",
      "step 5000: train loss 1.0830, val loss 1.0839\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.1606\n",
      "step 0: train loss 4.4255, val loss 4.4252\n",
      "iter 10: loss 3.7422\n",
      "iter 20: loss 3.4064\n",
      "iter 30: loss 3.1580\n",
      "iter 40: loss 2.9868\n",
      "iter 50: loss 2.8806\n",
      "iter 60: loss 2.7738\n",
      "iter 70: loss 2.7201\n",
      "iter 80: loss 2.7138\n",
      "iter 90: loss 2.6465\n",
      "iter 100: loss 2.6336\n",
      "iter 110: loss 2.6171\n",
      "iter 120: loss 2.6033\n",
      "iter 130: loss 2.5807\n",
      "iter 140: loss 2.5734\n",
      "iter 150: loss 2.5547\n",
      "iter 160: loss 2.5502\n",
      "iter 170: loss 2.5354\n",
      "iter 180: loss 2.5470\n",
      "iter 190: loss 2.5281\n",
      "iter 200: loss 2.5336\n",
      "iter 210: loss 2.5189\n",
      "iter 220: loss 2.5082\n",
      "iter 230: loss 2.5099\n",
      "iter 240: loss 2.5104\n",
      "iter 250: loss 2.4881\n",
      "iter 260: loss 2.4949\n",
      "iter 270: loss 2.4837\n",
      "iter 280: loss 2.4882\n",
      "iter 290: loss 2.4614\n",
      "iter 300: loss 2.4581\n",
      "iter 310: loss 2.4622\n",
      "iter 320: loss 2.4536\n",
      "iter 330: loss 2.4207\n",
      "iter 340: loss 2.4220\n",
      "iter 350: loss 2.4151\n",
      "iter 360: loss 2.3986\n",
      "iter 370: loss 2.3987\n",
      "iter 380: loss 2.3880\n",
      "iter 390: loss 2.3591\n",
      "iter 400: loss 2.3315\n",
      "iter 410: loss 2.3373\n",
      "iter 420: loss 2.3356\n",
      "iter 430: loss 2.3140\n",
      "iter 440: loss 2.2789\n",
      "iter 450: loss 2.2749\n",
      "iter 460: loss 2.2533\n",
      "iter 470: loss 2.2583\n",
      "iter 480: loss 2.2414\n",
      "iter 490: loss 2.2264\n",
      "iter 500: loss 2.2131\n",
      "step 500: train loss 2.1463, val loss 2.1475\n",
      "iter 510: loss 2.1846\n",
      "iter 520: loss 2.1927\n",
      "iter 530: loss 2.1589\n",
      "iter 540: loss 2.1793\n",
      "iter 550: loss 2.1302\n",
      "iter 560: loss 2.1434\n",
      "iter 570: loss 2.1089\n",
      "iter 580: loss 2.0822\n",
      "iter 590: loss 2.0931\n",
      "iter 600: loss 2.0866\n",
      "iter 610: loss 2.0661\n",
      "iter 620: loss 2.0471\n",
      "iter 630: loss 2.0332\n",
      "iter 640: loss 2.0201\n",
      "iter 650: loss 2.0491\n",
      "iter 660: loss 2.0282\n",
      "iter 670: loss 2.0002\n",
      "iter 680: loss 1.9753\n",
      "iter 690: loss 1.9736\n",
      "iter 700: loss 1.9662\n",
      "iter 710: loss 1.9554\n",
      "iter 720: loss 1.9478\n",
      "iter 730: loss 1.9250\n",
      "iter 740: loss 1.9159\n",
      "iter 750: loss 1.9147\n",
      "iter 760: loss 1.9169\n",
      "iter 770: loss 1.9226\n",
      "iter 780: loss 1.8739\n",
      "iter 790: loss 1.8823\n",
      "iter 800: loss 1.8769\n",
      "iter 810: loss 1.8887\n",
      "iter 820: loss 1.8831\n",
      "iter 830: loss 1.8409\n",
      "iter 840: loss 1.8695\n",
      "iter 850: loss 1.8575\n",
      "iter 860: loss 1.8360\n",
      "iter 870: loss 1.8299\n",
      "iter 880: loss 1.8249\n",
      "iter 890: loss 1.8173\n",
      "iter 900: loss 1.7901\n",
      "iter 910: loss 1.7791\n",
      "iter 920: loss 1.7617\n",
      "iter 930: loss 1.7778\n",
      "iter 940: loss 1.7840\n",
      "iter 950: loss 1.7756\n",
      "iter 960: loss 1.7679\n",
      "iter 970: loss 1.7681\n",
      "iter 980: loss 1.7425\n",
      "iter 990: loss 1.7338\n",
      "iter 1000: loss 1.7433\n",
      "step 1000: train loss 1.6501, val loss 1.6528\n",
      "iter 1010: loss 1.7112\n",
      "iter 1020: loss 1.7377\n",
      "iter 1030: loss 1.7094\n",
      "iter 1040: loss 1.6939\n",
      "iter 1050: loss 1.7087\n",
      "iter 1060: loss 1.6966\n",
      "iter 1070: loss 1.7150\n",
      "iter 1080: loss 1.6750\n",
      "iter 1090: loss 1.6588\n",
      "iter 1100: loss 1.6618\n",
      "iter 1110: loss 1.6566\n",
      "iter 1120: loss 1.6825\n",
      "iter 1130: loss 1.6605\n",
      "iter 1140: loss 1.6488\n",
      "iter 1150: loss 1.6561\n",
      "iter 1160: loss 1.6552\n",
      "iter 1170: loss 1.6200\n",
      "iter 1180: loss 1.6417\n",
      "iter 1190: loss 1.6192\n",
      "iter 1200: loss 1.6163\n",
      "iter 1210: loss 1.6066\n",
      "iter 1220: loss 1.6205\n",
      "iter 1230: loss 1.6082\n",
      "iter 1240: loss 1.6243\n",
      "iter 1250: loss 1.6079\n",
      "iter 1260: loss 1.5946\n",
      "iter 1270: loss 1.6045\n",
      "iter 1280: loss 1.5832\n",
      "iter 1290: loss 1.5936\n",
      "iter 1300: loss 1.5998\n",
      "iter 1310: loss 1.5804\n",
      "iter 1320: loss 1.5794\n",
      "iter 1330: loss 1.5782\n",
      "iter 1340: loss 1.5572\n",
      "iter 1350: loss 1.5389\n",
      "iter 1360: loss 1.5645\n",
      "iter 1370: loss 1.5778\n",
      "iter 1380: loss 1.5575\n",
      "iter 1390: loss 1.5564\n",
      "iter 1400: loss 1.5571\n",
      "iter 1410: loss 1.5465\n",
      "iter 1420: loss 1.5606\n",
      "iter 1430: loss 1.5377\n",
      "iter 1440: loss 1.5447\n",
      "iter 1450: loss 1.5468\n",
      "iter 1460: loss 1.5359\n",
      "iter 1470: loss 1.5146\n",
      "iter 1480: loss 1.5546\n",
      "iter 1490: loss 1.5366\n",
      "iter 1500: loss 1.5084\n",
      "step 1500: train loss 1.4490, val loss 1.4519\n",
      "iter 1510: loss 1.4932\n",
      "iter 1520: loss 1.5319\n",
      "iter 1530: loss 1.5498\n",
      "iter 1540: loss 1.5295\n",
      "iter 1550: loss 1.5198\n",
      "iter 1560: loss 1.4877\n",
      "iter 1570: loss 1.4927\n",
      "iter 1580: loss 1.5044\n",
      "iter 1590: loss 1.4966\n",
      "iter 1600: loss 1.5203\n",
      "iter 1610: loss 1.5083\n",
      "iter 1620: loss 1.4852\n",
      "iter 1630: loss 1.4989\n",
      "iter 1640: loss 1.4988\n",
      "iter 1650: loss 1.4725\n",
      "iter 1660: loss 1.4914\n",
      "iter 1670: loss 1.4672\n",
      "iter 1680: loss 1.4726\n",
      "iter 1690: loss 1.4506\n",
      "iter 1700: loss 1.4703\n",
      "iter 1710: loss 1.4776\n",
      "iter 1720: loss 1.4595\n",
      "iter 1730: loss 1.4559\n",
      "iter 1740: loss 1.4564\n",
      "iter 1750: loss 1.4520\n",
      "iter 1760: loss 1.4755\n",
      "iter 1770: loss 1.4688\n",
      "iter 1780: loss 1.4547\n",
      "iter 1790: loss 1.4645\n",
      "iter 1800: loss 1.4607\n",
      "iter 1810: loss 1.4412\n",
      "iter 1820: loss 1.4809\n",
      "iter 1830: loss 1.4522\n",
      "iter 1840: loss 1.4262\n",
      "iter 1850: loss 1.4320\n",
      "iter 1860: loss 1.4395\n",
      "iter 1870: loss 1.4465\n",
      "iter 1880: loss 1.4493\n",
      "iter 1890: loss 1.4286\n",
      "iter 1900: loss 1.4283\n",
      "iter 1910: loss 1.3989\n",
      "iter 1920: loss 1.4384\n",
      "iter 1930: loss 1.4115\n",
      "iter 1940: loss 1.4354\n",
      "iter 1950: loss 1.4237\n",
      "iter 1960: loss 1.4118\n",
      "iter 1970: loss 1.4237\n",
      "iter 1980: loss 1.4316\n",
      "iter 1990: loss 1.4007\n",
      "iter 2000: loss 1.3966\n",
      "step 2000: train loss 1.3459, val loss 1.3479\n",
      "iter 2010: loss 1.4055\n",
      "iter 2020: loss 1.4123\n",
      "iter 2030: loss 1.3993\n",
      "iter 2040: loss 1.4096\n",
      "iter 2050: loss 1.4007\n",
      "iter 2060: loss 1.4355\n",
      "iter 2070: loss 1.4082\n",
      "iter 2080: loss 1.4062\n",
      "iter 2090: loss 1.3847\n",
      "iter 2100: loss 1.4037\n",
      "iter 2110: loss 1.3983\n",
      "iter 2120: loss 1.3792\n",
      "iter 2130: loss 1.3971\n",
      "iter 2140: loss 1.3969\n",
      "iter 2150: loss 1.3720\n",
      "iter 2160: loss 1.3789\n",
      "iter 2170: loss 1.3940\n",
      "iter 2180: loss 1.4102\n",
      "iter 2190: loss 1.4080\n",
      "iter 2200: loss 1.3830\n",
      "iter 2210: loss 1.3686\n",
      "iter 2220: loss 1.3842\n",
      "iter 2230: loss 1.3606\n",
      "iter 2240: loss 1.3682\n",
      "iter 2250: loss 1.3755\n",
      "iter 2260: loss 1.3725\n",
      "iter 2270: loss 1.3773\n",
      "iter 2280: loss 1.3881\n",
      "iter 2290: loss 1.3806\n",
      "iter 2300: loss 1.3684\n",
      "iter 2310: loss 1.3743\n",
      "iter 2320: loss 1.3501\n",
      "iter 2330: loss 1.3774\n",
      "iter 2340: loss 1.3548\n",
      "iter 2350: loss 1.3720\n",
      "iter 2360: loss 1.3473\n",
      "iter 2370: loss 1.3536\n",
      "iter 2380: loss 1.3636\n",
      "iter 2390: loss 1.3298\n",
      "iter 2400: loss 1.3728\n",
      "iter 2410: loss 1.3472\n",
      "iter 2420: loss 1.3555\n",
      "iter 2430: loss 1.3322\n",
      "iter 2440: loss 1.3550\n",
      "iter 2450: loss 1.3614\n",
      "iter 2460: loss 1.3393\n",
      "iter 2470: loss 1.3444\n",
      "iter 2480: loss 1.3526\n",
      "iter 2490: loss 1.3597\n",
      "iter 2500: loss 1.3033\n",
      "step 2500: train loss 1.2758, val loss 1.2788\n",
      "iter 2510: loss 1.3325\n",
      "iter 2520: loss 1.3400\n",
      "iter 2530: loss 1.3400\n",
      "iter 2540: loss 1.3331\n",
      "iter 2550: loss 1.3215\n",
      "iter 2560: loss 1.3416\n",
      "iter 2570: loss 1.3456\n",
      "iter 2580: loss 1.3446\n",
      "iter 2590: loss 1.3519\n",
      "iter 2600: loss 1.3323\n",
      "iter 2610: loss 1.3165\n",
      "iter 2620: loss 1.3380\n",
      "iter 2630: loss 1.3242\n",
      "iter 2640: loss 1.3261\n",
      "iter 2650: loss 1.3101\n",
      "iter 2660: loss 1.3153\n",
      "iter 2670: loss 1.3198\n",
      "iter 2680: loss 1.3283\n",
      "iter 2690: loss 1.3281\n",
      "iter 2700: loss 1.3157\n",
      "iter 2710: loss 1.3187\n",
      "iter 2720: loss 1.3208\n",
      "iter 2730: loss 1.3149\n",
      "iter 2740: loss 1.3098\n",
      "iter 2750: loss 1.3262\n",
      "iter 2760: loss 1.3297\n",
      "iter 2770: loss 1.3236\n",
      "iter 2780: loss 1.3089\n",
      "iter 2790: loss 1.3020\n",
      "iter 2800: loss 1.3315\n",
      "iter 2810: loss 1.3166\n",
      "iter 2820: loss 1.2985\n",
      "iter 2830: loss 1.2972\n",
      "iter 2840: loss 1.3094\n",
      "iter 2850: loss 1.2898\n",
      "iter 2860: loss 1.2960\n",
      "iter 2870: loss 1.2972\n",
      "iter 2880: loss 1.3151\n",
      "iter 2890: loss 1.2916\n",
      "iter 2900: loss 1.2861\n",
      "iter 2910: loss 1.2848\n",
      "iter 2920: loss 1.2761\n",
      "iter 2930: loss 1.2706\n",
      "iter 2940: loss 1.3147\n",
      "iter 2950: loss 1.2968\n",
      "iter 2960: loss 1.2903\n",
      "iter 2970: loss 1.3083\n",
      "iter 2980: loss 1.2964\n",
      "iter 2990: loss 1.2896\n",
      "iter 3000: loss 1.2982\n",
      "step 3000: train loss 1.2250, val loss 1.2265\n",
      "iter 3010: loss 1.2821\n",
      "iter 3020: loss 1.2945\n",
      "iter 3030: loss 1.2925\n",
      "iter 3040: loss 1.2775\n",
      "iter 3050: loss 1.2820\n",
      "iter 3060: loss 1.2991\n",
      "iter 3070: loss 1.2822\n",
      "iter 3080: loss 1.2554\n",
      "iter 3090: loss 1.2651\n",
      "iter 3100: loss 1.2927\n",
      "iter 3110: loss 1.3015\n",
      "iter 3120: loss 1.2545\n",
      "iter 3130: loss 1.2434\n",
      "iter 3140: loss 1.2701\n",
      "iter 3150: loss 1.2556\n",
      "iter 3160: loss 1.2845\n",
      "iter 3170: loss 1.2553\n",
      "iter 3180: loss 1.2726\n",
      "iter 3190: loss 1.2630\n",
      "iter 3200: loss 1.2699\n",
      "iter 3210: loss 1.2693\n",
      "iter 3220: loss 1.2648\n",
      "iter 3230: loss 1.2640\n",
      "iter 3240: loss 1.2751\n",
      "iter 3250: loss 1.2725\n",
      "iter 3260: loss 1.2715\n",
      "iter 3270: loss 1.2522\n",
      "iter 3280: loss 1.2524\n",
      "iter 3290: loss 1.2435\n",
      "iter 3300: loss 1.2364\n",
      "iter 3310: loss 1.2670\n",
      "iter 3320: loss 1.2699\n",
      "iter 3330: loss 1.2751\n",
      "iter 3340: loss 1.2563\n",
      "iter 3350: loss 1.2652\n",
      "iter 3360: loss 1.2747\n",
      "iter 3370: loss 1.2549\n",
      "iter 3380: loss 1.2459\n",
      "iter 3390: loss 1.2630\n",
      "iter 3400: loss 1.2433\n",
      "iter 3410: loss 1.2648\n",
      "iter 3420: loss 1.2535\n",
      "iter 3430: loss 1.2433\n",
      "iter 3440: loss 1.2445\n",
      "iter 3450: loss 1.2710\n",
      "iter 3460: loss 1.2513\n",
      "iter 3470: loss 1.2501\n",
      "iter 3480: loss 1.2423\n",
      "iter 3490: loss 1.2572\n",
      "iter 3500: loss 1.2396\n",
      "step 3500: train loss 1.1844, val loss 1.1864\n",
      "iter 3510: loss 1.2449\n",
      "iter 3520: loss 1.2199\n",
      "iter 3530: loss 1.2354\n",
      "iter 3540: loss 1.2294\n",
      "iter 3550: loss 1.2374\n",
      "iter 3560: loss 1.2573\n",
      "iter 3570: loss 1.2136\n",
      "iter 3580: loss 1.2309\n",
      "iter 3590: loss 1.2410\n",
      "iter 3600: loss 1.2429\n",
      "iter 3610: loss 1.2458\n",
      "iter 3620: loss 1.2326\n",
      "iter 3630: loss 1.2411\n",
      "iter 3640: loss 1.2520\n",
      "iter 3650: loss 1.2381\n",
      "iter 3660: loss 1.2301\n",
      "iter 3670: loss 1.2471\n",
      "iter 3680: loss 1.2414\n",
      "iter 3690: loss 1.2373\n",
      "iter 3700: loss 1.2095\n",
      "iter 3710: loss 1.2397\n",
      "iter 3720: loss 1.2202\n",
      "iter 3730: loss 1.2286\n",
      "iter 3740: loss 1.2482\n",
      "iter 3750: loss 1.2310\n",
      "iter 3760: loss 1.2384\n",
      "iter 3770: loss 1.2356\n",
      "iter 3780: loss 1.2169\n",
      "iter 3790: loss 1.2223\n",
      "iter 3800: loss 1.2408\n",
      "iter 3810: loss 1.2115\n",
      "iter 3820: loss 1.2365\n",
      "iter 3830: loss 1.2133\n",
      "iter 3840: loss 1.2148\n",
      "iter 3850: loss 1.2271\n",
      "iter 3860: loss 1.2215\n",
      "iter 3870: loss 1.2379\n",
      "iter 3880: loss 1.2202\n",
      "iter 3890: loss 1.2218\n",
      "iter 3900: loss 1.2082\n",
      "iter 3910: loss 1.1953\n",
      "iter 3920: loss 1.2184\n",
      "iter 3930: loss 1.2195\n",
      "iter 3940: loss 1.2049\n",
      "iter 3950: loss 1.2237\n",
      "iter 3960: loss 1.2106\n",
      "iter 3970: loss 1.2242\n",
      "iter 3980: loss 1.2124\n",
      "iter 3990: loss 1.2134\n",
      "iter 4000: loss 1.2131\n",
      "step 4000: train loss 1.1455, val loss 1.1488\n",
      "iter 4010: loss 1.2012\n",
      "iter 4020: loss 1.2154\n",
      "iter 4030: loss 1.2100\n",
      "iter 4040: loss 1.2140\n",
      "iter 4050: loss 1.2115\n",
      "iter 4060: loss 1.1948\n",
      "iter 4070: loss 1.2182\n",
      "iter 4080: loss 1.2141\n",
      "iter 4090: loss 1.2117\n",
      "iter 4100: loss 1.2120\n",
      "iter 4110: loss 1.2147\n",
      "iter 4120: loss 1.1976\n",
      "iter 4130: loss 1.1856\n",
      "iter 4140: loss 1.2037\n",
      "iter 4150: loss 1.1820\n",
      "iter 4160: loss 1.2005\n",
      "iter 4170: loss 1.1976\n",
      "iter 4180: loss 1.1952\n",
      "iter 4190: loss 1.2005\n",
      "iter 4200: loss 1.2023\n",
      "iter 4210: loss 1.1964\n",
      "iter 4220: loss 1.1849\n",
      "iter 4230: loss 1.2019\n",
      "iter 4240: loss 1.1961\n",
      "iter 4250: loss 1.1858\n",
      "iter 4260: loss 1.1867\n",
      "iter 4270: loss 1.1960\n",
      "iter 4280: loss 1.1828\n",
      "iter 4290: loss 1.2051\n",
      "iter 4300: loss 1.1630\n",
      "iter 4310: loss 1.1829\n",
      "iter 4320: loss 1.1929\n",
      "iter 4330: loss 1.1954\n",
      "iter 4340: loss 1.2009\n",
      "iter 4350: loss 1.1939\n",
      "iter 4360: loss 1.1978\n",
      "iter 4370: loss 1.1877\n",
      "iter 4380: loss 1.1781\n",
      "iter 4390: loss 1.1762\n",
      "iter 4400: loss 1.1854\n",
      "iter 4410: loss 1.1843\n",
      "iter 4420: loss 1.2151\n",
      "iter 4430: loss 1.1682\n",
      "iter 4440: loss 1.2040\n",
      "iter 4450: loss 1.1606\n",
      "iter 4460: loss 1.1763\n",
      "iter 4470: loss 1.1813\n",
      "iter 4480: loss 1.1759\n",
      "iter 4490: loss 1.1728\n",
      "iter 4500: loss 1.1625\n",
      "step 4500: train loss 1.1161, val loss 1.1169\n",
      "iter 4510: loss 1.1883\n",
      "iter 4520: loss 1.1648\n",
      "iter 4530: loss 1.1826\n",
      "iter 4540: loss 1.1709\n",
      "iter 4550: loss 1.1972\n",
      "iter 4560: loss 1.1636\n",
      "iter 4570: loss 1.2086\n",
      "iter 4580: loss 1.1757\n",
      "iter 4590: loss 1.1686\n",
      "iter 4600: loss 1.1772\n",
      "iter 4610: loss 1.1753\n",
      "iter 4620: loss 1.1700\n",
      "iter 4630: loss 1.1706\n",
      "iter 4640: loss 1.1724\n",
      "iter 4650: loss 1.1842\n",
      "iter 4660: loss 1.1832\n",
      "iter 4670: loss 1.1736\n",
      "iter 4680: loss 1.1610\n",
      "iter 4690: loss 1.1795\n",
      "iter 4700: loss 1.1570\n",
      "iter 4710: loss 1.1681\n",
      "iter 4720: loss 1.1668\n",
      "iter 4730: loss 1.1625\n",
      "iter 4740: loss 1.1653\n",
      "iter 4750: loss 1.1733\n",
      "iter 4760: loss 1.1748\n",
      "iter 4770: loss 1.1747\n",
      "iter 4780: loss 1.1753\n",
      "iter 4790: loss 1.1588\n",
      "iter 4800: loss 1.1692\n",
      "iter 4810: loss 1.1734\n",
      "iter 4820: loss 1.1674\n",
      "iter 4830: loss 1.1625\n",
      "iter 4840: loss 1.1699\n",
      "iter 4850: loss 1.1601\n",
      "iter 4860: loss 1.1559\n",
      "iter 4870: loss 1.1639\n",
      "iter 4880: loss 1.1678\n",
      "iter 4890: loss 1.1587\n",
      "iter 4900: loss 1.1514\n",
      "iter 4910: loss 1.1582\n",
      "iter 4920: loss 1.1516\n",
      "iter 4930: loss 1.1842\n",
      "iter 4940: loss 1.1633\n",
      "iter 4950: loss 1.1472\n",
      "iter 4960: loss 1.1705\n",
      "iter 4970: loss 1.1554\n",
      "iter 4980: loss 1.1614\n",
      "iter 4990: loss 1.1411\n",
      "iter 5000: loss 1.1499\n",
      "step 5000: train loss 1.0860, val loss 1.0867\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.2306\n",
      "step 0: train loss 4.5291, val loss 4.5301\n",
      "iter 10: loss 3.1832\n",
      "iter 20: loss 3.1268\n",
      "iter 30: loss 3.0865\n",
      "iter 40: loss 2.9204\n",
      "iter 50: loss 2.7914\n",
      "iter 60: loss 2.7341\n",
      "iter 70: loss 2.6924\n",
      "iter 80: loss 2.6571\n",
      "iter 90: loss 2.6175\n",
      "iter 100: loss 2.6012\n",
      "iter 110: loss 2.5797\n",
      "iter 120: loss 2.5702\n",
      "iter 130: loss 2.5554\n",
      "iter 140: loss 2.5583\n",
      "iter 150: loss 2.5359\n",
      "iter 160: loss 2.5130\n",
      "iter 170: loss 2.5212\n",
      "iter 180: loss 2.5324\n",
      "iter 190: loss 2.4968\n",
      "iter 200: loss 2.4937\n",
      "iter 210: loss 2.4733\n",
      "iter 220: loss 2.4716\n",
      "iter 230: loss 2.4624\n",
      "iter 240: loss 2.4406\n",
      "iter 250: loss 2.4275\n",
      "iter 260: loss 2.4251\n",
      "iter 270: loss 2.4287\n",
      "iter 280: loss 2.3883\n",
      "iter 290: loss 2.3638\n",
      "iter 300: loss 2.3427\n",
      "iter 310: loss 2.3090\n",
      "iter 320: loss 2.2842\n",
      "iter 330: loss 2.2446\n",
      "iter 340: loss 2.2606\n",
      "iter 350: loss 2.1954\n",
      "iter 360: loss 2.1742\n",
      "iter 370: loss 2.1674\n",
      "iter 380: loss 2.1601\n",
      "iter 390: loss 2.1115\n",
      "iter 400: loss 2.0851\n",
      "iter 410: loss 2.0807\n",
      "iter 420: loss 2.0320\n",
      "iter 430: loss 2.0053\n",
      "iter 440: loss 1.9731\n",
      "iter 450: loss 1.9613\n",
      "iter 460: loss 1.9431\n",
      "iter 470: loss 1.9263\n",
      "iter 480: loss 1.9058\n",
      "iter 490: loss 1.8367\n",
      "iter 500: loss 1.8375\n",
      "step 500: train loss 1.7838, val loss 1.7813\n",
      "iter 510: loss 1.8025\n",
      "iter 520: loss 1.7995\n",
      "iter 530: loss 1.7623\n",
      "iter 540: loss 1.7291\n",
      "iter 550: loss 1.7498\n",
      "iter 560: loss 1.7070\n",
      "iter 570: loss 1.6794\n",
      "iter 580: loss 1.6989\n",
      "iter 590: loss 1.6623\n",
      "iter 600: loss 1.6249\n",
      "iter 610: loss 1.6247\n",
      "iter 620: loss 1.5954\n",
      "iter 630: loss 1.6237\n",
      "iter 640: loss 1.5631\n",
      "iter 650: loss 1.5876\n",
      "iter 660: loss 1.5782\n",
      "iter 670: loss 1.5694\n",
      "iter 680: loss 1.5310\n",
      "iter 690: loss 1.5439\n",
      "iter 700: loss 1.5376\n",
      "iter 710: loss 1.5235\n",
      "iter 720: loss 1.4910\n",
      "iter 730: loss 1.5016\n",
      "iter 740: loss 1.4826\n",
      "iter 750: loss 1.4907\n",
      "iter 760: loss 1.4608\n",
      "iter 770: loss 1.4705\n",
      "iter 780: loss 1.4655\n",
      "iter 790: loss 1.4652\n",
      "iter 800: loss 1.4411\n",
      "iter 810: loss 1.4326\n",
      "iter 820: loss 1.4445\n",
      "iter 830: loss 1.4132\n",
      "iter 840: loss 1.4243\n",
      "iter 850: loss 1.4286\n",
      "iter 860: loss 1.3941\n",
      "iter 870: loss 1.4339\n",
      "iter 880: loss 1.4125\n",
      "iter 890: loss 1.3990\n",
      "iter 900: loss 1.4044\n",
      "iter 910: loss 1.4073\n",
      "iter 920: loss 1.3815\n",
      "iter 930: loss 1.3839\n",
      "iter 940: loss 1.3986\n",
      "iter 950: loss 1.3604\n",
      "iter 960: loss 1.3870\n",
      "iter 970: loss 1.3680\n",
      "iter 980: loss 1.3694\n",
      "iter 990: loss 1.3680\n",
      "iter 1000: loss 1.3478\n",
      "step 1000: train loss 1.3077, val loss 1.3134\n",
      "iter 1010: loss 1.3583\n",
      "iter 1020: loss 1.3575\n",
      "iter 1030: loss 1.3450\n",
      "iter 1040: loss 1.3392\n",
      "iter 1050: loss 1.3249\n",
      "iter 1060: loss 1.3329\n",
      "iter 1070: loss 1.3066\n",
      "iter 1080: loss 1.3130\n",
      "iter 1090: loss 1.3135\n",
      "iter 1100: loss 1.3169\n",
      "iter 1110: loss 1.3245\n",
      "iter 1120: loss 1.3263\n",
      "iter 1130: loss 1.3342\n",
      "iter 1140: loss 1.3343\n",
      "iter 1150: loss 1.3185\n",
      "iter 1160: loss 1.2879\n",
      "iter 1170: loss 1.3077\n",
      "iter 1180: loss 1.3130\n",
      "iter 1190: loss 1.2904\n",
      "iter 1200: loss 1.3060\n",
      "iter 1210: loss 1.2953\n",
      "iter 1220: loss 1.2842\n",
      "iter 1230: loss 1.2585\n",
      "iter 1240: loss 1.2846\n",
      "iter 1250: loss 1.2735\n",
      "iter 1260: loss 1.2894\n",
      "iter 1270: loss 1.2825\n",
      "iter 1280: loss 1.2547\n",
      "iter 1290: loss 1.2423\n",
      "iter 1300: loss 1.2724\n",
      "iter 1310: loss 1.2656\n",
      "iter 1320: loss 1.2652\n",
      "iter 1330: loss 1.2758\n",
      "iter 1340: loss 1.2805\n",
      "iter 1350: loss 1.2363\n",
      "iter 1360: loss 1.2449\n",
      "iter 1370: loss 1.2505\n",
      "iter 1380: loss 1.2362\n",
      "iter 1390: loss 1.2448\n",
      "iter 1400: loss 1.2520\n",
      "iter 1410: loss 1.2531\n",
      "iter 1420: loss 1.2429\n",
      "iter 1430: loss 1.2411\n",
      "iter 1440: loss 1.2478\n",
      "iter 1450: loss 1.2502\n",
      "iter 1460: loss 1.2356\n",
      "iter 1470: loss 1.2423\n",
      "iter 1480: loss 1.2256\n",
      "iter 1490: loss 1.2177\n",
      "iter 1500: loss 1.2303\n",
      "step 1500: train loss 1.1866, val loss 1.1873\n",
      "iter 1510: loss 1.2334\n",
      "iter 1520: loss 1.2305\n",
      "iter 1530: loss 1.2051\n",
      "iter 1540: loss 1.2250\n",
      "iter 1550: loss 1.2371\n",
      "iter 1560: loss 1.2170\n",
      "iter 1570: loss 1.2103\n",
      "iter 1580: loss 1.2108\n",
      "iter 1590: loss 1.2089\n",
      "iter 1600: loss 1.2227\n",
      "iter 1610: loss 1.2231\n",
      "iter 1620: loss 1.2174\n",
      "iter 1630: loss 1.2176\n",
      "iter 1640: loss 1.2067\n",
      "iter 1650: loss 1.1934\n",
      "iter 1660: loss 1.1993\n",
      "iter 1670: loss 1.2115\n",
      "iter 1680: loss 1.1980\n",
      "iter 1690: loss 1.1835\n",
      "iter 1700: loss 1.1931\n",
      "iter 1710: loss 1.1825\n",
      "iter 1720: loss 1.1868\n",
      "iter 1730: loss 1.2166\n",
      "iter 1740: loss 1.2100\n",
      "iter 1750: loss 1.2113\n",
      "iter 1760: loss 1.1755\n",
      "iter 1770: loss 1.1893\n",
      "iter 1780: loss 1.1841\n",
      "iter 1790: loss 1.2014\n",
      "iter 1800: loss 1.2017\n",
      "iter 1810: loss 1.1817\n",
      "iter 1820: loss 1.1836\n",
      "iter 1830: loss 1.1797\n",
      "iter 1840: loss 1.1780\n",
      "iter 1850: loss 1.1762\n",
      "iter 1860: loss 1.1637\n",
      "iter 1870: loss 1.1751\n",
      "iter 1880: loss 1.1654\n",
      "iter 1890: loss 1.1788\n",
      "iter 1900: loss 1.1707\n",
      "iter 1910: loss 1.1554\n",
      "iter 1920: loss 1.1576\n",
      "iter 1930: loss 1.1530\n",
      "iter 1940: loss 1.1725\n",
      "iter 1950: loss 1.1660\n",
      "iter 1960: loss 1.1405\n",
      "iter 1970: loss 1.1334\n",
      "iter 1980: loss 1.1714\n",
      "iter 1990: loss 1.1709\n",
      "iter 2000: loss 1.1465\n",
      "step 2000: train loss 1.1013, val loss 1.1076\n",
      "iter 2010: loss 1.1580\n",
      "iter 2020: loss 1.1602\n",
      "iter 2030: loss 1.1333\n",
      "iter 2040: loss 1.1629\n",
      "iter 2050: loss 1.1513\n",
      "iter 2060: loss 1.1416\n",
      "iter 2070: loss 1.1474\n",
      "iter 2080: loss 1.1345\n",
      "iter 2090: loss 1.1392\n",
      "iter 2100: loss 1.1631\n",
      "iter 2110: loss 1.1544\n",
      "iter 2120: loss 1.1155\n",
      "iter 2130: loss 1.1418\n",
      "iter 2140: loss 1.1447\n",
      "iter 2150: loss 1.1390\n",
      "iter 2160: loss 1.1231\n",
      "iter 2170: loss 1.1271\n",
      "iter 2180: loss 1.1238\n",
      "iter 2190: loss 1.1551\n",
      "iter 2200: loss 1.1276\n",
      "iter 2210: loss 1.1303\n",
      "iter 2220: loss 1.1303\n",
      "iter 2230: loss 1.1456\n",
      "iter 2240: loss 1.1291\n",
      "iter 2250: loss 1.1229\n",
      "iter 2260: loss 1.1469\n",
      "iter 2270: loss 1.1333\n",
      "iter 2280: loss 1.1317\n",
      "iter 2290: loss 1.1373\n",
      "iter 2300: loss 1.0964\n",
      "iter 2310: loss 1.1146\n",
      "iter 2320: loss 1.0989\n",
      "iter 2330: loss 1.1177\n",
      "iter 2340: loss 1.1138\n",
      "iter 2350: loss 1.1178\n",
      "iter 2360: loss 1.1126\n",
      "iter 2370: loss 1.0989\n",
      "iter 2380: loss 1.1067\n",
      "iter 2390: loss 1.1109\n",
      "iter 2400: loss 1.1040\n",
      "iter 2410: loss 1.1109\n",
      "iter 2420: loss 1.1114\n",
      "iter 2430: loss 1.0939\n",
      "iter 2440: loss 1.1109\n",
      "iter 2450: loss 1.1041\n",
      "iter 2460: loss 1.0908\n",
      "iter 2470: loss 1.1057\n",
      "iter 2480: loss 1.0830\n",
      "iter 2490: loss 1.0949\n",
      "iter 2500: loss 1.1016\n",
      "step 2500: train loss 1.0428, val loss 1.0446\n",
      "iter 2510: loss 1.1038\n",
      "iter 2520: loss 1.0958\n",
      "iter 2530: loss 1.0920\n",
      "iter 2540: loss 1.0806\n",
      "iter 2550: loss 1.1164\n",
      "iter 2560: loss 1.0709\n",
      "iter 2570: loss 1.0678\n",
      "iter 2580: loss 1.0931\n",
      "iter 2590: loss 1.0738\n",
      "iter 2600: loss 1.0968\n",
      "iter 2610: loss 1.1035\n",
      "iter 2620: loss 1.0830\n",
      "iter 2630: loss 1.1112\n",
      "iter 2640: loss 1.0786\n",
      "iter 2650: loss 1.0994\n",
      "iter 2660: loss 1.0913\n",
      "iter 2670: loss 1.0800\n",
      "iter 2680: loss 1.0803\n",
      "iter 2690: loss 1.0853\n",
      "iter 2700: loss 1.0803\n",
      "iter 2710: loss 1.0697\n",
      "iter 2720: loss 1.0876\n",
      "iter 2730: loss 1.0695\n",
      "iter 2740: loss 1.0566\n",
      "iter 2750: loss 1.0774\n",
      "iter 2760: loss 1.0628\n",
      "iter 2770: loss 1.0638\n",
      "iter 2780: loss 1.0807\n",
      "iter 2790: loss 1.0637\n",
      "iter 2800: loss 1.0764\n",
      "iter 2810: loss 1.0762\n",
      "iter 2820: loss 1.0850\n",
      "iter 2830: loss 1.0671\n",
      "iter 2840: loss 1.0452\n",
      "iter 2850: loss 1.0592\n",
      "iter 2860: loss 1.0670\n",
      "iter 2870: loss 1.0478\n",
      "iter 2880: loss 1.0735\n",
      "iter 2890: loss 1.0430\n",
      "iter 2900: loss 1.0535\n",
      "iter 2910: loss 1.0487\n",
      "iter 2920: loss 1.0728\n",
      "iter 2930: loss 1.0550\n",
      "iter 2940: loss 1.0612\n",
      "iter 2950: loss 1.0554\n",
      "iter 2960: loss 1.0526\n",
      "iter 2970: loss 1.0527\n",
      "iter 2980: loss 1.0545\n",
      "iter 2990: loss 1.0529\n",
      "iter 3000: loss 1.0461\n",
      "step 3000: train loss 0.9885, val loss 0.9894\n",
      "iter 3010: loss 1.0684\n",
      "iter 3020: loss 1.0442\n",
      "iter 3030: loss 1.0576\n",
      "iter 3040: loss 1.0380\n",
      "iter 3050: loss 1.0561\n",
      "iter 3060: loss 1.0518\n",
      "iter 3070: loss 1.0551\n",
      "iter 3080: loss 1.0320\n",
      "iter 3090: loss 1.0562\n",
      "iter 3100: loss 1.0338\n",
      "iter 3110: loss 1.0267\n",
      "iter 3120: loss 1.0397\n",
      "iter 3130: loss 1.0424\n",
      "iter 3140: loss 1.0447\n",
      "iter 3150: loss 1.0452\n",
      "iter 3160: loss 1.0373\n",
      "iter 3170: loss 1.0556\n",
      "iter 3180: loss 1.0360\n",
      "iter 3190: loss 1.0498\n",
      "iter 3200: loss 1.0325\n",
      "iter 3210: loss 1.0410\n",
      "iter 3220: loss 1.0228\n",
      "iter 3230: loss 1.0523\n",
      "iter 3240: loss 1.0297\n",
      "iter 3250: loss 1.0265\n",
      "iter 3260: loss 1.0204\n",
      "iter 3270: loss 1.0321\n",
      "iter 3280: loss 1.0379\n",
      "iter 3290: loss 1.0212\n",
      "iter 3300: loss 1.0281\n",
      "iter 3310: loss 1.0212\n",
      "iter 3320: loss 1.0386\n",
      "iter 3330: loss 1.0351\n",
      "iter 3340: loss 1.0217\n",
      "iter 3350: loss 1.0129\n",
      "iter 3360: loss 1.0361\n",
      "iter 3370: loss 1.0175\n",
      "iter 3380: loss 1.0239\n",
      "iter 3390: loss 1.0334\n",
      "iter 3400: loss 1.0012\n",
      "iter 3410: loss 0.9999\n",
      "iter 3420: loss 1.0086\n",
      "iter 3430: loss 1.0190\n",
      "iter 3440: loss 1.0025\n",
      "iter 3450: loss 1.0086\n",
      "iter 3460: loss 1.0310\n",
      "iter 3470: loss 1.0234\n",
      "iter 3480: loss 1.0072\n",
      "iter 3490: loss 1.0256\n",
      "iter 3500: loss 1.0108\n",
      "step 3500: train loss 0.9380, val loss 0.9394\n",
      "iter 3510: loss 0.9964\n",
      "iter 3520: loss 1.0123\n",
      "iter 3530: loss 0.9968\n",
      "iter 3540: loss 0.9983\n",
      "iter 3550: loss 1.0108\n",
      "iter 3560: loss 1.0038\n",
      "iter 3570: loss 0.9890\n",
      "iter 3580: loss 0.9972\n",
      "iter 3590: loss 1.0003\n",
      "iter 3600: loss 0.9906\n",
      "iter 3610: loss 0.9996\n",
      "iter 3620: loss 0.9957\n",
      "iter 3630: loss 1.0058\n",
      "iter 3640: loss 0.9818\n",
      "iter 3650: loss 1.0246\n",
      "iter 3660: loss 1.0198\n",
      "iter 3670: loss 1.0050\n",
      "iter 3680: loss 1.0152\n",
      "iter 3690: loss 1.0109\n",
      "iter 3700: loss 0.9884\n",
      "iter 3710: loss 0.9982\n",
      "iter 3720: loss 1.0047\n",
      "iter 3730: loss 0.9918\n",
      "iter 3740: loss 0.9829\n",
      "iter 3750: loss 1.0042\n",
      "iter 3760: loss 0.9990\n",
      "iter 3770: loss 0.9932\n",
      "iter 3780: loss 0.9841\n",
      "iter 3790: loss 0.9795\n",
      "iter 3800: loss 1.0003\n",
      "iter 3810: loss 0.9777\n",
      "iter 3820: loss 0.9902\n",
      "iter 3830: loss 1.0103\n",
      "iter 3840: loss 0.9994\n",
      "iter 3850: loss 0.9961\n",
      "iter 3860: loss 0.9832\n",
      "iter 3870: loss 1.0155\n",
      "iter 3880: loss 0.9988\n",
      "iter 3890: loss 0.9745\n",
      "iter 3900: loss 1.0008\n",
      "iter 3910: loss 0.9849\n",
      "iter 3920: loss 0.9702\n",
      "iter 3930: loss 0.9717\n",
      "iter 3940: loss 0.9876\n",
      "iter 3950: loss 0.9690\n",
      "iter 3960: loss 0.9689\n",
      "iter 3970: loss 0.9661\n",
      "iter 3980: loss 0.9843\n",
      "iter 3990: loss 0.9737\n",
      "iter 4000: loss 0.9703\n",
      "step 4000: train loss 0.8953, val loss 0.8949\n",
      "iter 4010: loss 0.9790\n",
      "iter 4020: loss 0.9788\n",
      "iter 4030: loss 0.9472\n",
      "iter 4040: loss 0.9817\n",
      "iter 4050: loss 0.9850\n",
      "iter 4060: loss 0.9666\n",
      "iter 4070: loss 0.9759\n",
      "iter 4080: loss 0.9727\n",
      "iter 4090: loss 0.9719\n",
      "iter 4100: loss 0.9669\n",
      "iter 4110: loss 0.9737\n",
      "iter 4120: loss 0.9780\n",
      "iter 4130: loss 0.9672\n",
      "iter 4140: loss 0.9734\n",
      "iter 4150: loss 0.9588\n",
      "iter 4160: loss 0.9717\n",
      "iter 4170: loss 0.9475\n",
      "iter 4180: loss 0.9832\n",
      "iter 4190: loss 0.9500\n",
      "iter 4200: loss 0.9809\n",
      "iter 4210: loss 0.9778\n",
      "iter 4220: loss 0.9604\n",
      "iter 4230: loss 0.9717\n",
      "iter 4240: loss 0.9524\n",
      "iter 4250: loss 0.9743\n",
      "iter 4260: loss 0.9847\n",
      "iter 4270: loss 0.9670\n",
      "iter 4280: loss 0.9609\n",
      "iter 4290: loss 0.9625\n",
      "iter 4300: loss 0.9620\n",
      "iter 4310: loss 0.9617\n",
      "iter 4320: loss 0.9551\n",
      "iter 4330: loss 0.9632\n",
      "iter 4340: loss 0.9701\n",
      "iter 4350: loss 0.9449\n",
      "iter 4360: loss 0.9929\n",
      "iter 4370: loss 0.9479\n",
      "iter 4380: loss 0.9558\n",
      "iter 4390: loss 0.9731\n",
      "iter 4400: loss 0.9272\n",
      "iter 4410: loss 0.9386\n",
      "iter 4420: loss 0.9610\n",
      "iter 4430: loss 0.9399\n",
      "iter 4440: loss 0.9631\n",
      "iter 4450: loss 0.9487\n",
      "iter 4460: loss 0.9462\n",
      "iter 4470: loss 0.9425\n",
      "iter 4480: loss 0.9375\n",
      "iter 4490: loss 0.9543\n",
      "iter 4500: loss 0.9577\n",
      "step 4500: train loss 0.8548, val loss 0.8525\n",
      "iter 4510: loss 0.9587\n",
      "iter 4520: loss 0.9453\n",
      "iter 4530: loss 0.9554\n",
      "iter 4540: loss 0.9478\n",
      "iter 4550: loss 0.9573\n",
      "iter 4560: loss 0.9462\n",
      "iter 4570: loss 0.9559\n",
      "iter 4580: loss 0.9388\n",
      "iter 4590: loss 0.9477\n",
      "iter 4600: loss 0.9332\n",
      "iter 4610: loss 0.9434\n",
      "iter 4620: loss 0.9447\n",
      "iter 4630: loss 0.9403\n",
      "iter 4640: loss 0.9466\n",
      "iter 4650: loss 0.9328\n",
      "iter 4660: loss 0.9531\n",
      "iter 4670: loss 0.9484\n",
      "iter 4680: loss 0.9273\n",
      "iter 4690: loss 0.9306\n",
      "iter 4700: loss 0.9279\n",
      "iter 4710: loss 0.9296\n",
      "iter 4720: loss 0.9293\n",
      "iter 4730: loss 0.9243\n",
      "iter 4740: loss 0.9485\n",
      "iter 4750: loss 0.9195\n",
      "iter 4760: loss 0.9251\n",
      "iter 4770: loss 0.9323\n",
      "iter 4780: loss 0.9320\n",
      "iter 4790: loss 0.9382\n",
      "iter 4800: loss 0.9311\n",
      "iter 4810: loss 0.9210\n",
      "iter 4820: loss 0.9366\n",
      "iter 4830: loss 0.9319\n",
      "iter 4840: loss 0.9457\n",
      "iter 4850: loss 0.9342\n",
      "iter 4860: loss 0.9319\n",
      "iter 4870: loss 0.9472\n",
      "iter 4880: loss 0.9358\n",
      "iter 4890: loss 0.9322\n",
      "iter 4900: loss 0.9372\n",
      "iter 4910: loss 0.9386\n",
      "iter 4920: loss 0.9336\n",
      "iter 4930: loss 0.9276\n",
      "iter 4940: loss 0.9206\n",
      "iter 4950: loss 0.9255\n",
      "iter 4960: loss 0.9255\n",
      "iter 4970: loss 0.9164\n",
      "iter 4980: loss 0.9182\n",
      "iter 4990: loss 0.9287\n",
      "iter 5000: loss 0.9181\n",
      "step 5000: train loss 0.8206, val loss 0.8205\n",
      "training done\n",
      "Number of parameters in GPT: 6.43M\n",
      "iter 0: loss 5.1822\n",
      "step 0: train loss 4.4300, val loss 4.4300\n",
      "iter 10: loss 3.1957\n",
      "iter 20: loss 3.1266\n",
      "iter 30: loss 3.1315\n",
      "iter 40: loss 3.1177\n",
      "iter 50: loss 3.1167\n",
      "iter 60: loss 3.1131\n",
      "iter 70: loss 3.1229\n",
      "iter 80: loss 3.1184\n",
      "iter 90: loss 3.1027\n",
      "iter 100: loss 3.0775\n",
      "iter 110: loss 3.0397\n",
      "iter 120: loss 2.9920\n",
      "iter 130: loss 2.9312\n",
      "iter 140: loss 2.9113\n",
      "iter 150: loss 2.8747\n",
      "iter 160: loss 2.8266\n",
      "iter 170: loss 2.7784\n",
      "iter 180: loss 2.7689\n",
      "iter 190: loss 2.7178\n",
      "iter 200: loss 2.6985\n",
      "iter 210: loss 2.7011\n",
      "iter 220: loss 2.6794\n",
      "iter 230: loss 2.6591\n",
      "iter 240: loss 2.6530\n",
      "iter 250: loss 2.6481\n",
      "iter 260: loss 2.6270\n",
      "iter 270: loss 2.6125\n",
      "iter 280: loss 2.6051\n",
      "iter 290: loss 2.5881\n",
      "iter 300: loss 2.5604\n",
      "iter 310: loss 2.5676\n",
      "iter 320: loss 2.5497\n",
      "iter 330: loss 2.5430\n",
      "iter 340: loss 2.5431\n",
      "iter 350: loss 2.5431\n",
      "iter 360: loss 2.5296\n",
      "iter 370: loss 2.5201\n",
      "iter 380: loss 2.5476\n",
      "iter 390: loss 2.5407\n",
      "iter 400: loss 2.5194\n",
      "iter 410: loss 2.5651\n",
      "iter 420: loss 2.5221\n",
      "iter 430: loss 2.5095\n",
      "iter 440: loss 2.5098\n",
      "iter 450: loss 2.5049\n",
      "iter 460: loss 2.4830\n",
      "iter 470: loss 2.4994\n",
      "iter 480: loss 2.4723\n",
      "iter 490: loss 2.4926\n",
      "iter 500: loss 2.4757\n",
      "step 500: train loss 2.4609, val loss 2.4625\n",
      "iter 510: loss 2.4700\n",
      "iter 520: loss 2.4634\n",
      "iter 530: loss 2.4540\n",
      "iter 540: loss 2.4550\n",
      "iter 550: loss 2.4305\n",
      "iter 560: loss 2.4088\n",
      "iter 570: loss 2.4131\n",
      "iter 580: loss 2.3882\n",
      "iter 590: loss 2.3699\n",
      "iter 600: loss 2.3554\n",
      "iter 610: loss 2.3412\n",
      "iter 620: loss 2.3220\n",
      "iter 630: loss 2.3103\n",
      "iter 640: loss 2.2840\n",
      "iter 650: loss 2.2777\n",
      "iter 660: loss 2.2387\n",
      "iter 670: loss 2.2379\n",
      "iter 680: loss 2.1758\n",
      "iter 690: loss 2.2059\n",
      "iter 700: loss 2.1866\n",
      "iter 710: loss 2.1336\n",
      "iter 720: loss 2.1443\n",
      "iter 730: loss 2.0899\n",
      "iter 740: loss 2.0417\n",
      "iter 750: loss 2.0586\n",
      "iter 760: loss 2.0183\n",
      "iter 770: loss 1.9710\n",
      "iter 780: loss 1.9692\n",
      "iter 790: loss 1.9350\n",
      "iter 800: loss 1.8986\n",
      "iter 810: loss 1.8996\n",
      "iter 820: loss 1.8389\n",
      "iter 830: loss 1.8384\n",
      "iter 840: loss 1.8224\n",
      "iter 850: loss 1.8000\n",
      "iter 860: loss 1.7684\n",
      "iter 870: loss 1.7819\n",
      "iter 880: loss 1.7285\n",
      "iter 890: loss 1.7334\n",
      "iter 900: loss 1.8010\n",
      "iter 910: loss 1.7318\n",
      "iter 920: loss 1.7272\n",
      "iter 930: loss 1.6835\n",
      "iter 940: loss 1.6701\n",
      "iter 950: loss 1.6568\n",
      "iter 960: loss 1.6324\n",
      "iter 970: loss 1.6194\n",
      "iter 980: loss 1.6140\n",
      "iter 990: loss 1.6341\n",
      "iter 1000: loss 1.6191\n",
      "step 1000: train loss 1.5562, val loss 1.5528\n",
      "iter 1010: loss 1.6152\n",
      "iter 1020: loss 1.6067\n",
      "iter 1030: loss 1.5732\n",
      "iter 1040: loss 1.5563\n",
      "iter 1050: loss 1.5597\n",
      "iter 1060: loss 1.5582\n",
      "iter 1070: loss 1.5518\n",
      "iter 1080: loss 1.5449\n",
      "iter 1090: loss 1.5266\n",
      "iter 1100: loss 1.5208\n",
      "iter 1110: loss 1.5071\n",
      "iter 1120: loss 1.5278\n",
      "iter 1130: loss 1.4969\n",
      "iter 1140: loss 1.4926\n",
      "iter 1150: loss 1.4500\n",
      "iter 1160: loss 1.5050\n",
      "iter 1170: loss 1.4737\n",
      "iter 1180: loss 1.4590\n",
      "iter 1190: loss 1.4643\n",
      "iter 1200: loss 1.4291\n",
      "iter 1210: loss 1.4715\n",
      "iter 1220: loss 1.4319\n",
      "iter 1230: loss 1.4281\n",
      "iter 1240: loss 1.4197\n",
      "iter 1250: loss 1.4229\n",
      "iter 1260: loss 1.4082\n",
      "iter 1270: loss 1.4311\n",
      "iter 1280: loss 1.4125\n",
      "iter 1290: loss 1.4244\n",
      "iter 1300: loss 1.4142\n",
      "iter 1310: loss 1.4074\n",
      "iter 1320: loss 1.3836\n",
      "iter 1330: loss 1.3759\n",
      "iter 1340: loss 1.4092\n",
      "iter 1350: loss 1.3896\n",
      "iter 1360: loss 1.3743\n",
      "iter 1370: loss 1.3829\n",
      "iter 1380: loss 1.3513\n",
      "iter 1390: loss 1.3761\n",
      "iter 1400: loss 1.3507\n",
      "iter 1410: loss 1.3728\n",
      "iter 1420: loss 1.3749\n",
      "iter 1430: loss 1.3612\n",
      "iter 1440: loss 1.3415\n",
      "iter 1450: loss 1.3668\n",
      "iter 1460: loss 1.3381\n",
      "iter 1470: loss 1.3627\n",
      "iter 1480: loss 1.3461\n",
      "iter 1490: loss 1.3291\n",
      "iter 1500: loss 1.3499\n",
      "step 1500: train loss 1.2967, val loss 1.2967\n",
      "iter 1510: loss 1.3545\n",
      "iter 1520: loss 1.3171\n",
      "iter 1530: loss 1.3469\n",
      "iter 1540: loss 1.3272\n",
      "iter 1550: loss 1.3177\n",
      "iter 1560: loss 1.3215\n",
      "iter 1570: loss 1.3517\n",
      "iter 1580: loss 1.3295\n",
      "iter 1590: loss 1.3151\n",
      "iter 1600: loss 1.3184\n",
      "iter 1610: loss 1.3243\n",
      "iter 1620: loss 1.3225\n",
      "iter 1630: loss 1.3196\n",
      "iter 1640: loss 1.2914\n",
      "iter 1650: loss 1.3079\n",
      "iter 1660: loss 1.2984\n",
      "iter 1670: loss 1.2823\n",
      "iter 1680: loss 1.2893\n",
      "iter 1690: loss 1.2980\n",
      "iter 1700: loss 1.2790\n",
      "iter 1710: loss 1.2794\n",
      "iter 1720: loss 1.2968\n",
      "iter 1730: loss 1.2587\n",
      "iter 1740: loss 1.2525\n",
      "iter 1750: loss 1.2673\n",
      "iter 1760: loss 1.2600\n",
      "iter 1770: loss 1.2774\n",
      "iter 1780: loss 1.2706\n",
      "iter 1790: loss 1.2806\n",
      "iter 1800: loss 1.2612\n",
      "iter 1810: loss 1.2693\n",
      "iter 1820: loss 1.2804\n",
      "iter 1830: loss 1.2713\n",
      "iter 1840: loss 1.2620\n",
      "iter 1850: loss 1.2247\n",
      "iter 1860: loss 1.2841\n",
      "iter 1870: loss 1.2621\n",
      "iter 1880: loss 1.2773\n",
      "iter 1890: loss 1.2450\n",
      "iter 1900: loss 1.2758\n",
      "iter 1910: loss 1.2652\n",
      "iter 1920: loss 1.2259\n",
      "iter 1930: loss 1.2404\n",
      "iter 1940: loss 1.2544\n",
      "iter 1950: loss 1.2603\n",
      "iter 1960: loss 1.2495\n",
      "iter 1970: loss 1.2427\n",
      "iter 1980: loss 1.2603\n",
      "iter 1990: loss 1.2679\n",
      "iter 2000: loss 1.2421\n",
      "step 2000: train loss 1.1922, val loss 1.1972\n",
      "iter 2010: loss 1.2352\n",
      "iter 2020: loss 1.2479\n",
      "iter 2030: loss 1.2264\n",
      "iter 2040: loss 1.2467\n",
      "iter 2050: loss 1.2313\n",
      "iter 2060: loss 1.2241\n",
      "iter 2070: loss 1.2482\n",
      "iter 2080: loss 1.2219\n",
      "iter 2090: loss 1.2280\n",
      "iter 2100: loss 1.2216\n",
      "iter 2110: loss 1.2069\n",
      "iter 2120: loss 1.2325\n",
      "iter 2130: loss 1.2047\n",
      "iter 2140: loss 1.2216\n",
      "iter 2150: loss 1.2080\n",
      "iter 2160: loss 1.1998\n",
      "iter 2170: loss 1.2328\n",
      "iter 2180: loss 1.2064\n",
      "iter 2190: loss 1.2113\n",
      "iter 2200: loss 1.2053\n",
      "iter 2210: loss 1.2302\n",
      "iter 2220: loss 1.2056\n",
      "iter 2230: loss 1.2168\n",
      "iter 2240: loss 1.1971\n",
      "iter 2250: loss 1.1933\n",
      "iter 2260: loss 1.1937\n",
      "iter 2270: loss 1.1806\n",
      "iter 2280: loss 1.2148\n",
      "iter 2290: loss 1.1918\n",
      "iter 2300: loss 1.1848\n",
      "iter 2310: loss 1.2042\n",
      "iter 2320: loss 1.1999\n",
      "iter 2330: loss 1.1853\n",
      "iter 2340: loss 1.2160\n",
      "iter 2350: loss 1.1709\n",
      "iter 2360: loss 1.1984\n",
      "iter 2370: loss 1.1692\n",
      "iter 2380: loss 1.1823\n",
      "iter 2390: loss 1.1770\n",
      "iter 2400: loss 1.1669\n",
      "iter 2410: loss 1.1806\n",
      "iter 2420: loss 1.1781\n",
      "iter 2430: loss 1.1597\n",
      "iter 2440: loss 1.1834\n",
      "iter 2450: loss 1.1969\n",
      "iter 2460: loss 1.1971\n",
      "iter 2470: loss 1.1734\n",
      "iter 2480: loss 1.1804\n",
      "iter 2490: loss 1.1927\n",
      "iter 2500: loss 1.2072\n",
      "step 2500: train loss 1.1203, val loss 1.1250\n",
      "iter 2510: loss 1.1880\n",
      "iter 2520: loss 1.1945\n",
      "iter 2530: loss 1.1568\n",
      "iter 2540: loss 1.1670\n",
      "iter 2550: loss 1.1708\n",
      "iter 2560: loss 1.1544\n",
      "iter 2570: loss 1.1512\n",
      "iter 2580: loss 1.1713\n",
      "iter 2590: loss 1.1504\n",
      "iter 2600: loss 1.1524\n",
      "iter 2610: loss 1.1568\n",
      "iter 2620: loss 1.1539\n",
      "iter 2630: loss 1.1500\n",
      "iter 2640: loss 1.1482\n",
      "iter 2650: loss 1.1511\n",
      "iter 2660: loss 1.1591\n",
      "iter 2670: loss 1.1573\n",
      "iter 2680: loss 1.1577\n",
      "iter 2690: loss 1.1390\n",
      "iter 2700: loss 1.1536\n",
      "iter 2710: loss 1.1689\n",
      "iter 2720: loss 1.1507\n",
      "iter 2730: loss 1.1584\n",
      "iter 2740: loss 1.1803\n",
      "iter 2750: loss 1.1372\n",
      "iter 2760: loss 1.1441\n",
      "iter 2770: loss 1.1500\n",
      "iter 2780: loss 1.1632\n",
      "iter 2790: loss 1.1416\n",
      "iter 2800: loss 1.1312\n",
      "iter 2810: loss 1.1354\n",
      "iter 2820: loss 1.1344\n",
      "iter 2830: loss 1.1445\n",
      "iter 2840: loss 1.1299\n",
      "iter 2850: loss 1.1453\n",
      "iter 2860: loss 1.1207\n",
      "iter 2870: loss 1.1309\n",
      "iter 2880: loss 1.1414\n",
      "iter 2890: loss 1.1447\n",
      "iter 2900: loss 1.1404\n",
      "iter 2910: loss 1.1335\n",
      "iter 2920: loss 1.1356\n",
      "iter 2930: loss 1.0965\n",
      "iter 2940: loss 1.1341\n",
      "iter 2950: loss 1.1216\n",
      "iter 2960: loss 1.1101\n",
      "iter 2970: loss 1.1165\n",
      "iter 2980: loss 1.1175\n",
      "iter 2990: loss 1.1123\n",
      "iter 3000: loss 1.1276\n",
      "step 3000: train loss 1.0611, val loss 1.0664\n",
      "iter 3010: loss 1.1122\n",
      "iter 3020: loss 1.0972\n",
      "iter 3030: loss 1.1343\n",
      "iter 3040: loss 1.1288\n",
      "iter 3050: loss 1.1164\n",
      "iter 3060: loss 1.1059\n",
      "iter 3070: loss 1.1017\n",
      "iter 3080: loss 1.1040\n",
      "iter 3090: loss 1.1077\n",
      "iter 3100: loss 1.1270\n",
      "iter 3110: loss 1.1117\n",
      "iter 3120: loss 1.0979\n",
      "iter 3130: loss 1.1331\n",
      "iter 3140: loss 1.0859\n",
      "iter 3150: loss 1.0895\n",
      "iter 3160: loss 1.1018\n",
      "iter 3170: loss 1.1077\n",
      "iter 3180: loss 1.1026\n",
      "iter 3190: loss 1.1022\n",
      "iter 3200: loss 1.1023\n",
      "iter 3210: loss 1.0802\n",
      "iter 3220: loss 1.1123\n",
      "iter 3230: loss 1.1083\n",
      "iter 3240: loss 1.1070\n",
      "iter 3250: loss 1.1091\n",
      "iter 3260: loss 1.1034\n",
      "iter 3270: loss 1.1058\n",
      "iter 3280: loss 1.1025\n",
      "iter 3290: loss 1.1230\n",
      "iter 3300: loss 1.0943\n",
      "iter 3310: loss 1.0920\n",
      "iter 3320: loss 1.1037\n",
      "iter 3330: loss 1.0763\n",
      "iter 3340: loss 1.0974\n",
      "iter 3350: loss 1.0936\n",
      "iter 3360: loss 1.0666\n",
      "iter 3370: loss 1.0708\n",
      "iter 3380: loss 1.0896\n",
      "iter 3390: loss 1.0780\n",
      "iter 3400: loss 1.0872\n",
      "iter 3410: loss 1.0929\n",
      "iter 3420: loss 1.0842\n",
      "iter 3430: loss 1.0844\n",
      "iter 3440: loss 1.0765\n",
      "iter 3450: loss 1.0900\n",
      "iter 3460: loss 1.0916\n",
      "iter 3470: loss 1.0617\n",
      "iter 3480: loss 1.0771\n",
      "iter 3490: loss 1.0872\n",
      "iter 3500: loss 1.0710\n",
      "step 3500: train loss 1.0148, val loss 1.0170\n",
      "iter 3510: loss 1.0521\n",
      "iter 3520: loss 1.0704\n",
      "iter 3530: loss 1.0809\n",
      "iter 3540: loss 1.0552\n",
      "iter 3550: loss 1.0696\n",
      "iter 3560: loss 1.0802\n",
      "iter 3570: loss 1.0889\n",
      "iter 3580: loss 1.0691\n",
      "iter 3590: loss 1.0679\n",
      "iter 3600: loss 1.0696\n",
      "iter 3610: loss 1.0689\n",
      "iter 3620: loss 1.0515\n",
      "iter 3630: loss 1.0640\n",
      "iter 3640: loss 1.0599\n",
      "iter 3650: loss 1.0392\n",
      "iter 3660: loss 1.0596\n",
      "iter 3670: loss 1.0622\n",
      "iter 3680: loss 1.0692\n",
      "iter 3690: loss 1.0738\n",
      "iter 3700: loss 1.0579\n",
      "iter 3710: loss 1.0554\n",
      "iter 3720: loss 1.0575\n",
      "iter 3730: loss 1.0662\n",
      "iter 3740: loss 1.0666\n",
      "iter 3750: loss 1.0530\n",
      "iter 3760: loss 1.0529\n",
      "iter 3770: loss 1.0391\n",
      "iter 3780: loss 1.0704\n",
      "iter 3790: loss 1.0546\n",
      "iter 3800: loss 1.0351\n",
      "iter 3810: loss 1.0731\n",
      "iter 3820: loss 1.0488\n",
      "iter 3830: loss 1.0381\n",
      "iter 3840: loss 1.0623\n",
      "iter 3850: loss 1.0383\n",
      "iter 3860: loss 1.0448\n",
      "iter 3870: loss 1.0522\n",
      "iter 3880: loss 1.0621\n",
      "iter 3890: loss 1.0513\n",
      "iter 3900: loss 1.0493\n",
      "iter 3910: loss 1.0271\n",
      "iter 3920: loss 1.0633\n",
      "iter 3930: loss 1.0464\n",
      "iter 3940: loss 1.0618\n",
      "iter 3950: loss 1.0557\n",
      "iter 3960: loss 1.0536\n",
      "iter 3970: loss 1.0400\n",
      "iter 3980: loss 1.0531\n",
      "iter 3990: loss 1.0528\n",
      "iter 4000: loss 1.0501\n",
      "step 4000: train loss 0.9695, val loss 0.9690\n",
      "iter 4010: loss 1.0546\n",
      "iter 4020: loss 1.0319\n",
      "iter 4030: loss 1.0358\n",
      "iter 4040: loss 1.0487\n",
      "iter 4050: loss 1.0339\n",
      "iter 4060: loss 1.0307\n",
      "iter 4070: loss 1.0389\n",
      "iter 4080: loss 1.0446\n",
      "iter 4090: loss 1.0205\n",
      "iter 4100: loss 1.0509\n",
      "iter 4110: loss 1.0377\n",
      "iter 4120: loss 1.0219\n",
      "iter 4130: loss 1.0341\n",
      "iter 4140: loss 1.0396\n",
      "iter 4150: loss 1.0235\n",
      "iter 4160: loss 1.0222\n",
      "iter 4170: loss 1.0152\n",
      "iter 4180: loss 1.0398\n",
      "iter 4190: loss 1.0138\n",
      "iter 4200: loss 1.0228\n",
      "iter 4210: loss 1.0254\n",
      "iter 4220: loss 1.0135\n",
      "iter 4230: loss 1.0150\n",
      "iter 4240: loss 1.0298\n",
      "iter 4250: loss 1.0067\n",
      "iter 4260: loss 1.0214\n",
      "iter 4270: loss 1.0283\n",
      "iter 4280: loss 1.0327\n",
      "iter 4290: loss 1.0280\n",
      "iter 4300: loss 0.9982\n",
      "iter 4310: loss 1.0248\n",
      "iter 4320: loss 1.0147\n",
      "iter 4330: loss 1.0351\n",
      "iter 4340: loss 1.0355\n",
      "iter 4350: loss 1.0159\n",
      "iter 4360: loss 1.0129\n",
      "iter 4370: loss 1.0222\n",
      "iter 4380: loss 1.0180\n",
      "iter 4390: loss 1.0310\n",
      "iter 4400: loss 1.0142\n",
      "iter 4410: loss 1.0065\n",
      "iter 4420: loss 1.0304\n",
      "iter 4430: loss 1.0266\n",
      "iter 4440: loss 1.0037\n",
      "iter 4450: loss 1.0081\n",
      "iter 4460: loss 1.0153\n",
      "iter 4470: loss 0.9981\n",
      "iter 4480: loss 1.0317\n",
      "iter 4490: loss 1.0130\n",
      "iter 4500: loss 1.0041\n",
      "step 4500: train loss 0.9256, val loss 0.9321\n",
      "iter 4510: loss 1.0033\n",
      "iter 4520: loss 0.9928\n",
      "iter 4530: loss 0.9836\n",
      "iter 4540: loss 1.0074\n",
      "iter 4550: loss 0.9978\n",
      "iter 4560: loss 0.9933\n",
      "iter 4570: loss 0.9928\n",
      "iter 4580: loss 0.9953\n",
      "iter 4590: loss 0.9962\n",
      "iter 4600: loss 1.0143\n",
      "iter 4610: loss 0.9967\n",
      "iter 4620: loss 0.9998\n",
      "iter 4630: loss 0.9989\n",
      "iter 4640: loss 0.9840\n",
      "iter 4650: loss 1.0303\n",
      "iter 4660: loss 1.0007\n",
      "iter 4670: loss 0.9809\n",
      "iter 4680: loss 1.0033\n",
      "iter 4690: loss 0.9851\n",
      "iter 4700: loss 0.9906\n",
      "iter 4710: loss 0.9936\n",
      "iter 4720: loss 0.9993\n",
      "iter 4730: loss 1.0084\n",
      "iter 4740: loss 0.9888\n",
      "iter 4750: loss 1.0029\n",
      "iter 4760: loss 0.9751\n",
      "iter 4770: loss 0.9968\n",
      "iter 4780: loss 0.9842\n",
      "iter 4790: loss 0.9752\n",
      "iter 4800: loss 0.9965\n",
      "iter 4810: loss 0.9978\n",
      "iter 4820: loss 1.0112\n",
      "iter 4830: loss 0.9877\n",
      "iter 4840: loss 0.9852\n",
      "iter 4850: loss 0.9801\n",
      "iter 4860: loss 0.9872\n",
      "iter 4870: loss 0.9743\n",
      "iter 4880: loss 0.9702\n",
      "iter 4890: loss 0.9688\n",
      "iter 4900: loss 0.9761\n",
      "iter 4910: loss 0.9700\n",
      "iter 4920: loss 0.9774\n",
      "iter 4930: loss 0.9819\n",
      "iter 4940: loss 0.9702\n",
      "iter 4950: loss 0.9745\n",
      "iter 4960: loss 0.9853\n",
      "iter 4970: loss 0.9819\n",
      "iter 4980: loss 0.9622\n",
      "iter 4990: loss 0.9744\n",
      "iter 5000: loss 0.9723\n",
      "step 5000: train loss 0.8882, val loss 0.8913\n",
      "training done\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGiHQ07OTenm",
    "ExecuteTime": {
     "end_time": "2025-01-10T19:06:47.016442Z",
     "start_time": "2025-01-10T19:06:33.502708Z"
    }
   },
   "source": [
    "# %reset -f\n",
    "# import gc\n",
    "# gc.collect()\n",
    "num_samples = 2  # Number of samples to draw\n",
    "max_new_tokens = 500  # Number of tokens generated in each sample\n",
    "temperature = 0.5  # chosen Temperature\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 345\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "path = 'MovieGPT/point_h_model1.pt'\n",
    "# Load the model\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "config = checkpoint['model_args']\n",
    "lr, weight, grad_clip = checkpoint['optimizer_lr'], checkpoint['optimizer_wd'], checkpoint['optimizer_gc']\n",
    "print(lr, weight, grad_clip)\n",
    "print('\\n')\n",
    "model = GPT(config)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create dataset to get encoder/decoder\n",
    "dataset = MovieDataset(block_size=config.block_size)\n",
    "encode = lambda s: [dataset.string_to_int[c] for c in s]\n",
    "decode = dataset.decode\n",
    "\n",
    "# Generate samples\n",
    "print('-'*20)\n",
    "with torch.no_grad():\n",
    "    for k in range(num_samples):\n",
    "        start_prompt = \"\\n\"  # Start prompt\n",
    "        prompt_ids = encode(start_prompt)\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "        y = model.sample(x, max_new_tokens, temperature=temperature)\n",
    "        print(decode(y[0]))\n",
    "        print('-'*20)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_54668\\1566888890.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0 1.0\n",
      "\n",
      "\n",
      "Number of parameters in GPT: 6.43M\n",
      "--------------------\n",
      "\n",
      "The Invincible Living Death: When a young mortal named Andrea approaches her to help her partner must put a sophisticated new enemy in jail, she is forced to survive the water she had been living a secret that wants to destroy her boyfriend.\n",
      "Shark Territory: A family is frustrated by a brutal wireler who has been haunted by an opportunity to get into the judge and forgets out of her past. The story focuses on the planet and puts the fact that they soon fall in love with the case. They discover t\n",
      "--------------------\n",
      "\n",
      "The Lost Boys II: The Black Months: The Rise of the River Saga: In this comedian in the area, it faces the championship of the Southern Universal Soldier that decides to imprison the police and reunite the town with his little sister, Anna Johnson, and her friends are the country of their creatures. As this dastardly starts to do anything, they see an enigmatic teen with some former captain friends from the horizon. The trip becomes a present and a strange boy seems to be prey on the ocean. With\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:58:21.125435Z",
     "start_time": "2025-01-10T18:58:20.997413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reconstructed from logging as it was overwritten due to not paying attention\n",
    "all_losses_train_large = [4.124,1.9689,1.3708,1.2148,1.1235,1.0525,0.9862,0.9275,0.8712,0.8146,0.7686]\n",
    "all_losses_test_large = [4.1255,1.9659,1.3703,1.2156,1.1252,1.0531,0.9861,0.9299,0.8729,0.8218,0.7713]\n",
    "\n",
    "plt.plot(iteration_vec,all_losses_train_large)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"cross entropy loss\")\n",
    "plt.grid()\n",
    "plt.plot(iteration_vec,all_losses_test_large)\n",
    "plt.legend([\"train\", \"validation\"])\n",
    "plt.suptitle(\"Cross entropy loss\")\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8486f8e930d14b988c9576aff4eadbdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4fd107c77894c94be6b099043c28007",
       "IPY_MODEL_50b193ad0cac4b3f8f321ff9822ffd9e",
       "IPY_MODEL_c7a72289d1664799b29914fc8dd3d42a"
      ],
      "layout": "IPY_MODEL_8ee1680569d44635af200597b51ede8e"
     }
    },
    "a4fd107c77894c94be6b099043c28007": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96804033fac24e449b010373184b7f82",
      "placeholder": "​",
      "style": "IPY_MODEL_654a949c3834435088ede31b5ed6e6f8",
      "value": "README.md: 100%"
     }
    },
    "50b193ad0cac4b3f8f321ff9822ffd9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f54524ced5c44cf19d193d4c14bfdfd7",
      "max": 1449,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83ff8032ea86484b87962332c038538d",
      "value": 1449
     }
    },
    "c7a72289d1664799b29914fc8dd3d42a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87a443c53bc34577a600cb54f6a668f2",
      "placeholder": "​",
      "style": "IPY_MODEL_09f6f073d421419684c0d8763e9ba36a",
      "value": " 1.45k/1.45k [00:00&lt;00:00, 55.9kB/s]"
     }
    },
    "8ee1680569d44635af200597b51ede8e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96804033fac24e449b010373184b7f82": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "654a949c3834435088ede31b5ed6e6f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f54524ced5c44cf19d193d4c14bfdfd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83ff8032ea86484b87962332c038538d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "87a443c53bc34577a600cb54f6a668f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09f6f073d421419684c0d8763e9ba36a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe969ebc6a4f4ff1915e52b7846402cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e9a95c75f4a947c8a80fb26237296408",
       "IPY_MODEL_5b48eeb6fa124f1bbb0a8d6d3bf8bf38",
       "IPY_MODEL_035873dbff6b4be1bb046b2b9946bef6"
      ],
      "layout": "IPY_MODEL_ed85aa5dcb144f21ae8e8eb20ed463ea"
     }
    },
    "e9a95c75f4a947c8a80fb26237296408": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ac614601cac47a580c9f283fc6a867b",
      "placeholder": "​",
      "style": "IPY_MODEL_f3d33f775153438c97b5db96dde80ea1",
      "value": "9000plus.csv: 100%"
     }
    },
    "5b48eeb6fa124f1bbb0a8d6d3bf8bf38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b8806b023e54893926ca063fbe8322d",
      "max": 4208091,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b7fc96607bf482e91dec028e6d8ff9c",
      "value": 4208091
     }
    },
    "035873dbff6b4be1bb046b2b9946bef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_155e5edcc1184f92a614c1831713a132",
      "placeholder": "​",
      "style": "IPY_MODEL_3657a78bbcb74f16a644d1199bc07967",
      "value": " 4.21M/4.21M [00:03&lt;00:00, 1.32MB/s]"
     }
    },
    "ed85aa5dcb144f21ae8e8eb20ed463ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ac614601cac47a580c9f283fc6a867b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3d33f775153438c97b5db96dde80ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b8806b023e54893926ca063fbe8322d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b7fc96607bf482e91dec028e6d8ff9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "155e5edcc1184f92a614c1831713a132": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3657a78bbcb74f16a644d1199bc07967": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a4b44de032a4d06bf88a04d70768a8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e4b1c0acaffc43289b1210be7d0db9c3",
       "IPY_MODEL_d81d6fcb6b9d497f9f6b9bdaf1f4e826",
       "IPY_MODEL_9a6f4d5775874d7891e917bf397c7b64"
      ],
      "layout": "IPY_MODEL_7f136828b38b471ea045816f21b30c13"
     }
    },
    "e4b1c0acaffc43289b1210be7d0db9c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f435f1304714fd1a3a6495874ebf463",
      "placeholder": "​",
      "style": "IPY_MODEL_d1fcfbb3fc8b47e885c93b994157a4c8",
      "value": "Generating train split: 100%"
     }
    },
    "d81d6fcb6b9d497f9f6b9bdaf1f4e826": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc46d935a47d499786133c3b31e0d07a",
      "max": 9837,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dba8958cab854b5cb1d635713665f3bc",
      "value": 9837
     }
    },
    "9a6f4d5775874d7891e917bf397c7b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_930f0736e88c41fc8aa0270c6b0184d7",
      "placeholder": "​",
      "style": "IPY_MODEL_3a9a34c86c6b4cc3a9c677d9ae7950c9",
      "value": " 9837/9837 [00:00&lt;00:00, 37039.96 examples/s]"
     }
    },
    "7f136828b38b471ea045816f21b30c13": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f435f1304714fd1a3a6495874ebf463": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1fcfbb3fc8b47e885c93b994157a4c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc46d935a47d499786133c3b31e0d07a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dba8958cab854b5cb1d635713665f3bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "930f0736e88c41fc8aa0270c6b0184d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a9a34c86c6b4cc3a9c677d9ae7950c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
